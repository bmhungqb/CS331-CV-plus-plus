{"metadata":{"colab":{"authorship_tag":"ABX9TyM3GgQX9wAHsY2zHAbVwAG/","name":"DistanceEstimator.ipynb","provenance":[{"file_id":"1WN5OSA-TXiMkTLDr9xyt2F7Z_Hd16-b4","timestamp":1649567852353}]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":7182817,"datasetId":4139879,"databundleVersionId":7272045}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Distance Estimator\nTo estimate the real distance(unit: meter) of the object\n\n__Input__: Bounding box coordinates(xmin, ymin, xmax, ymax)   \n__Output__: 3D location z of carmera coordinates","metadata":{"id":"8xXp-L947DlL"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:18.625849Z","iopub.execute_input":"2023-12-12T10:16:18.626471Z","iopub.status.idle":"2023-12-12T10:16:19.718448Z","shell.execute_reply.started":"2023-12-12T10:16:18.626437Z","shell.execute_reply":"2023-12-12T10:16:19.717267Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue Dec 12 10:16:19 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Module","metadata":{"id":"_LiXtU2475cb"}},{"cell_type":"code","source":"%cd /kaggle/working/\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:21.969885Z","iopub.execute_input":"2023-12-12T10:16:21.970336Z","iopub.status.idle":"2023-12-12T10:16:22.943699Z","shell.execute_reply.started":"2023-12-12T10:16:21.970295Z","shell.execute_reply":"2023-12-12T10:16:22.942633Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile custom_datasets.py\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# get data set\nclass CustomDataset():\n    def __init__(self, data, variable, scaler=False, train=False):\n        self.df=data\n        self.inp = self.df[variable].values\n        self.outp = self.df[['zloc']].values\n        if train==True:\n            self.scaler = StandardScaler().fit(self.inp)\n        else: \n            self.scaler = train\n        # Scaler\n        if scaler==True:\n            self.inp = self.scaler.transform(self.inp)\n                \n    def __len__(self):\n        return len(self.inp) # 1314\n    \n    def __getitem__(self,idx):\n        inp = torch.FloatTensor(self.inp[idx])\n        outp = torch.FloatTensor(self.outp[idx])\n        return inp, outp ","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:36.999552Z","iopub.execute_input":"2023-12-12T10:16:36.999951Z","iopub.status.idle":"2023-12-12T10:16:37.007344Z","shell.execute_reply.started":"2023-12-12T10:16:36.999916Z","shell.execute_reply":"2023-12-12T10:16:37.006262Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing custom_datasets.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile early_stopping.py\n\"\"\"\n@bmhungqb\n\"\"\"\nimport numpy as np\nimport torch\n\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:46.059652Z","iopub.execute_input":"2023-12-12T10:16:46.060057Z","iopub.status.idle":"2023-12-12T10:16:46.066933Z","shell.execute_reply.started":"2023-12-12T10:16:46.060024Z","shell.execute_reply":"2023-12-12T10:16:46.065966Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing early_stopping.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/\nfrom tqdm import tqdm\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader \nfrom sklearn.preprocessing import StandardScaler\nfrom custom_datasets import CustomDataset\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder","metadata":{"id":"J4GISwk4884Q","execution":{"iopub.status.busy":"2023-12-12T10:16:49.657397Z","iopub.execute_input":"2023-12-12T10:16:49.657831Z","iopub.status.idle":"2023-12-12T10:16:55.658345Z","shell.execute_reply.started":"2023-12-12T10:16:49.657797Z","shell.execute_reply":"2023-12-12T10:16:55.657472Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"os.makedirs('./weights', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:55.659910Z","iopub.execute_input":"2023-12-12T10:16:55.660359Z","iopub.status.idle":"2023-12-12T10:16:55.665015Z","shell.execute_reply.started":"2023-12-12T10:16:55.660315Z","shell.execute_reply":"2023-12-12T10:16:55.664077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"cJxQzId_79SS"}},{"cell_type":"code","source":"# get data set\ndf_train = pd.read_csv('/kaggle/input/dataset-distane-estimate-kitti/iou_train.csv')\ndf_valid = pd.read_csv('/kaggle/input/dataset-distane-estimate-kitti/iou_valid.csv')\ndf_test = pd.read_csv('/kaggle/input/dataset-distane-estimate-kitti/iou_test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:55.666196Z","iopub.execute_input":"2023-12-12T10:16:55.666465Z","iopub.status.idle":"2023-12-12T10:16:55.888817Z","shell.execute_reply.started":"2023-12-12T10:16:55.666440Z","shell.execute_reply":"2023-12-12T10:16:55.887701Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:55.891104Z","iopub.execute_input":"2023-12-12T10:16:55.891405Z","iopub.status.idle":"2023-12-12T10:16:55.934683Z","shell.execute_reply.started":"2023-12-12T10:16:55.891378Z","shell.execute_reply":"2023-12-12T10:16:55.933763Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0    filename    class        xmin       ymin        xmax  \\\n0            6142  001150.png    Truck  670.500000  149.35594  742.871340   \n1             482  000110.png      Car  557.085270  164.77954  590.911130   \n2           15893  002947.png      Car  389.736700  184.53830  441.125760   \n3            8778  001651.png      Car  657.857240  168.36122  760.743300   \n4           32882  006039.png      Car  425.418460  171.49203  532.927730   \n...           ...         ...      ...         ...        ...         ...   \n26098       30813  005654.png      Car  688.074000  174.46814  803.448800   \n26099       16473  003042.png  Cyclist  667.827600  159.74207  726.245300   \n26100       23436  004332.png      Car  582.633700  168.72162  638.120500   \n26101       21207  003925.png      Car  578.589800  180.45372  644.207030   \n26102       37425  006877.png      Car    0.208715  201.87906   55.549824   \n\n            ymax  angle   zloc weather  depth_x  depth_mean  depth_y  \\\n0      186.78688   3.04  71.59   clone      741    7.721305      184   \n1      180.29369  -1.02  80.22   clone      575    9.787977      179   \n2      216.58354  -1.34  34.42   clone      389    4.736269      215   \n3      205.31885   2.98  34.05   clone      759    4.808442      201   \n4      257.45590   1.75  16.76   clone      488    2.294223      221   \n...          ...    ...    ...     ...      ...         ...      ...   \n26098  259.98520  -1.77  16.14   clone      800    2.376355      238   \n26099  219.74590  -2.47  20.92   clone      704    3.516689      167   \n26100  224.25133  -1.59  23.82   clone      632    3.432011      185   \n26101  229.85207  -1.70  22.71   clone      643    3.171076      216   \n26102  234.26111  -2.20  49.85   clone       54    5.254285      233   \n\n       depth_min  depth_median  depth_max  depth_mean_trim       width  \\\n0       5.135296      8.066543   9.804019         7.689227   72.371340   \n1       9.487015      9.812434   9.918779         9.809649   33.825860   \n2       3.778310      3.231332   6.782291         3.413900   51.389060   \n3       2.010450      4.758797   8.164209         4.823367  102.886060   \n4       1.821436      1.982113   6.658547         2.007758  107.509270   \n...          ...           ...        ...              ...         ...   \n26098   1.843411      1.909149   6.295566         1.840246  115.374800   \n26099   2.389574      2.873597   8.546009         2.864978   58.417700   \n26100   2.810252      3.020881   9.402097         3.030918   55.486800   \n26101   2.405436      2.626303   8.296898         2.648588   65.617230   \n26102   4.294097      4.901922   7.077499         5.050554   55.341109   \n\n         height  \n0      37.43094  \n1      15.51415  \n2      32.04524  \n3      36.95763  \n4      85.96387  \n...         ...  \n26098  85.51706  \n26099  60.00383  \n26100  55.52971  \n26101  49.39835  \n26102  32.38205  \n\n[26103 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>angle</th>\n      <th>zloc</th>\n      <th>weather</th>\n      <th>depth_x</th>\n      <th>depth_mean</th>\n      <th>depth_y</th>\n      <th>depth_min</th>\n      <th>depth_median</th>\n      <th>depth_max</th>\n      <th>depth_mean_trim</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6142</td>\n      <td>001150.png</td>\n      <td>Truck</td>\n      <td>670.500000</td>\n      <td>149.35594</td>\n      <td>742.871340</td>\n      <td>186.78688</td>\n      <td>3.04</td>\n      <td>71.59</td>\n      <td>clone</td>\n      <td>741</td>\n      <td>7.721305</td>\n      <td>184</td>\n      <td>5.135296</td>\n      <td>8.066543</td>\n      <td>9.804019</td>\n      <td>7.689227</td>\n      <td>72.371340</td>\n      <td>37.43094</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>482</td>\n      <td>000110.png</td>\n      <td>Car</td>\n      <td>557.085270</td>\n      <td>164.77954</td>\n      <td>590.911130</td>\n      <td>180.29369</td>\n      <td>-1.02</td>\n      <td>80.22</td>\n      <td>clone</td>\n      <td>575</td>\n      <td>9.787977</td>\n      <td>179</td>\n      <td>9.487015</td>\n      <td>9.812434</td>\n      <td>9.918779</td>\n      <td>9.809649</td>\n      <td>33.825860</td>\n      <td>15.51415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15893</td>\n      <td>002947.png</td>\n      <td>Car</td>\n      <td>389.736700</td>\n      <td>184.53830</td>\n      <td>441.125760</td>\n      <td>216.58354</td>\n      <td>-1.34</td>\n      <td>34.42</td>\n      <td>clone</td>\n      <td>389</td>\n      <td>4.736269</td>\n      <td>215</td>\n      <td>3.778310</td>\n      <td>3.231332</td>\n      <td>6.782291</td>\n      <td>3.413900</td>\n      <td>51.389060</td>\n      <td>32.04524</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8778</td>\n      <td>001651.png</td>\n      <td>Car</td>\n      <td>657.857240</td>\n      <td>168.36122</td>\n      <td>760.743300</td>\n      <td>205.31885</td>\n      <td>2.98</td>\n      <td>34.05</td>\n      <td>clone</td>\n      <td>759</td>\n      <td>4.808442</td>\n      <td>201</td>\n      <td>2.010450</td>\n      <td>4.758797</td>\n      <td>8.164209</td>\n      <td>4.823367</td>\n      <td>102.886060</td>\n      <td>36.95763</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32882</td>\n      <td>006039.png</td>\n      <td>Car</td>\n      <td>425.418460</td>\n      <td>171.49203</td>\n      <td>532.927730</td>\n      <td>257.45590</td>\n      <td>1.75</td>\n      <td>16.76</td>\n      <td>clone</td>\n      <td>488</td>\n      <td>2.294223</td>\n      <td>221</td>\n      <td>1.821436</td>\n      <td>1.982113</td>\n      <td>6.658547</td>\n      <td>2.007758</td>\n      <td>107.509270</td>\n      <td>85.96387</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26098</th>\n      <td>30813</td>\n      <td>005654.png</td>\n      <td>Car</td>\n      <td>688.074000</td>\n      <td>174.46814</td>\n      <td>803.448800</td>\n      <td>259.98520</td>\n      <td>-1.77</td>\n      <td>16.14</td>\n      <td>clone</td>\n      <td>800</td>\n      <td>2.376355</td>\n      <td>238</td>\n      <td>1.843411</td>\n      <td>1.909149</td>\n      <td>6.295566</td>\n      <td>1.840246</td>\n      <td>115.374800</td>\n      <td>85.51706</td>\n    </tr>\n    <tr>\n      <th>26099</th>\n      <td>16473</td>\n      <td>003042.png</td>\n      <td>Cyclist</td>\n      <td>667.827600</td>\n      <td>159.74207</td>\n      <td>726.245300</td>\n      <td>219.74590</td>\n      <td>-2.47</td>\n      <td>20.92</td>\n      <td>clone</td>\n      <td>704</td>\n      <td>3.516689</td>\n      <td>167</td>\n      <td>2.389574</td>\n      <td>2.873597</td>\n      <td>8.546009</td>\n      <td>2.864978</td>\n      <td>58.417700</td>\n      <td>60.00383</td>\n    </tr>\n    <tr>\n      <th>26100</th>\n      <td>23436</td>\n      <td>004332.png</td>\n      <td>Car</td>\n      <td>582.633700</td>\n      <td>168.72162</td>\n      <td>638.120500</td>\n      <td>224.25133</td>\n      <td>-1.59</td>\n      <td>23.82</td>\n      <td>clone</td>\n      <td>632</td>\n      <td>3.432011</td>\n      <td>185</td>\n      <td>2.810252</td>\n      <td>3.020881</td>\n      <td>9.402097</td>\n      <td>3.030918</td>\n      <td>55.486800</td>\n      <td>55.52971</td>\n    </tr>\n    <tr>\n      <th>26101</th>\n      <td>21207</td>\n      <td>003925.png</td>\n      <td>Car</td>\n      <td>578.589800</td>\n      <td>180.45372</td>\n      <td>644.207030</td>\n      <td>229.85207</td>\n      <td>-1.70</td>\n      <td>22.71</td>\n      <td>clone</td>\n      <td>643</td>\n      <td>3.171076</td>\n      <td>216</td>\n      <td>2.405436</td>\n      <td>2.626303</td>\n      <td>8.296898</td>\n      <td>2.648588</td>\n      <td>65.617230</td>\n      <td>49.39835</td>\n    </tr>\n    <tr>\n      <th>26102</th>\n      <td>37425</td>\n      <td>006877.png</td>\n      <td>Car</td>\n      <td>0.208715</td>\n      <td>201.87906</td>\n      <td>55.549824</td>\n      <td>234.26111</td>\n      <td>-2.20</td>\n      <td>49.85</td>\n      <td>clone</td>\n      <td>54</td>\n      <td>5.254285</td>\n      <td>233</td>\n      <td>4.294097</td>\n      <td>4.901922</td>\n      <td>7.077499</td>\n      <td>5.050554</td>\n      <td>55.341109</td>\n      <td>32.38205</td>\n    </tr>\n  </tbody>\n</table>\n<p>26103 rows × 19 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# sort the z_loc values\ndf_train['zloc'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:55.935809Z","iopub.execute_input":"2023-12-12T10:16:55.936122Z","iopub.status.idle":"2023-12-12T10:16:55.946741Z","shell.execute_reply.started":"2023-12-12T10:16:55.936093Z","shell.execute_reply":"2023-12-12T10:16:55.945864Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"24252      2.88\n10803      2.93\n22145      2.95\n208        3.03\n14678      3.07\n          ...  \n12455    101.70\n7596     103.74\n10698    106.63\n13117    112.40\n12039    115.28\nName: zloc, Length: 26103, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"df_train.isna().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:55.947869Z","iopub.execute_input":"2023-12-12T10:16:55.948200Z","iopub.status.idle":"2023-12-12T10:16:55.967343Z","shell.execute_reply.started":"2023-12-12T10:16:55.948163Z","shell.execute_reply":"2023-12-12T10:16:55.966561Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0         0\nfilename           0\nclass              0\nxmin               0\nymin               0\nxmax               0\nymax               0\nangle              0\nzloc               0\nweather            0\ndepth_x            0\ndepth_mean         0\ndepth_y            0\ndepth_min          0\ndepth_median       0\ndepth_max          0\ndepth_mean_trim    0\nwidth              0\nheight             0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_valid.isna().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:56.714137Z","iopub.execute_input":"2023-12-12T10:16:56.714787Z","iopub.status.idle":"2023-12-12T10:16:56.724616Z","shell.execute_reply.started":"2023-12-12T10:16:56.714739Z","shell.execute_reply":"2023-12-12T10:16:56.723597Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0         0\nfilename           0\nclass              0\nxmin               0\nymin               0\nxmax               0\nymax               0\nangle              0\nzloc               0\nweather            0\ndepth_x            0\ndepth_mean         0\ndepth_y            0\ndepth_min          0\ndepth_median       0\ndepth_max          0\ndepth_mean_trim    0\nwidth              0\nheight             0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_test.isna().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:57.128315Z","iopub.execute_input":"2023-12-12T10:16:57.128688Z","iopub.status.idle":"2023-12-12T10:16:57.137788Z","shell.execute_reply.started":"2023-12-12T10:16:57.128645Z","shell.execute_reply":"2023-12-12T10:16:57.136914Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0         0\nfilename           0\nclass              0\nxmin               0\nymin               0\nxmax               0\nymax               0\nangle              0\nzloc               0\nweather            0\ndepth_x            0\ndepth_mean         0\ndepth_y            0\ndepth_min          0\ndepth_median       0\ndepth_max          0\ndepth_mean_trim    0\nwidth              0\nheight             0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_train = df_train[df_train['zloc'] > 0]\ndf_valid = df_valid[df_valid['zloc'] > 0]\ndf_test = df_test[df_test['zloc'] > 0]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:16:59.363632Z","iopub.execute_input":"2023-12-12T10:16:59.364035Z","iopub.status.idle":"2023-12-12T10:16:59.373055Z","shell.execute_reply.started":"2023-12-12T10:16:59.364001Z","shell.execute_reply":"2023-12-12T10:16:59.372093Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#check the class\ndf_train['class'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:17:01.165993Z","iopub.execute_input":"2023-12-12T10:17:01.166345Z","iopub.status.idle":"2023-12-12T10:17:01.181190Z","shell.execute_reply.started":"2023-12-12T10:17:01.166307Z","shell.execute_reply":"2023-12-12T10:17:01.180155Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array(['Truck', 'Car', 'Pedestrian', 'Van', 'Cyclist', 'Tram',\n       'Person_sitting', 'Misc'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:17:02.450337Z","iopub.execute_input":"2023-12-12T10:17:02.450945Z","iopub.status.idle":"2023-12-12T10:17:02.481505Z","shell.execute_reply.started":"2023-12-12T10:17:02.450912Z","shell.execute_reply":"2023-12-12T10:17:02.480551Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0    filename    class        xmin       ymin        xmax  \\\n0            6142  001150.png    Truck  670.500000  149.35594  742.871340   \n1             482  000110.png      Car  557.085270  164.77954  590.911130   \n2           15893  002947.png      Car  389.736700  184.53830  441.125760   \n3            8778  001651.png      Car  657.857240  168.36122  760.743300   \n4           32882  006039.png      Car  425.418460  171.49203  532.927730   \n...           ...         ...      ...         ...        ...         ...   \n26098       30813  005654.png      Car  688.074000  174.46814  803.448800   \n26099       16473  003042.png  Cyclist  667.827600  159.74207  726.245300   \n26100       23436  004332.png      Car  582.633700  168.72162  638.120500   \n26101       21207  003925.png      Car  578.589800  180.45372  644.207030   \n26102       37425  006877.png      Car    0.208715  201.87906   55.549824   \n\n            ymax  angle   zloc weather  depth_x  depth_mean  depth_y  \\\n0      186.78688   3.04  71.59   clone      741    7.721305      184   \n1      180.29369  -1.02  80.22   clone      575    9.787977      179   \n2      216.58354  -1.34  34.42   clone      389    4.736269      215   \n3      205.31885   2.98  34.05   clone      759    4.808442      201   \n4      257.45590   1.75  16.76   clone      488    2.294223      221   \n...          ...    ...    ...     ...      ...         ...      ...   \n26098  259.98520  -1.77  16.14   clone      800    2.376355      238   \n26099  219.74590  -2.47  20.92   clone      704    3.516689      167   \n26100  224.25133  -1.59  23.82   clone      632    3.432011      185   \n26101  229.85207  -1.70  22.71   clone      643    3.171076      216   \n26102  234.26111  -2.20  49.85   clone       54    5.254285      233   \n\n       depth_min  depth_median  depth_max  depth_mean_trim       width  \\\n0       5.135296      8.066543   9.804019         7.689227   72.371340   \n1       9.487015      9.812434   9.918779         9.809649   33.825860   \n2       3.778310      3.231332   6.782291         3.413900   51.389060   \n3       2.010450      4.758797   8.164209         4.823367  102.886060   \n4       1.821436      1.982113   6.658547         2.007758  107.509270   \n...          ...           ...        ...              ...         ...   \n26098   1.843411      1.909149   6.295566         1.840246  115.374800   \n26099   2.389574      2.873597   8.546009         2.864978   58.417700   \n26100   2.810252      3.020881   9.402097         3.030918   55.486800   \n26101   2.405436      2.626303   8.296898         2.648588   65.617230   \n26102   4.294097      4.901922   7.077499         5.050554   55.341109   \n\n         height  \n0      37.43094  \n1      15.51415  \n2      32.04524  \n3      36.95763  \n4      85.96387  \n...         ...  \n26098  85.51706  \n26099  60.00383  \n26100  55.52971  \n26101  49.39835  \n26102  32.38205  \n\n[26103 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>class</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n      <th>angle</th>\n      <th>zloc</th>\n      <th>weather</th>\n      <th>depth_x</th>\n      <th>depth_mean</th>\n      <th>depth_y</th>\n      <th>depth_min</th>\n      <th>depth_median</th>\n      <th>depth_max</th>\n      <th>depth_mean_trim</th>\n      <th>width</th>\n      <th>height</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6142</td>\n      <td>001150.png</td>\n      <td>Truck</td>\n      <td>670.500000</td>\n      <td>149.35594</td>\n      <td>742.871340</td>\n      <td>186.78688</td>\n      <td>3.04</td>\n      <td>71.59</td>\n      <td>clone</td>\n      <td>741</td>\n      <td>7.721305</td>\n      <td>184</td>\n      <td>5.135296</td>\n      <td>8.066543</td>\n      <td>9.804019</td>\n      <td>7.689227</td>\n      <td>72.371340</td>\n      <td>37.43094</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>482</td>\n      <td>000110.png</td>\n      <td>Car</td>\n      <td>557.085270</td>\n      <td>164.77954</td>\n      <td>590.911130</td>\n      <td>180.29369</td>\n      <td>-1.02</td>\n      <td>80.22</td>\n      <td>clone</td>\n      <td>575</td>\n      <td>9.787977</td>\n      <td>179</td>\n      <td>9.487015</td>\n      <td>9.812434</td>\n      <td>9.918779</td>\n      <td>9.809649</td>\n      <td>33.825860</td>\n      <td>15.51415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15893</td>\n      <td>002947.png</td>\n      <td>Car</td>\n      <td>389.736700</td>\n      <td>184.53830</td>\n      <td>441.125760</td>\n      <td>216.58354</td>\n      <td>-1.34</td>\n      <td>34.42</td>\n      <td>clone</td>\n      <td>389</td>\n      <td>4.736269</td>\n      <td>215</td>\n      <td>3.778310</td>\n      <td>3.231332</td>\n      <td>6.782291</td>\n      <td>3.413900</td>\n      <td>51.389060</td>\n      <td>32.04524</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8778</td>\n      <td>001651.png</td>\n      <td>Car</td>\n      <td>657.857240</td>\n      <td>168.36122</td>\n      <td>760.743300</td>\n      <td>205.31885</td>\n      <td>2.98</td>\n      <td>34.05</td>\n      <td>clone</td>\n      <td>759</td>\n      <td>4.808442</td>\n      <td>201</td>\n      <td>2.010450</td>\n      <td>4.758797</td>\n      <td>8.164209</td>\n      <td>4.823367</td>\n      <td>102.886060</td>\n      <td>36.95763</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32882</td>\n      <td>006039.png</td>\n      <td>Car</td>\n      <td>425.418460</td>\n      <td>171.49203</td>\n      <td>532.927730</td>\n      <td>257.45590</td>\n      <td>1.75</td>\n      <td>16.76</td>\n      <td>clone</td>\n      <td>488</td>\n      <td>2.294223</td>\n      <td>221</td>\n      <td>1.821436</td>\n      <td>1.982113</td>\n      <td>6.658547</td>\n      <td>2.007758</td>\n      <td>107.509270</td>\n      <td>85.96387</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26098</th>\n      <td>30813</td>\n      <td>005654.png</td>\n      <td>Car</td>\n      <td>688.074000</td>\n      <td>174.46814</td>\n      <td>803.448800</td>\n      <td>259.98520</td>\n      <td>-1.77</td>\n      <td>16.14</td>\n      <td>clone</td>\n      <td>800</td>\n      <td>2.376355</td>\n      <td>238</td>\n      <td>1.843411</td>\n      <td>1.909149</td>\n      <td>6.295566</td>\n      <td>1.840246</td>\n      <td>115.374800</td>\n      <td>85.51706</td>\n    </tr>\n    <tr>\n      <th>26099</th>\n      <td>16473</td>\n      <td>003042.png</td>\n      <td>Cyclist</td>\n      <td>667.827600</td>\n      <td>159.74207</td>\n      <td>726.245300</td>\n      <td>219.74590</td>\n      <td>-2.47</td>\n      <td>20.92</td>\n      <td>clone</td>\n      <td>704</td>\n      <td>3.516689</td>\n      <td>167</td>\n      <td>2.389574</td>\n      <td>2.873597</td>\n      <td>8.546009</td>\n      <td>2.864978</td>\n      <td>58.417700</td>\n      <td>60.00383</td>\n    </tr>\n    <tr>\n      <th>26100</th>\n      <td>23436</td>\n      <td>004332.png</td>\n      <td>Car</td>\n      <td>582.633700</td>\n      <td>168.72162</td>\n      <td>638.120500</td>\n      <td>224.25133</td>\n      <td>-1.59</td>\n      <td>23.82</td>\n      <td>clone</td>\n      <td>632</td>\n      <td>3.432011</td>\n      <td>185</td>\n      <td>2.810252</td>\n      <td>3.020881</td>\n      <td>9.402097</td>\n      <td>3.030918</td>\n      <td>55.486800</td>\n      <td>55.52971</td>\n    </tr>\n    <tr>\n      <th>26101</th>\n      <td>21207</td>\n      <td>003925.png</td>\n      <td>Car</td>\n      <td>578.589800</td>\n      <td>180.45372</td>\n      <td>644.207030</td>\n      <td>229.85207</td>\n      <td>-1.70</td>\n      <td>22.71</td>\n      <td>clone</td>\n      <td>643</td>\n      <td>3.171076</td>\n      <td>216</td>\n      <td>2.405436</td>\n      <td>2.626303</td>\n      <td>8.296898</td>\n      <td>2.648588</td>\n      <td>65.617230</td>\n      <td>49.39835</td>\n    </tr>\n    <tr>\n      <th>26102</th>\n      <td>37425</td>\n      <td>006877.png</td>\n      <td>Car</td>\n      <td>0.208715</td>\n      <td>201.87906</td>\n      <td>55.549824</td>\n      <td>234.26111</td>\n      <td>-2.20</td>\n      <td>49.85</td>\n      <td>clone</td>\n      <td>54</td>\n      <td>5.254285</td>\n      <td>233</td>\n      <td>4.294097</td>\n      <td>4.901922</td>\n      <td>7.077499</td>\n      <td>5.050554</td>\n      <td>55.341109</td>\n      <td>32.38205</td>\n    </tr>\n  </tbody>\n</table>\n<p>26103 rows × 19 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"variable = ['xmin','ymin','xmax','ymax','width', 'height','depth_mean_trim','depth_mean','depth_max','depth_median']\nval_length = len(variable)\nbatch_sz = 64\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    # Set the current device to the first GPU\n    device = torch.device(\"cuda:0\")\n    # If there are multiple GPUs, you can set the current device to 'cuda:0' and use DataParallel\n    if num_gpus > 1:\n        device = torch.device(\"cuda:0\")\n        print(f\"Using {num_gpus} GPUs.\")\n    else:\n        print(\"Using a single GPU.\")\nelse:\n    device = torch.device(\"cpu\")\n\n# train\ntrain_dataset = CustomDataset(df_train, variable, scaler=True, train=True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)\n# train_sclaer\nscaler_train = train_dataset.scaler\n\n# valid\nvalid_dataset = CustomDataset(df_valid, variable, True, train=scaler_train)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_sz, shuffle=True)\n\n# test\ntest_dataset = CustomDataset(df_test, variable, True, train=scaler_train)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(df_test), shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:17:05.050338Z","iopub.execute_input":"2023-12-12T10:17:05.051198Z","iopub.status.idle":"2023-12-12T10:17:05.173881Z","shell.execute_reply.started":"2023-12-12T10:17:05.051151Z","shell.execute_reply":"2023-12-12T10:17:05.172649Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Using 2 GPUs.\n","output_type":"stream"}]},{"cell_type":"code","source":"from pickle import dump\n# scaler\ndump(scaler_train, open('/kaggle/working/weights/lstm_scaler.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:17:18.250576Z","iopub.execute_input":"2023-12-12T10:17:18.250970Z","iopub.status.idle":"2023-12-12T10:17:18.256304Z","shell.execute_reply.started":"2023-12-12T10:17:18.250937Z","shell.execute_reply":"2023-12-12T10:17:18.255238Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# look the dataset\nfor idx, batch in enumerate(train_dataloader):\n    if idx == 1:\n        break\n    print(batch[0])\n    print(batch[0].shape)\n    print(batch[0].dtype)\n    print(batch[1])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-12T10:17:30.038033Z","iopub.execute_input":"2023-12-12T10:17:30.038408Z","iopub.status.idle":"2023-12-12T10:17:30.237115Z","shell.execute_reply.started":"2023-12-12T10:17:30.038376Z","shell.execute_reply":"2023-12-12T10:17:30.235731Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"tensor([[-1.8766,  0.5172, -1.7060,  2.2447,  0.5462,  1.8606, -1.1885, -1.3463,\n         -2.0886, -1.1606],\n        [ 0.5429,  0.0261,  0.2183, -0.6315, -0.9700, -0.5914,  0.5273,  0.3494,\n         -0.5454,  0.5209],\n        [ 0.2849, -0.0183,  0.0265, -0.8028, -0.7692, -0.7314,  0.8290,  0.7866,\n          1.2555,  0.8311],\n        [-1.1222,  0.4964, -0.9765, -0.1887,  0.4555, -0.3702, -0.3844, -0.5012,\n          0.7990, -0.3524],\n        [-1.8773,  0.8436, -1.8975,  1.6715, -0.0168,  1.2041, -0.9868, -1.0888,\n         -1.4753, -0.9715],\n        [-1.0133,  0.4669, -0.3227,  2.2271,  2.0608,  1.8643, -1.1771, -1.2353,\n         -0.4568, -1.1664],\n        [ 0.1381,  0.3062, -0.0113, -0.3432, -0.4442, -0.4370, -0.3802, -0.1292,\n          0.0974, -0.3485],\n        [ 0.4523,  0.3093,  0.1761, -0.8710, -0.8254, -0.9238,  1.6138,  1.5367,\n          0.8209,  1.5951],\n        [-0.1493,  0.5832, -0.4101, -0.7047, -0.7662, -0.8793,  1.1268,  1.0861,\n          0.6356,  1.1129],\n        [ 0.4471,  0.2721,  0.2041, -0.9785, -0.7271, -1.0080,  2.4584,  2.3192,\n          0.9668,  2.4490],\n        [ 0.4340,  0.3246,  0.1893, -0.6921, -0.7317, -0.7653,  0.9718,  1.1607,\n          1.1470,  0.9559],\n        [-1.8775,  0.9235, -1.1977,  2.2537,  2.0485,  1.7080, -1.2195, -1.2915,\n         -1.1969, -1.2026],\n        [-0.7242, -0.3516, -0.8401, -0.4159, -0.3255, -0.2435, -0.4416, -0.3146,\n          0.9045, -0.4537],\n        [-0.2018,  0.2685, -0.4246, -0.8874, -0.6528, -0.9228,  1.8136,  1.4365,\n          0.8949,  1.8039],\n        [ 2.1034,  0.4965,  1.8815, -0.0409, -0.7027, -0.2342, -0.3301, -0.2852,\n         -0.2205, -0.3808],\n        [-1.4592,  0.3186, -1.5164, -0.6366, -0.1357, -0.7119,  0.3320,  0.3340,\n          0.1329,  0.3186],\n        [ 0.1888, -0.4964, -0.0982, -1.0244, -0.8513, -0.7460,  0.8385,  1.0722,\n         -0.0491,  1.1953],\n        [-0.0044,  0.3306, -0.0068, -0.6009, -0.0069, -0.6838,  0.5151,  0.4757,\n         -0.0686,  0.4545],\n        [ 0.5613,  0.3309,  0.2621, -0.8822, -0.8954, -0.9428,  1.0105,  1.0391,\n          0.7410,  1.0089],\n        [ 0.1099, -0.4887, -0.1900, -0.4406, -0.8876, -0.2119, -0.2562,  0.0509,\n          0.7749, -0.3601],\n        [-1.8748,  0.7617, -1.5522,  0.9684,  0.9945,  0.5895, -0.9329, -1.0412,\n         -0.0795, -0.9300],\n        [-0.6643,  0.2640, -0.8436,  1.2790, -0.5140,  1.0723, -1.0586, -1.1649,\n         -1.8237, -1.0626],\n        [ 0.5754,  0.2246,  0.4542, -0.2778, -0.3707, -0.3445, -0.3920, -0.3576,\n          0.1992, -0.3858],\n        [-0.8433, -1.4511, -1.0998,  0.1685, -0.7378,  0.7295, -1.0304, -1.1243,\n         -1.3443, -1.0199],\n        [-0.3016,  0.6534, -0.4134, -0.2155, -0.3232, -0.4570, -0.3025, -0.2336,\n         -0.0983, -0.2276],\n        [-1.1246,  1.0415, -1.2594,  1.8166, -0.3721,  1.2591, -1.0551, -1.1340,\n         -1.6560, -1.1122],\n        [-0.1829,  0.3203, -0.3683, -0.7575, -0.5427, -0.8238,  1.3113,  1.2902,\n          0.9334,  1.2916],\n        [-0.8817,  0.6772, -0.7096,  0.0228,  0.5278, -0.2471, -0.5538, -0.4132,\n          0.2777, -0.5520],\n        [ 0.9649, -1.5616,  1.8309,  2.0860,  2.5333,  2.5375, -1.1810, -1.2822,\n         -1.6448, -1.1770],\n        [ 0.1179,  0.1638, -0.1551, -0.8720, -0.8082, -0.8672,  1.1298,  1.1861,\n          1.0478,  1.0659],\n        [ 0.6197, -0.1095,  0.2858, -0.8684, -0.9994, -0.7557,  1.2641,  1.1679,\n          0.5491,  1.2495],\n        [-1.7200,  0.5159, -1.5173,  0.0368,  0.6374, -0.1704, -0.7329, -0.3165,\n          0.0658, -0.7056],\n        [ 1.0138, -0.3845,  0.7773, -1.0569, -0.7211, -0.8203,  1.4836,  1.3284,\n          0.7394,  1.4687],\n        [ 0.0134, -0.0114, -0.1700, -0.6602, -0.5412, -0.6029,  0.0518,  0.0387,\n          1.1698,  0.0521],\n        [ 0.2881, -0.1838,  0.0905, -0.6321, -0.5895, -0.5088, -0.0132,  0.0086,\n          0.9662, -0.0189],\n        [-1.7756,  0.2073, -1.4202, -0.1352,  1.0893, -0.2065, -0.3911, -0.3610,\n         -0.2227, -0.4220],\n        [-0.4584,  0.8122, -0.6937, -0.3758, -0.6837, -0.6673,  0.6150,  0.6939,\n          0.7507,  0.5439],\n        [-0.6662, -0.1851, -0.7944, -0.8231, -0.3631, -0.6841,  0.3201,  0.7749,\n         -0.0259,  0.5278],\n        [-0.3979,  0.3433, -0.6040, -0.6812, -0.5991, -0.7627,  0.7776,  0.8897,\n          1.2241,  0.7428],\n        [-1.4180,  0.3643, -1.1526,  0.3664,  0.8155,  0.1929, -0.7348, -0.7144,\n          0.7166, -0.7370],\n        [ 1.2546, -0.2022,  1.3356, -0.6598,  0.2105, -0.5270,  0.0325,  0.0034,\n         -0.2413, -0.0384],\n        [ 2.1625,  1.4447,  2.3056,  2.2381,  0.3727,  1.4874, -1.1933, -1.3257,\n         -2.0562, -1.1807],\n        [-0.3274,  0.6221, -0.2631,  0.2431,  0.1972, -0.0226, -0.7693, -0.8450,\n         -1.1203, -0.7650],\n        [ 0.4115,  0.2140,  0.1225, -0.9460, -0.8623, -0.9552,  1.5365,  1.4358,\n          0.7442,  1.5235],\n        [ 0.6753, -0.5143,  0.4482, -0.8935, -0.6853, -0.6185,  0.4802,  0.6580,\n          1.0303,  0.3719],\n        [ 0.2147,  0.3418, -0.1073, -1.0095, -0.9550, -1.0642,  2.8102,  2.6615,\n          1.1218,  2.8108],\n        [-1.2821,  1.1015, -0.9537,  1.0820,  0.9982,  0.5595, -1.0700, -0.8615,\n         -1.6228, -1.0502],\n        [ 0.5181,  0.2149,  0.2870, -0.6583, -0.6935, -0.6908,  0.3206,  0.3847,\n          0.9789,  0.3193],\n        [ 0.8381, -0.1301,  0.6248, -0.7534, -0.6485, -0.6417,  0.3264,  0.4220,\n          1.0420,  0.3179],\n        [ 0.5130,  0.2359,  0.3173, -0.5558, -0.5894, -0.6047,  0.0388,  0.2098,\n          0.4842,  0.0934],\n        [ 0.2650,  0.1416, -0.0150, -0.8765, -0.8322, -0.8626,  1.2956,  1.2014,\n          0.7483,  1.2901],\n        [-1.2516,  0.9591, -0.6113,  1.9489,  1.9180,  1.4135, -1.0985, -1.0928,\n         -0.2250, -1.1244],\n        [ 0.4202, -0.2230,  0.1225, -1.1711, -0.8879, -0.9893,  2.5744,  2.4690,\n          1.2980,  2.5346],\n        [-0.8170, -0.1957, -0.9311, -0.5973, -0.3180, -0.4721,  0.3353,  0.5767,\n          1.0276,  0.4047],\n        [ 2.1675, -2.3183,  2.1799,  2.2080, -0.0130,  2.9493, -1.3313, -1.4638,\n         -2.3500, -1.3033],\n        [ 0.2842,  0.4750,  0.1417, -0.2128, -0.4270, -0.3838, -0.3156, -0.2398,\n          1.0514, -0.3087],\n        [ 0.6120, -0.1791,  0.3972, -1.0891, -0.6478, -0.9312,  1.2420,  1.3687,\n          0.4033,  1.2773],\n        [ 0.0702,  0.1229, -0.1136, -0.4326, -0.5439, -0.4467,  0.1074,  0.2610,\n          0.2741,  0.1562],\n        [ 0.5767, -0.0129,  0.2472, -0.8515, -0.9853, -0.7784,  0.7790,  0.6699,\n         -0.2936,  0.7688],\n        [ 0.7120,  0.5196,  0.4841,  0.1924, -0.6886, -0.0287, -0.4242, -0.4931,\n         -0.6577, -0.4563],\n        [-1.3199,  0.6401, -1.1373,  0.2651,  0.5687, -0.0095, -0.6435, -0.6738,\n          0.2394, -0.6403],\n        [ 0.5742,  0.1078,  0.5634, -0.7341, -0.0448, -0.7181,  0.6628,  0.7050,\n          1.0327,  0.5644],\n        [ 0.8023, -0.2985,  0.6342, -1.1583, -0.5141, -0.9476,  0.9540,  0.9234,\n          0.1845,  0.9640],\n        [ 2.3721, -0.0458,  2.3043, -0.7192, -0.2539, -0.6436,  0.4524,  0.4526,\n          0.2527,  0.4164]])\ntorch.Size([64, 10])\ntorch.float32\ntensor([[ 4.0600],\n        [30.2500],\n        [37.9700],\n        [22.8700],\n        [13.3000],\n        [ 7.9600],\n        [23.7100],\n        [50.5100],\n        [48.2800],\n        [65.7600],\n        [39.0500],\n        [ 8.1800],\n        [20.0700],\n        [49.8200],\n        [15.0000],\n        [40.1300],\n        [50.0900],\n        [31.3200],\n        [51.1400],\n        [22.5800],\n        [ 4.1800],\n        [ 9.1700],\n        [20.6000],\n        [12.7300],\n        [24.2800],\n        [ 8.7600],\n        [43.4600],\n        [20.4900],\n        [ 8.3900],\n        [42.4700],\n        [41.2200],\n        [23.2500],\n        [49.4900],\n        [28.0100],\n        [26.4800],\n        [23.4000],\n        [36.8000],\n        [40.4900],\n        [38.9800],\n        [18.0600],\n        [25.4000],\n        [ 3.6200],\n        [14.5500],\n        [49.2800],\n        [34.0000],\n        [71.9100],\n        [13.7100],\n        [30.9900],\n        [32.5300],\n        [28.4400],\n        [46.8500],\n        [ 9.5000],\n        [62.5100],\n        [33.4700],\n        [ 6.7700],\n        [20.5500],\n        [52.5400],\n        [29.3300],\n        [38.0800],\n        [16.8900],\n        [18.3300],\n        [33.0200],\n        [54.9900],\n        [42.9100]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Modeling","metadata":{"id":"J_18WIN49vj6"}},{"cell_type":"code","source":"# zloc estimator model\nclass Distance_Estimator(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim=1):\n        super().__init__()\n        \n        self.rnn = nn.LSTM(input_dim, hidden_dim, layer_dim,\n                           batch_first=True, bidirectional=False)\n        \n        #Layer\n        layersize=[306, 154, 76]\n        layerlist= []\n        n_in=hidden_dim\n        for i in layersize:\n            layerlist.append(nn.Linear(n_in,i))\n            layerlist.append(nn.ReLU())\n            n_in=i           \n        layerlist.append(nn.Linear(layersize[-1],1))        \n        self.fc=nn.Sequential(*layerlist)\n\n        \n    def forward(self, x):\n        out, hn = self.rnn(x)\n        output = self.fc(out[:,-1])\n        return output","metadata":{"id":"6SqWrYRLCdaO","execution":{"iopub.status.busy":"2023-12-12T10:18:15.809694Z","iopub.execute_input":"2023-12-12T10:18:15.810155Z","iopub.status.idle":"2023-12-12T10:18:15.818449Z","shell.execute_reply.started":"2023-12-12T10:18:15.810121Z","shell.execute_reply":"2023-12-12T10:18:15.817582Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Make  variable","metadata":{}},{"cell_type":"code","source":"\nimport torch.nn.init as init\ninput_dim = val_length\nhidden_dim = 612\nlayer_dim = 3\n        \nmodel = Distance_Estimator(input_dim, hidden_dim, layer_dim)\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                       factor=0.5,\n                                                       patience = 10,\n                                                       mode='min',\n                                                       verbose=True,\n                                                       min_lr=5e-5)\nfrom early_stopping import EarlyStopping\nearly_stopping = EarlyStopping(70, verbose=True)   \n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:19:06.203568Z","iopub.execute_input":"2023-12-12T10:19:06.204363Z","iopub.status.idle":"2023-12-12T10:19:15.405378Z","shell.execute_reply.started":"2023-12-12T10:19:06.204331Z","shell.execute_reply":"2023-12-12T10:19:15.404388Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Distance_Estimator(\n  (rnn): LSTM(10, 612, num_layers=3, batch_first=True)\n  (fc): Sequential(\n    (0): Linear(in_features=612, out_features=306, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=306, out_features=154, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=154, out_features=76, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=76, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# train parameters\ndef count_parameter(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\ncount_parameter(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:19:25.897924Z","iopub.execute_input":"2023-12-12T10:19:25.898817Z","iopub.status.idle":"2023-12-12T10:19:25.908263Z","shell.execute_reply.started":"2023-12-12T10:19:25.898770Z","shell.execute_reply":"2023-12-12T10:19:25.907110Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"7776761"},"metadata":{}}]},{"cell_type":"markdown","source":"## Make Train, Valid function","metadata":{"id":"Q7pqhZ4a9y99"}},{"cell_type":"code","source":"def train(model, train_dataloader, idx_interval):\n    model.train()\n    \n    train_loss = 0\n    train_rmse = 0\n    \n    for idx, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        \n        inp = batch[0].reshape(len(batch[0]),1,-1)\n        \n        prediction = model(inp.to(device))\n        loss = loss_fn(prediction, batch[1].to(device)).cpu()\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n    \n        train_loss += loss.item()\n        if idx % idx_interval == 0:\n            print(\"Train Epoch: {} [{}/{}] \\t Train Loss(MAE): {:.4f} \\t Train RMAE: {:.4f}\".format(epoch, batch_sz*(idx+1), \\\n                                                                            len(train_dataloader)*batch_sz, \\\n                                                                            loss.item(), np.sqrt(loss.item())))\n    \n    train_loss /= len(train_dataloader)\n    train_rmse = np.sqrt(train_loss)\n        \n    return train_loss, train_rmse","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:19:51.763567Z","iopub.execute_input":"2023-12-12T10:19:51.764320Z","iopub.status.idle":"2023-12-12T10:19:51.772192Z","shell.execute_reply.started":"2023-12-12T10:19:51.764289Z","shell.execute_reply":"2023-12-12T10:19:51.771251Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, valid_dataloader):\n    model.eval()\n    \n    valid_loss = 0\n    valid_rmse = 0\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(valid_dataloader):\n            inp = batch[0].reshape(len(batch[0]),1,-1)\n            predictions = model(inp.to(device))\n            loss = loss_fn(predictions, batch[1].to(device)).cpu()\n            valid_loss += loss.item()\n            \n    valid_loss /= len(valid_dataloader)\n    valid_rmse = np.sqrt(valid_loss)\n    \n    return valid_loss,valid_rmse","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:19:57.048173Z","iopub.execute_input":"2023-12-12T10:19:57.048536Z","iopub.status.idle":"2023-12-12T10:19:57.055522Z","shell.execute_reply.started":"2023-12-12T10:19:57.048508Z","shell.execute_reply":"2023-12-12T10:19:57.054341Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Train and Validation","metadata":{}},{"cell_type":"code","source":"# training\nEpoch = 500\nbest_mae = 99999\nbest_train_mae = 99999\n\ntrain_mae_list = []\nvalid_mae_list = []\n\n\nfor epoch in range(1,(Epoch+1)):\n    train_mae, train_rmae = train(model, train_dataloader, 200)\n    valid_mae, valid_rmae = evaluate(model, valid_dataloader)\n\n    print(\"[Epoch: {} \\t Valid MAE: {:.4f}\".format(epoch, valid_mae))\n    print(\"[Epoch: {} \\t Train MAE: {:.4f}\".format(epoch, train_mae))\n    \n    scheduler.step(valid_mae)       \n    # Save model\n    if valid_mae < best_mae:\n        path = \"./weights/lstm_model.pth\"\n        torch.save(model.state_dict(), path)\n        best_mae = valid_mae\n        best_train_mae = train_mae\n        \n    train_mae_list.append(train_mae)\n    valid_mae_list.append(valid_mae)\n    \n    early_stopping(valid_mae, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-12T10:20:54.537609Z","iopub.execute_input":"2023-12-12T10:20:54.538005Z","iopub.status.idle":"2023-12-12T10:30:34.873062Z","shell.execute_reply.started":"2023-12-12T10:20:54.537972Z","shell.execute_reply":"2023-12-12T10:30:34.872052Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Train Epoch: 1 [64/26112] \t Train Loss(MAE): 2.3677 \t Train RMAE: 1.5387\nTrain Epoch: 1 [12864/26112] \t Train Loss(MAE): 4.1184 \t Train RMAE: 2.0294\nTrain Epoch: 1 [25664/26112] \t Train Loss(MAE): 2.0701 \t Train RMAE: 1.4388\n[Epoch: 1 \t Valid MAE: 2.2749\n[Epoch: 1 \t Train MAE: 2.4984\nValidation loss decreased (2.296333 --> 2.274882).  Saving model ...\nTrain Epoch: 2 [64/26112] \t Train Loss(MAE): 2.9629 \t Train RMAE: 1.7213\nTrain Epoch: 2 [12864/26112] \t Train Loss(MAE): 2.3949 \t Train RMAE: 1.5476\nTrain Epoch: 2 [25664/26112] \t Train Loss(MAE): 2.6888 \t Train RMAE: 1.6397\n[Epoch: 2 \t Valid MAE: 1.9878\n[Epoch: 2 \t Train MAE: 2.3645\nValidation loss decreased (2.274882 --> 1.987810).  Saving model ...\nTrain Epoch: 3 [64/26112] \t Train Loss(MAE): 1.9893 \t Train RMAE: 1.4104\nTrain Epoch: 3 [12864/26112] \t Train Loss(MAE): 3.0442 \t Train RMAE: 1.7448\nTrain Epoch: 3 [25664/26112] \t Train Loss(MAE): 1.4233 \t Train RMAE: 1.1930\n[Epoch: 3 \t Valid MAE: 2.1958\n[Epoch: 3 \t Train MAE: 2.2751\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 4 [64/26112] \t Train Loss(MAE): 1.6041 \t Train RMAE: 1.2665\nTrain Epoch: 4 [12864/26112] \t Train Loss(MAE): 1.7840 \t Train RMAE: 1.3357\nTrain Epoch: 4 [25664/26112] \t Train Loss(MAE): 1.9071 \t Train RMAE: 1.3810\n[Epoch: 4 \t Valid MAE: 2.4837\n[Epoch: 4 \t Train MAE: 2.2465\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 5 [64/26112] \t Train Loss(MAE): 2.2844 \t Train RMAE: 1.5114\nTrain Epoch: 5 [12864/26112] \t Train Loss(MAE): 2.3900 \t Train RMAE: 1.5460\nTrain Epoch: 5 [25664/26112] \t Train Loss(MAE): 1.9255 \t Train RMAE: 1.3876\n[Epoch: 5 \t Valid MAE: 1.8932\n[Epoch: 5 \t Train MAE: 2.2385\nValidation loss decreased (1.987810 --> 1.893212).  Saving model ...\nTrain Epoch: 6 [64/26112] \t Train Loss(MAE): 2.9375 \t Train RMAE: 1.7139\nTrain Epoch: 6 [12864/26112] \t Train Loss(MAE): 1.6476 \t Train RMAE: 1.2836\nTrain Epoch: 6 [25664/26112] \t Train Loss(MAE): 2.1861 \t Train RMAE: 1.4785\n[Epoch: 6 \t Valid MAE: 2.0646\n[Epoch: 6 \t Train MAE: 2.1809\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 7 [64/26112] \t Train Loss(MAE): 2.1151 \t Train RMAE: 1.4543\nTrain Epoch: 7 [12864/26112] \t Train Loss(MAE): 2.7054 \t Train RMAE: 1.6448\nTrain Epoch: 7 [25664/26112] \t Train Loss(MAE): 1.9241 \t Train RMAE: 1.3871\n[Epoch: 7 \t Valid MAE: 2.0725\n[Epoch: 7 \t Train MAE: 2.1771\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 8 [64/26112] \t Train Loss(MAE): 1.9548 \t Train RMAE: 1.3982\nTrain Epoch: 8 [12864/26112] \t Train Loss(MAE): 2.3797 \t Train RMAE: 1.5426\nTrain Epoch: 8 [25664/26112] \t Train Loss(MAE): 2.1471 \t Train RMAE: 1.4653\n[Epoch: 8 \t Valid MAE: 2.0658\n[Epoch: 8 \t Train MAE: 2.0630\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 9 [64/26112] \t Train Loss(MAE): 1.6697 \t Train RMAE: 1.2922\nTrain Epoch: 9 [12864/26112] \t Train Loss(MAE): 2.2598 \t Train RMAE: 1.5033\nTrain Epoch: 9 [25664/26112] \t Train Loss(MAE): 2.9627 \t Train RMAE: 1.7213\n[Epoch: 9 \t Valid MAE: 1.8819\n[Epoch: 9 \t Train MAE: 2.0223\nValidation loss decreased (1.893212 --> 1.881929).  Saving model ...\nTrain Epoch: 10 [64/26112] \t Train Loss(MAE): 2.0805 \t Train RMAE: 1.4424\nTrain Epoch: 10 [12864/26112] \t Train Loss(MAE): 1.6454 \t Train RMAE: 1.2827\nTrain Epoch: 10 [25664/26112] \t Train Loss(MAE): 2.5408 \t Train RMAE: 1.5940\n[Epoch: 10 \t Valid MAE: 1.8274\n[Epoch: 10 \t Train MAE: 1.9972\nValidation loss decreased (1.881929 --> 1.827389).  Saving model ...\nTrain Epoch: 11 [64/26112] \t Train Loss(MAE): 2.4282 \t Train RMAE: 1.5583\nTrain Epoch: 11 [12864/26112] \t Train Loss(MAE): 1.9340 \t Train RMAE: 1.3907\nTrain Epoch: 11 [25664/26112] \t Train Loss(MAE): 2.1617 \t Train RMAE: 1.4703\n[Epoch: 11 \t Valid MAE: 2.1667\n[Epoch: 11 \t Train MAE: 2.0033\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 12 [64/26112] \t Train Loss(MAE): 1.9217 \t Train RMAE: 1.3863\nTrain Epoch: 12 [12864/26112] \t Train Loss(MAE): 1.4777 \t Train RMAE: 1.2156\nTrain Epoch: 12 [25664/26112] \t Train Loss(MAE): 2.4773 \t Train RMAE: 1.5739\n[Epoch: 12 \t Valid MAE: 1.9822\n[Epoch: 12 \t Train MAE: 1.9990\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 13 [64/26112] \t Train Loss(MAE): 1.5502 \t Train RMAE: 1.2451\nTrain Epoch: 13 [12864/26112] \t Train Loss(MAE): 1.6271 \t Train RMAE: 1.2756\nTrain Epoch: 13 [25664/26112] \t Train Loss(MAE): 1.7859 \t Train RMAE: 1.3364\n[Epoch: 13 \t Valid MAE: 1.7427\n[Epoch: 13 \t Train MAE: 1.9877\nValidation loss decreased (1.827389 --> 1.742734).  Saving model ...\nTrain Epoch: 14 [64/26112] \t Train Loss(MAE): 1.7962 \t Train RMAE: 1.3402\nTrain Epoch: 14 [12864/26112] \t Train Loss(MAE): 1.9061 \t Train RMAE: 1.3806\nTrain Epoch: 14 [25664/26112] \t Train Loss(MAE): 2.7459 \t Train RMAE: 1.6571\n[Epoch: 14 \t Valid MAE: 1.9822\n[Epoch: 14 \t Train MAE: 1.9310\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 15 [64/26112] \t Train Loss(MAE): 1.6047 \t Train RMAE: 1.2667\nTrain Epoch: 15 [12864/26112] \t Train Loss(MAE): 2.0111 \t Train RMAE: 1.4181\nTrain Epoch: 15 [25664/26112] \t Train Loss(MAE): 1.7614 \t Train RMAE: 1.3272\n[Epoch: 15 \t Valid MAE: 1.7066\n[Epoch: 15 \t Train MAE: 1.9013\nValidation loss decreased (1.742734 --> 1.706582).  Saving model ...\nTrain Epoch: 16 [64/26112] \t Train Loss(MAE): 2.7284 \t Train RMAE: 1.6518\nTrain Epoch: 16 [12864/26112] \t Train Loss(MAE): 1.8811 \t Train RMAE: 1.3715\nTrain Epoch: 16 [25664/26112] \t Train Loss(MAE): 1.4395 \t Train RMAE: 1.1998\n[Epoch: 16 \t Valid MAE: 1.7530\n[Epoch: 16 \t Train MAE: 1.8871\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 17 [64/26112] \t Train Loss(MAE): 1.9580 \t Train RMAE: 1.3993\nTrain Epoch: 17 [12864/26112] \t Train Loss(MAE): 2.1450 \t Train RMAE: 1.4646\nTrain Epoch: 17 [25664/26112] \t Train Loss(MAE): 1.5818 \t Train RMAE: 1.2577\n[Epoch: 17 \t Valid MAE: 1.8326\n[Epoch: 17 \t Train MAE: 1.8965\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 18 [64/26112] \t Train Loss(MAE): 1.5487 \t Train RMAE: 1.2445\nTrain Epoch: 18 [12864/26112] \t Train Loss(MAE): 1.6903 \t Train RMAE: 1.3001\nTrain Epoch: 18 [25664/26112] \t Train Loss(MAE): 1.5586 \t Train RMAE: 1.2485\n[Epoch: 18 \t Valid MAE: 1.8759\n[Epoch: 18 \t Train MAE: 1.8795\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 19 [64/26112] \t Train Loss(MAE): 1.7687 \t Train RMAE: 1.3299\nTrain Epoch: 19 [12864/26112] \t Train Loss(MAE): 2.2054 \t Train RMAE: 1.4851\nTrain Epoch: 19 [25664/26112] \t Train Loss(MAE): 1.4575 \t Train RMAE: 1.2073\n[Epoch: 19 \t Valid MAE: 1.6372\n[Epoch: 19 \t Train MAE: 1.8290\nValidation loss decreased (1.706582 --> 1.637238).  Saving model ...\nTrain Epoch: 20 [64/26112] \t Train Loss(MAE): 1.3932 \t Train RMAE: 1.1803\nTrain Epoch: 20 [12864/26112] \t Train Loss(MAE): 1.8854 \t Train RMAE: 1.3731\nTrain Epoch: 20 [25664/26112] \t Train Loss(MAE): 1.8172 \t Train RMAE: 1.3480\n[Epoch: 20 \t Valid MAE: 1.7532\n[Epoch: 20 \t Train MAE: 1.8456\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 21 [64/26112] \t Train Loss(MAE): 1.8857 \t Train RMAE: 1.3732\nTrain Epoch: 21 [12864/26112] \t Train Loss(MAE): 2.4576 \t Train RMAE: 1.5677\nTrain Epoch: 21 [25664/26112] \t Train Loss(MAE): 1.6438 \t Train RMAE: 1.2821\n[Epoch: 21 \t Valid MAE: 1.6639\n[Epoch: 21 \t Train MAE: 1.8365\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 22 [64/26112] \t Train Loss(MAE): 1.2648 \t Train RMAE: 1.1246\nTrain Epoch: 22 [12864/26112] \t Train Loss(MAE): 1.4192 \t Train RMAE: 1.1913\nTrain Epoch: 22 [25664/26112] \t Train Loss(MAE): 1.6657 \t Train RMAE: 1.2906\n[Epoch: 22 \t Valid MAE: 1.5887\n[Epoch: 22 \t Train MAE: 1.8085\nValidation loss decreased (1.637238 --> 1.588715).  Saving model ...\nTrain Epoch: 23 [64/26112] \t Train Loss(MAE): 1.9908 \t Train RMAE: 1.4110\nTrain Epoch: 23 [12864/26112] \t Train Loss(MAE): 1.6776 \t Train RMAE: 1.2952\nTrain Epoch: 23 [25664/26112] \t Train Loss(MAE): 2.0364 \t Train RMAE: 1.4270\n[Epoch: 23 \t Valid MAE: 2.3774\n[Epoch: 23 \t Train MAE: 1.7720\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 24 [64/26112] \t Train Loss(MAE): 2.2834 \t Train RMAE: 1.5111\nTrain Epoch: 24 [12864/26112] \t Train Loss(MAE): 1.7408 \t Train RMAE: 1.3194\nTrain Epoch: 24 [25664/26112] \t Train Loss(MAE): 1.7886 \t Train RMAE: 1.3374\n[Epoch: 24 \t Valid MAE: 1.6598\n[Epoch: 24 \t Train MAE: 1.7419\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 25 [64/26112] \t Train Loss(MAE): 2.2300 \t Train RMAE: 1.4933\nTrain Epoch: 25 [12864/26112] \t Train Loss(MAE): 2.0696 \t Train RMAE: 1.4386\nTrain Epoch: 25 [25664/26112] \t Train Loss(MAE): 2.3774 \t Train RMAE: 1.5419\n[Epoch: 25 \t Valid MAE: 1.6563\n[Epoch: 25 \t Train MAE: 1.7872\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 26 [64/26112] \t Train Loss(MAE): 1.8728 \t Train RMAE: 1.3685\nTrain Epoch: 26 [12864/26112] \t Train Loss(MAE): 1.4081 \t Train RMAE: 1.1867\nTrain Epoch: 26 [25664/26112] \t Train Loss(MAE): 2.0069 \t Train RMAE: 1.4167\n[Epoch: 26 \t Valid MAE: 1.5907\n[Epoch: 26 \t Train MAE: 1.7163\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 27 [64/26112] \t Train Loss(MAE): 2.8048 \t Train RMAE: 1.6748\nTrain Epoch: 27 [12864/26112] \t Train Loss(MAE): 1.4613 \t Train RMAE: 1.2089\nTrain Epoch: 27 [25664/26112] \t Train Loss(MAE): 1.8680 \t Train RMAE: 1.3668\n[Epoch: 27 \t Valid MAE: 1.5471\n[Epoch: 27 \t Train MAE: 1.7423\nValidation loss decreased (1.588715 --> 1.547062).  Saving model ...\nTrain Epoch: 28 [64/26112] \t Train Loss(MAE): 1.9041 \t Train RMAE: 1.3799\nTrain Epoch: 28 [12864/26112] \t Train Loss(MAE): 3.0411 \t Train RMAE: 1.7439\nTrain Epoch: 28 [25664/26112] \t Train Loss(MAE): 1.9814 \t Train RMAE: 1.4076\n[Epoch: 28 \t Valid MAE: 1.5774\n[Epoch: 28 \t Train MAE: 1.7833\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 29 [64/26112] \t Train Loss(MAE): 1.4341 \t Train RMAE: 1.1975\nTrain Epoch: 29 [12864/26112] \t Train Loss(MAE): 1.7085 \t Train RMAE: 1.3071\nTrain Epoch: 29 [25664/26112] \t Train Loss(MAE): 1.2935 \t Train RMAE: 1.1373\n[Epoch: 29 \t Valid MAE: 2.1859\n[Epoch: 29 \t Train MAE: 1.7486\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 30 [64/26112] \t Train Loss(MAE): 1.8825 \t Train RMAE: 1.3720\nTrain Epoch: 30 [12864/26112] \t Train Loss(MAE): 1.9956 \t Train RMAE: 1.4127\nTrain Epoch: 30 [25664/26112] \t Train Loss(MAE): 1.4303 \t Train RMAE: 1.1959\n[Epoch: 30 \t Valid MAE: 1.7771\n[Epoch: 30 \t Train MAE: 1.7165\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 31 [64/26112] \t Train Loss(MAE): 1.7825 \t Train RMAE: 1.3351\nTrain Epoch: 31 [12864/26112] \t Train Loss(MAE): 1.6336 \t Train RMAE: 1.2781\nTrain Epoch: 31 [25664/26112] \t Train Loss(MAE): 1.4744 \t Train RMAE: 1.2143\n[Epoch: 31 \t Valid MAE: 1.5511\n[Epoch: 31 \t Train MAE: 1.6551\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 32 [64/26112] \t Train Loss(MAE): 1.3190 \t Train RMAE: 1.1485\nTrain Epoch: 32 [12864/26112] \t Train Loss(MAE): 1.6805 \t Train RMAE: 1.2963\nTrain Epoch: 32 [25664/26112] \t Train Loss(MAE): 1.1602 \t Train RMAE: 1.0771\n[Epoch: 32 \t Valid MAE: 1.5718\n[Epoch: 32 \t Train MAE: 1.6772\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 33 [64/26112] \t Train Loss(MAE): 1.1615 \t Train RMAE: 1.0777\nTrain Epoch: 33 [12864/26112] \t Train Loss(MAE): 2.2410 \t Train RMAE: 1.4970\nTrain Epoch: 33 [25664/26112] \t Train Loss(MAE): 2.2317 \t Train RMAE: 1.4939\n[Epoch: 33 \t Valid MAE: 1.5163\n[Epoch: 33 \t Train MAE: 1.6946\nValidation loss decreased (1.547062 --> 1.516275).  Saving model ...\nTrain Epoch: 34 [64/26112] \t Train Loss(MAE): 2.2404 \t Train RMAE: 1.4968\nTrain Epoch: 34 [12864/26112] \t Train Loss(MAE): 2.6279 \t Train RMAE: 1.6211\nTrain Epoch: 34 [25664/26112] \t Train Loss(MAE): 2.1264 \t Train RMAE: 1.4582\n[Epoch: 34 \t Valid MAE: 1.6052\n[Epoch: 34 \t Train MAE: 1.6588\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 35 [64/26112] \t Train Loss(MAE): 1.8110 \t Train RMAE: 1.3457\nTrain Epoch: 35 [12864/26112] \t Train Loss(MAE): 1.8116 \t Train RMAE: 1.3459\nTrain Epoch: 35 [25664/26112] \t Train Loss(MAE): 1.2097 \t Train RMAE: 1.0999\n[Epoch: 35 \t Valid MAE: 1.6923\n[Epoch: 35 \t Train MAE: 1.6673\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 36 [64/26112] \t Train Loss(MAE): 1.3216 \t Train RMAE: 1.1496\nTrain Epoch: 36 [12864/26112] \t Train Loss(MAE): 1.3293 \t Train RMAE: 1.1530\nTrain Epoch: 36 [25664/26112] \t Train Loss(MAE): 1.7854 \t Train RMAE: 1.3362\n[Epoch: 36 \t Valid MAE: 1.5243\n[Epoch: 36 \t Train MAE: 1.6494\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 37 [64/26112] \t Train Loss(MAE): 1.2108 \t Train RMAE: 1.1004\nTrain Epoch: 37 [12864/26112] \t Train Loss(MAE): 1.4640 \t Train RMAE: 1.2099\nTrain Epoch: 37 [25664/26112] \t Train Loss(MAE): 1.2388 \t Train RMAE: 1.1130\n[Epoch: 37 \t Valid MAE: 1.6251\n[Epoch: 37 \t Train MAE: 1.6446\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 38 [64/26112] \t Train Loss(MAE): 2.1765 \t Train RMAE: 1.4753\nTrain Epoch: 38 [12864/26112] \t Train Loss(MAE): 1.1555 \t Train RMAE: 1.0750\nTrain Epoch: 38 [25664/26112] \t Train Loss(MAE): 2.1578 \t Train RMAE: 1.4689\n[Epoch: 38 \t Valid MAE: 1.4875\n[Epoch: 38 \t Train MAE: 1.6359\nValidation loss decreased (1.516275 --> 1.487526).  Saving model ...\nTrain Epoch: 39 [64/26112] \t Train Loss(MAE): 1.3456 \t Train RMAE: 1.1600\nTrain Epoch: 39 [12864/26112] \t Train Loss(MAE): 1.6836 \t Train RMAE: 1.2976\nTrain Epoch: 39 [25664/26112] \t Train Loss(MAE): 1.6612 \t Train RMAE: 1.2889\n[Epoch: 39 \t Valid MAE: 1.7510\n[Epoch: 39 \t Train MAE: 1.6271\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 40 [64/26112] \t Train Loss(MAE): 1.8231 \t Train RMAE: 1.3502\nTrain Epoch: 40 [12864/26112] \t Train Loss(MAE): 1.7944 \t Train RMAE: 1.3395\nTrain Epoch: 40 [25664/26112] \t Train Loss(MAE): 1.6549 \t Train RMAE: 1.2864\n[Epoch: 40 \t Valid MAE: 1.5319\n[Epoch: 40 \t Train MAE: 1.6209\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 41 [64/26112] \t Train Loss(MAE): 2.0158 \t Train RMAE: 1.4198\nTrain Epoch: 41 [12864/26112] \t Train Loss(MAE): 1.4244 \t Train RMAE: 1.1935\nTrain Epoch: 41 [25664/26112] \t Train Loss(MAE): 1.2496 \t Train RMAE: 1.1179\n[Epoch: 41 \t Valid MAE: 1.5538\n[Epoch: 41 \t Train MAE: 1.5937\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 42 [64/26112] \t Train Loss(MAE): 1.9232 \t Train RMAE: 1.3868\nTrain Epoch: 42 [12864/26112] \t Train Loss(MAE): 1.5205 \t Train RMAE: 1.2331\nTrain Epoch: 42 [25664/26112] \t Train Loss(MAE): 1.5391 \t Train RMAE: 1.2406\n[Epoch: 42 \t Valid MAE: 1.5531\n[Epoch: 42 \t Train MAE: 1.5813\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 43 [64/26112] \t Train Loss(MAE): 1.6250 \t Train RMAE: 1.2748\nTrain Epoch: 43 [12864/26112] \t Train Loss(MAE): 1.3696 \t Train RMAE: 1.1703\nTrain Epoch: 43 [25664/26112] \t Train Loss(MAE): 1.8191 \t Train RMAE: 1.3487\n[Epoch: 43 \t Valid MAE: 1.8175\n[Epoch: 43 \t Train MAE: 1.5851\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 44 [64/26112] \t Train Loss(MAE): 1.6942 \t Train RMAE: 1.3016\nTrain Epoch: 44 [12864/26112] \t Train Loss(MAE): 1.1945 \t Train RMAE: 1.0929\nTrain Epoch: 44 [25664/26112] \t Train Loss(MAE): 1.0836 \t Train RMAE: 1.0410\n[Epoch: 44 \t Valid MAE: 1.5022\n[Epoch: 44 \t Train MAE: 1.5833\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 45 [64/26112] \t Train Loss(MAE): 1.3894 \t Train RMAE: 1.1787\nTrain Epoch: 45 [12864/26112] \t Train Loss(MAE): 1.6491 \t Train RMAE: 1.2842\nTrain Epoch: 45 [25664/26112] \t Train Loss(MAE): 1.7043 \t Train RMAE: 1.3055\n[Epoch: 45 \t Valid MAE: 1.5283\n[Epoch: 45 \t Train MAE: 1.5843\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 46 [64/26112] \t Train Loss(MAE): 1.3833 \t Train RMAE: 1.1761\nTrain Epoch: 46 [12864/26112] \t Train Loss(MAE): 1.3374 \t Train RMAE: 1.1565\nTrain Epoch: 46 [25664/26112] \t Train Loss(MAE): 1.8782 \t Train RMAE: 1.3705\n[Epoch: 46 \t Valid MAE: 1.4670\n[Epoch: 46 \t Train MAE: 1.5566\nValidation loss decreased (1.487526 --> 1.467031).  Saving model ...\nTrain Epoch: 47 [64/26112] \t Train Loss(MAE): 1.5355 \t Train RMAE: 1.2392\nTrain Epoch: 47 [12864/26112] \t Train Loss(MAE): 1.3093 \t Train RMAE: 1.1443\nTrain Epoch: 47 [25664/26112] \t Train Loss(MAE): 1.5674 \t Train RMAE: 1.2520\n[Epoch: 47 \t Valid MAE: 1.5147\n[Epoch: 47 \t Train MAE: 1.5433\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 48 [64/26112] \t Train Loss(MAE): 1.5378 \t Train RMAE: 1.2401\nTrain Epoch: 48 [12864/26112] \t Train Loss(MAE): 1.6844 \t Train RMAE: 1.2979\nTrain Epoch: 48 [25664/26112] \t Train Loss(MAE): 1.2417 \t Train RMAE: 1.1143\n[Epoch: 48 \t Valid MAE: 1.4547\n[Epoch: 48 \t Train MAE: 1.5299\nValidation loss decreased (1.467031 --> 1.454736).  Saving model ...\nTrain Epoch: 49 [64/26112] \t Train Loss(MAE): 1.1105 \t Train RMAE: 1.0538\nTrain Epoch: 49 [12864/26112] \t Train Loss(MAE): 1.3408 \t Train RMAE: 1.1579\nTrain Epoch: 49 [25664/26112] \t Train Loss(MAE): 1.2709 \t Train RMAE: 1.1273\n[Epoch: 49 \t Valid MAE: 1.5279\n[Epoch: 49 \t Train MAE: 1.5397\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 50 [64/26112] \t Train Loss(MAE): 1.5883 \t Train RMAE: 1.2603\nTrain Epoch: 50 [12864/26112] \t Train Loss(MAE): 1.4026 \t Train RMAE: 1.1843\nTrain Epoch: 50 [25664/26112] \t Train Loss(MAE): 1.2048 \t Train RMAE: 1.0976\n[Epoch: 50 \t Valid MAE: 1.4488\n[Epoch: 50 \t Train MAE: 1.5173\nValidation loss decreased (1.454736 --> 1.448820).  Saving model ...\nTrain Epoch: 51 [64/26112] \t Train Loss(MAE): 1.3020 \t Train RMAE: 1.1411\nTrain Epoch: 51 [12864/26112] \t Train Loss(MAE): 0.9298 \t Train RMAE: 0.9643\nTrain Epoch: 51 [25664/26112] \t Train Loss(MAE): 0.8891 \t Train RMAE: 0.9429\n[Epoch: 51 \t Valid MAE: 1.4640\n[Epoch: 51 \t Train MAE: 1.5115\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 52 [64/26112] \t Train Loss(MAE): 1.6027 \t Train RMAE: 1.2660\nTrain Epoch: 52 [12864/26112] \t Train Loss(MAE): 1.3044 \t Train RMAE: 1.1421\nTrain Epoch: 52 [25664/26112] \t Train Loss(MAE): 1.0608 \t Train RMAE: 1.0299\n[Epoch: 52 \t Valid MAE: 1.4794\n[Epoch: 52 \t Train MAE: 1.4847\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 53 [64/26112] \t Train Loss(MAE): 0.9475 \t Train RMAE: 0.9734\nTrain Epoch: 53 [12864/26112] \t Train Loss(MAE): 1.2393 \t Train RMAE: 1.1132\nTrain Epoch: 53 [25664/26112] \t Train Loss(MAE): 1.2000 \t Train RMAE: 1.0955\n[Epoch: 53 \t Valid MAE: 1.7533\n[Epoch: 53 \t Train MAE: 1.4900\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 54 [64/26112] \t Train Loss(MAE): 1.3754 \t Train RMAE: 1.1728\nTrain Epoch: 54 [12864/26112] \t Train Loss(MAE): 1.3360 \t Train RMAE: 1.1559\nTrain Epoch: 54 [25664/26112] \t Train Loss(MAE): 1.3841 \t Train RMAE: 1.1765\n[Epoch: 54 \t Valid MAE: 1.4032\n[Epoch: 54 \t Train MAE: 1.4711\nValidation loss decreased (1.448820 --> 1.403245).  Saving model ...\nTrain Epoch: 55 [64/26112] \t Train Loss(MAE): 0.8802 \t Train RMAE: 0.9382\nTrain Epoch: 55 [12864/26112] \t Train Loss(MAE): 1.4873 \t Train RMAE: 1.2195\nTrain Epoch: 55 [25664/26112] \t Train Loss(MAE): 1.2269 \t Train RMAE: 1.1076\n[Epoch: 55 \t Valid MAE: 1.5522\n[Epoch: 55 \t Train MAE: 1.4897\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 56 [64/26112] \t Train Loss(MAE): 1.0127 \t Train RMAE: 1.0063\nTrain Epoch: 56 [12864/26112] \t Train Loss(MAE): 1.5124 \t Train RMAE: 1.2298\nTrain Epoch: 56 [25664/26112] \t Train Loss(MAE): 1.0217 \t Train RMAE: 1.0108\n[Epoch: 56 \t Valid MAE: 1.4151\n[Epoch: 56 \t Train MAE: 1.4656\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 57 [64/26112] \t Train Loss(MAE): 0.9404 \t Train RMAE: 0.9697\nTrain Epoch: 57 [12864/26112] \t Train Loss(MAE): 1.3075 \t Train RMAE: 1.1435\nTrain Epoch: 57 [25664/26112] \t Train Loss(MAE): 2.1829 \t Train RMAE: 1.4775\n[Epoch: 57 \t Valid MAE: 1.4376\n[Epoch: 57 \t Train MAE: 1.4777\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 58 [64/26112] \t Train Loss(MAE): 1.1714 \t Train RMAE: 1.0823\nTrain Epoch: 58 [12864/26112] \t Train Loss(MAE): 0.9806 \t Train RMAE: 0.9903\nTrain Epoch: 58 [25664/26112] \t Train Loss(MAE): 1.3120 \t Train RMAE: 1.1454\n[Epoch: 58 \t Valid MAE: 1.5943\n[Epoch: 58 \t Train MAE: 1.4596\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 59 [64/26112] \t Train Loss(MAE): 1.3193 \t Train RMAE: 1.1486\nTrain Epoch: 59 [12864/26112] \t Train Loss(MAE): 2.2337 \t Train RMAE: 1.4945\nTrain Epoch: 59 [25664/26112] \t Train Loss(MAE): 1.5256 \t Train RMAE: 1.2352\n[Epoch: 59 \t Valid MAE: 1.4387\n[Epoch: 59 \t Train MAE: 1.4583\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 60 [64/26112] \t Train Loss(MAE): 0.8603 \t Train RMAE: 0.9275\nTrain Epoch: 60 [12864/26112] \t Train Loss(MAE): 1.7269 \t Train RMAE: 1.3141\nTrain Epoch: 60 [25664/26112] \t Train Loss(MAE): 1.4835 \t Train RMAE: 1.2180\n[Epoch: 60 \t Valid MAE: 1.5028\n[Epoch: 60 \t Train MAE: 1.4210\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 61 [64/26112] \t Train Loss(MAE): 1.6950 \t Train RMAE: 1.3019\nTrain Epoch: 61 [12864/26112] \t Train Loss(MAE): 1.1384 \t Train RMAE: 1.0669\nTrain Epoch: 61 [25664/26112] \t Train Loss(MAE): 1.2022 \t Train RMAE: 1.0964\n[Epoch: 61 \t Valid MAE: 1.4699\n[Epoch: 61 \t Train MAE: 1.4316\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 62 [64/26112] \t Train Loss(MAE): 1.2290 \t Train RMAE: 1.1086\nTrain Epoch: 62 [12864/26112] \t Train Loss(MAE): 1.9191 \t Train RMAE: 1.3853\nTrain Epoch: 62 [25664/26112] \t Train Loss(MAE): 1.2933 \t Train RMAE: 1.1372\n[Epoch: 62 \t Valid MAE: 1.4992\n[Epoch: 62 \t Train MAE: 1.4519\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 63 [64/26112] \t Train Loss(MAE): 1.0393 \t Train RMAE: 1.0194\nTrain Epoch: 63 [12864/26112] \t Train Loss(MAE): 1.6448 \t Train RMAE: 1.2825\nTrain Epoch: 63 [25664/26112] \t Train Loss(MAE): 1.5994 \t Train RMAE: 1.2647\n[Epoch: 63 \t Valid MAE: 1.4528\n[Epoch: 63 \t Train MAE: 1.4667\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 64 [64/26112] \t Train Loss(MAE): 1.4981 \t Train RMAE: 1.2240\nTrain Epoch: 64 [12864/26112] \t Train Loss(MAE): 1.8196 \t Train RMAE: 1.3489\nTrain Epoch: 64 [25664/26112] \t Train Loss(MAE): 1.2760 \t Train RMAE: 1.1296\n[Epoch: 64 \t Valid MAE: 1.4914\n[Epoch: 64 \t Train MAE: 1.3907\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 65 [64/26112] \t Train Loss(MAE): 1.3908 \t Train RMAE: 1.1793\nTrain Epoch: 65 [12864/26112] \t Train Loss(MAE): 0.9350 \t Train RMAE: 0.9669\nTrain Epoch: 65 [25664/26112] \t Train Loss(MAE): 1.2779 \t Train RMAE: 1.1304\n[Epoch: 65 \t Valid MAE: 1.5786\n[Epoch: 65 \t Train MAE: 1.4100\nEpoch 00066: reducing learning rate of group 0 to 2.5000e-03.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 66 [64/26112] \t Train Loss(MAE): 1.5658 \t Train RMAE: 1.2513\nTrain Epoch: 66 [12864/26112] \t Train Loss(MAE): 1.0897 \t Train RMAE: 1.0439\nTrain Epoch: 66 [25664/26112] \t Train Loss(MAE): 1.4928 \t Train RMAE: 1.2218\n[Epoch: 66 \t Valid MAE: 1.3589\n[Epoch: 66 \t Train MAE: 1.3027\nValidation loss decreased (1.403245 --> 1.358862).  Saving model ...\nTrain Epoch: 67 [64/26112] \t Train Loss(MAE): 1.1876 \t Train RMAE: 1.0898\nTrain Epoch: 67 [12864/26112] \t Train Loss(MAE): 0.9324 \t Train RMAE: 0.9656\nTrain Epoch: 67 [25664/26112] \t Train Loss(MAE): 0.8891 \t Train RMAE: 0.9429\n[Epoch: 67 \t Valid MAE: 1.3575\n[Epoch: 67 \t Train MAE: 1.2788\nValidation loss decreased (1.358862 --> 1.357496).  Saving model ...\nTrain Epoch: 68 [64/26112] \t Train Loss(MAE): 1.1432 \t Train RMAE: 1.0692\nTrain Epoch: 68 [12864/26112] \t Train Loss(MAE): 1.0514 \t Train RMAE: 1.0254\nTrain Epoch: 68 [25664/26112] \t Train Loss(MAE): 1.4076 \t Train RMAE: 1.1864\n[Epoch: 68 \t Valid MAE: 1.3247\n[Epoch: 68 \t Train MAE: 1.2757\nValidation loss decreased (1.357496 --> 1.324742).  Saving model ...\nTrain Epoch: 69 [64/26112] \t Train Loss(MAE): 1.4183 \t Train RMAE: 1.1909\nTrain Epoch: 69 [12864/26112] \t Train Loss(MAE): 1.1459 \t Train RMAE: 1.0705\nTrain Epoch: 69 [25664/26112] \t Train Loss(MAE): 0.7292 \t Train RMAE: 0.8539\n[Epoch: 69 \t Valid MAE: 1.3268\n[Epoch: 69 \t Train MAE: 1.2786\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 70 [64/26112] \t Train Loss(MAE): 1.0635 \t Train RMAE: 1.0313\nTrain Epoch: 70 [12864/26112] \t Train Loss(MAE): 1.4627 \t Train RMAE: 1.2094\nTrain Epoch: 70 [25664/26112] \t Train Loss(MAE): 0.9950 \t Train RMAE: 0.9975\n[Epoch: 70 \t Valid MAE: 1.3885\n[Epoch: 70 \t Train MAE: 1.2626\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 71 [64/26112] \t Train Loss(MAE): 0.9494 \t Train RMAE: 0.9744\nTrain Epoch: 71 [12864/26112] \t Train Loss(MAE): 2.0814 \t Train RMAE: 1.4427\nTrain Epoch: 71 [25664/26112] \t Train Loss(MAE): 1.0440 \t Train RMAE: 1.0218\n[Epoch: 71 \t Valid MAE: 1.3229\n[Epoch: 71 \t Train MAE: 1.2483\nValidation loss decreased (1.324742 --> 1.322872).  Saving model ...\nTrain Epoch: 72 [64/26112] \t Train Loss(MAE): 0.7935 \t Train RMAE: 0.8908\nTrain Epoch: 72 [12864/26112] \t Train Loss(MAE): 1.5370 \t Train RMAE: 1.2398\nTrain Epoch: 72 [25664/26112] \t Train Loss(MAE): 1.8878 \t Train RMAE: 1.3740\n[Epoch: 72 \t Valid MAE: 1.3159\n[Epoch: 72 \t Train MAE: 1.2496\nValidation loss decreased (1.322872 --> 1.315877).  Saving model ...\nTrain Epoch: 73 [64/26112] \t Train Loss(MAE): 1.2059 \t Train RMAE: 1.0981\nTrain Epoch: 73 [12864/26112] \t Train Loss(MAE): 0.8544 \t Train RMAE: 0.9243\nTrain Epoch: 73 [25664/26112] \t Train Loss(MAE): 1.4747 \t Train RMAE: 1.2144\n[Epoch: 73 \t Valid MAE: 1.3291\n[Epoch: 73 \t Train MAE: 1.2509\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 74 [64/26112] \t Train Loss(MAE): 1.4476 \t Train RMAE: 1.2032\nTrain Epoch: 74 [12864/26112] \t Train Loss(MAE): 1.4188 \t Train RMAE: 1.1911\nTrain Epoch: 74 [25664/26112] \t Train Loss(MAE): 0.9828 \t Train RMAE: 0.9914\n[Epoch: 74 \t Valid MAE: 1.3295\n[Epoch: 74 \t Train MAE: 1.2474\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 75 [64/26112] \t Train Loss(MAE): 1.3871 \t Train RMAE: 1.1778\nTrain Epoch: 75 [12864/26112] \t Train Loss(MAE): 0.9789 \t Train RMAE: 0.9894\nTrain Epoch: 75 [25664/26112] \t Train Loss(MAE): 1.3226 \t Train RMAE: 1.1500\n[Epoch: 75 \t Valid MAE: 1.3282\n[Epoch: 75 \t Train MAE: 1.2391\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 76 [64/26112] \t Train Loss(MAE): 1.2164 \t Train RMAE: 1.1029\nTrain Epoch: 76 [12864/26112] \t Train Loss(MAE): 1.2501 \t Train RMAE: 1.1181\nTrain Epoch: 76 [25664/26112] \t Train Loss(MAE): 0.9280 \t Train RMAE: 0.9633\n[Epoch: 76 \t Valid MAE: 1.3335\n[Epoch: 76 \t Train MAE: 1.2304\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 77 [64/26112] \t Train Loss(MAE): 1.3922 \t Train RMAE: 1.1799\nTrain Epoch: 77 [12864/26112] \t Train Loss(MAE): 1.8418 \t Train RMAE: 1.3571\nTrain Epoch: 77 [25664/26112] \t Train Loss(MAE): 1.1619 \t Train RMAE: 1.0779\n[Epoch: 77 \t Valid MAE: 1.3230\n[Epoch: 77 \t Train MAE: 1.2245\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 78 [64/26112] \t Train Loss(MAE): 1.1163 \t Train RMAE: 1.0566\nTrain Epoch: 78 [12864/26112] \t Train Loss(MAE): 1.4090 \t Train RMAE: 1.1870\nTrain Epoch: 78 [25664/26112] \t Train Loss(MAE): 1.2073 \t Train RMAE: 1.0988\n[Epoch: 78 \t Valid MAE: 1.3086\n[Epoch: 78 \t Train MAE: 1.2221\nValidation loss decreased (1.315877 --> 1.308557).  Saving model ...\nTrain Epoch: 79 [64/26112] \t Train Loss(MAE): 0.8770 \t Train RMAE: 0.9365\nTrain Epoch: 79 [12864/26112] \t Train Loss(MAE): 1.1752 \t Train RMAE: 1.0840\nTrain Epoch: 79 [25664/26112] \t Train Loss(MAE): 1.2370 \t Train RMAE: 1.1122\n[Epoch: 79 \t Valid MAE: 1.3312\n[Epoch: 79 \t Train MAE: 1.2244\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 80 [64/26112] \t Train Loss(MAE): 1.1593 \t Train RMAE: 1.0767\nTrain Epoch: 80 [12864/26112] \t Train Loss(MAE): 1.2061 \t Train RMAE: 1.0982\nTrain Epoch: 80 [25664/26112] \t Train Loss(MAE): 1.0926 \t Train RMAE: 1.0453\n[Epoch: 80 \t Valid MAE: 1.3203\n[Epoch: 80 \t Train MAE: 1.2141\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 81 [64/26112] \t Train Loss(MAE): 0.8070 \t Train RMAE: 0.8983\nTrain Epoch: 81 [12864/26112] \t Train Loss(MAE): 1.2498 \t Train RMAE: 1.1179\nTrain Epoch: 81 [25664/26112] \t Train Loss(MAE): 1.6365 \t Train RMAE: 1.2793\n[Epoch: 81 \t Valid MAE: 1.3177\n[Epoch: 81 \t Train MAE: 1.2078\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 82 [64/26112] \t Train Loss(MAE): 1.8720 \t Train RMAE: 1.3682\nTrain Epoch: 82 [12864/26112] \t Train Loss(MAE): 0.7893 \t Train RMAE: 0.8884\nTrain Epoch: 82 [25664/26112] \t Train Loss(MAE): 1.2670 \t Train RMAE: 1.1256\n[Epoch: 82 \t Valid MAE: 1.3292\n[Epoch: 82 \t Train MAE: 1.2036\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 83 [64/26112] \t Train Loss(MAE): 1.0794 \t Train RMAE: 1.0389\nTrain Epoch: 83 [12864/26112] \t Train Loss(MAE): 1.0139 \t Train RMAE: 1.0069\nTrain Epoch: 83 [25664/26112] \t Train Loss(MAE): 1.1529 \t Train RMAE: 1.0737\n[Epoch: 83 \t Valid MAE: 1.2968\n[Epoch: 83 \t Train MAE: 1.1961\nValidation loss decreased (1.308557 --> 1.296842).  Saving model ...\nTrain Epoch: 84 [64/26112] \t Train Loss(MAE): 1.4793 \t Train RMAE: 1.2163\nTrain Epoch: 84 [12864/26112] \t Train Loss(MAE): 1.1520 \t Train RMAE: 1.0733\nTrain Epoch: 84 [25664/26112] \t Train Loss(MAE): 0.9979 \t Train RMAE: 0.9989\n[Epoch: 84 \t Valid MAE: 1.4161\n[Epoch: 84 \t Train MAE: 1.1946\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 85 [64/26112] \t Train Loss(MAE): 1.4380 \t Train RMAE: 1.1992\nTrain Epoch: 85 [12864/26112] \t Train Loss(MAE): 1.1631 \t Train RMAE: 1.0785\nTrain Epoch: 85 [25664/26112] \t Train Loss(MAE): 1.0526 \t Train RMAE: 1.0260\n[Epoch: 85 \t Valid MAE: 1.3419\n[Epoch: 85 \t Train MAE: 1.1897\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 86 [64/26112] \t Train Loss(MAE): 1.2142 \t Train RMAE: 1.1019\nTrain Epoch: 86 [12864/26112] \t Train Loss(MAE): 1.2435 \t Train RMAE: 1.1151\nTrain Epoch: 86 [25664/26112] \t Train Loss(MAE): 1.2642 \t Train RMAE: 1.1244\n[Epoch: 86 \t Valid MAE: 1.3085\n[Epoch: 86 \t Train MAE: 1.1843\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 87 [64/26112] \t Train Loss(MAE): 1.4539 \t Train RMAE: 1.2058\nTrain Epoch: 87 [12864/26112] \t Train Loss(MAE): 1.0384 \t Train RMAE: 1.0190\nTrain Epoch: 87 [25664/26112] \t Train Loss(MAE): 1.3753 \t Train RMAE: 1.1727\n[Epoch: 87 \t Valid MAE: 1.3588\n[Epoch: 87 \t Train MAE: 1.1829\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 88 [64/26112] \t Train Loss(MAE): 1.7244 \t Train RMAE: 1.3132\nTrain Epoch: 88 [12864/26112] \t Train Loss(MAE): 1.3353 \t Train RMAE: 1.1555\nTrain Epoch: 88 [25664/26112] \t Train Loss(MAE): 1.0434 \t Train RMAE: 1.0215\n[Epoch: 88 \t Valid MAE: 1.3131\n[Epoch: 88 \t Train MAE: 1.1841\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 89 [64/26112] \t Train Loss(MAE): 1.2142 \t Train RMAE: 1.1019\nTrain Epoch: 89 [12864/26112] \t Train Loss(MAE): 1.1446 \t Train RMAE: 1.0699\nTrain Epoch: 89 [25664/26112] \t Train Loss(MAE): 1.0763 \t Train RMAE: 1.0375\n[Epoch: 89 \t Valid MAE: 1.3272\n[Epoch: 89 \t Train MAE: 1.1631\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 90 [64/26112] \t Train Loss(MAE): 1.1295 \t Train RMAE: 1.0628\nTrain Epoch: 90 [12864/26112] \t Train Loss(MAE): 1.0516 \t Train RMAE: 1.0255\nTrain Epoch: 90 [25664/26112] \t Train Loss(MAE): 1.9458 \t Train RMAE: 1.3949\n[Epoch: 90 \t Valid MAE: 1.2693\n[Epoch: 90 \t Train MAE: 1.1706\nValidation loss decreased (1.296842 --> 1.269268).  Saving model ...\nTrain Epoch: 91 [64/26112] \t Train Loss(MAE): 1.0987 \t Train RMAE: 1.0482\nTrain Epoch: 91 [12864/26112] \t Train Loss(MAE): 0.9967 \t Train RMAE: 0.9983\nTrain Epoch: 91 [25664/26112] \t Train Loss(MAE): 1.4578 \t Train RMAE: 1.2074\n[Epoch: 91 \t Valid MAE: 1.3652\n[Epoch: 91 \t Train MAE: 1.1616\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 92 [64/26112] \t Train Loss(MAE): 1.4165 \t Train RMAE: 1.1902\nTrain Epoch: 92 [12864/26112] \t Train Loss(MAE): 0.7077 \t Train RMAE: 0.8413\nTrain Epoch: 92 [25664/26112] \t Train Loss(MAE): 1.0213 \t Train RMAE: 1.0106\n[Epoch: 92 \t Valid MAE: 1.3254\n[Epoch: 92 \t Train MAE: 1.1671\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 93 [64/26112] \t Train Loss(MAE): 1.2804 \t Train RMAE: 1.1315\nTrain Epoch: 93 [12864/26112] \t Train Loss(MAE): 0.8333 \t Train RMAE: 0.9129\nTrain Epoch: 93 [25664/26112] \t Train Loss(MAE): 1.0244 \t Train RMAE: 1.0121\n[Epoch: 93 \t Valid MAE: 1.3105\n[Epoch: 93 \t Train MAE: 1.1490\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 94 [64/26112] \t Train Loss(MAE): 1.2113 \t Train RMAE: 1.1006\nTrain Epoch: 94 [12864/26112] \t Train Loss(MAE): 1.0937 \t Train RMAE: 1.0458\nTrain Epoch: 94 [25664/26112] \t Train Loss(MAE): 1.6726 \t Train RMAE: 1.2933\n[Epoch: 94 \t Valid MAE: 1.2898\n[Epoch: 94 \t Train MAE: 1.1460\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 95 [64/26112] \t Train Loss(MAE): 0.6368 \t Train RMAE: 0.7980\nTrain Epoch: 95 [12864/26112] \t Train Loss(MAE): 0.7209 \t Train RMAE: 0.8491\nTrain Epoch: 95 [25664/26112] \t Train Loss(MAE): 1.2281 \t Train RMAE: 1.1082\n[Epoch: 95 \t Valid MAE: 1.3081\n[Epoch: 95 \t Train MAE: 1.1457\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 96 [64/26112] \t Train Loss(MAE): 1.1570 \t Train RMAE: 1.0756\nTrain Epoch: 96 [12864/26112] \t Train Loss(MAE): 1.1149 \t Train RMAE: 1.0559\nTrain Epoch: 96 [25664/26112] \t Train Loss(MAE): 0.9474 \t Train RMAE: 0.9733\n[Epoch: 96 \t Valid MAE: 1.2776\n[Epoch: 96 \t Train MAE: 1.1361\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 97 [64/26112] \t Train Loss(MAE): 0.9251 \t Train RMAE: 0.9618\nTrain Epoch: 97 [12864/26112] \t Train Loss(MAE): 0.9369 \t Train RMAE: 0.9679\nTrain Epoch: 97 [25664/26112] \t Train Loss(MAE): 1.0945 \t Train RMAE: 1.0462\n[Epoch: 97 \t Valid MAE: 1.2777\n[Epoch: 97 \t Train MAE: 1.1345\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 98 [64/26112] \t Train Loss(MAE): 1.2813 \t Train RMAE: 1.1319\nTrain Epoch: 98 [12864/26112] \t Train Loss(MAE): 1.2561 \t Train RMAE: 1.1207\nTrain Epoch: 98 [25664/26112] \t Train Loss(MAE): 1.4269 \t Train RMAE: 1.1945\n[Epoch: 98 \t Valid MAE: 1.3132\n[Epoch: 98 \t Train MAE: 1.1407\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 99 [64/26112] \t Train Loss(MAE): 1.1172 \t Train RMAE: 1.0570\nTrain Epoch: 99 [12864/26112] \t Train Loss(MAE): 1.4249 \t Train RMAE: 1.1937\nTrain Epoch: 99 [25664/26112] \t Train Loss(MAE): 1.1009 \t Train RMAE: 1.0493\n[Epoch: 99 \t Valid MAE: 1.3453\n[Epoch: 99 \t Train MAE: 1.1343\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 100 [64/26112] \t Train Loss(MAE): 0.8580 \t Train RMAE: 0.9263\nTrain Epoch: 100 [12864/26112] \t Train Loss(MAE): 0.9806 \t Train RMAE: 0.9902\nTrain Epoch: 100 [25664/26112] \t Train Loss(MAE): 1.1129 \t Train RMAE: 1.0549\n[Epoch: 100 \t Valid MAE: 1.2703\n[Epoch: 100 \t Train MAE: 1.1283\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 101 [64/26112] \t Train Loss(MAE): 0.8841 \t Train RMAE: 0.9403\nTrain Epoch: 101 [12864/26112] \t Train Loss(MAE): 0.9034 \t Train RMAE: 0.9505\nTrain Epoch: 101 [25664/26112] \t Train Loss(MAE): 0.9195 \t Train RMAE: 0.9589\n[Epoch: 101 \t Valid MAE: 1.2887\n[Epoch: 101 \t Train MAE: 1.1300\nEpoch 00102: reducing learning rate of group 0 to 1.2500e-03.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 102 [64/26112] \t Train Loss(MAE): 1.6584 \t Train RMAE: 1.2878\nTrain Epoch: 102 [12864/26112] \t Train Loss(MAE): 1.0119 \t Train RMAE: 1.0059\nTrain Epoch: 102 [25664/26112] \t Train Loss(MAE): 1.0581 \t Train RMAE: 1.0286\n[Epoch: 102 \t Valid MAE: 1.2552\n[Epoch: 102 \t Train MAE: 1.0520\nValidation loss decreased (1.269268 --> 1.255209).  Saving model ...\nTrain Epoch: 103 [64/26112] \t Train Loss(MAE): 0.8974 \t Train RMAE: 0.9473\nTrain Epoch: 103 [12864/26112] \t Train Loss(MAE): 0.9481 \t Train RMAE: 0.9737\nTrain Epoch: 103 [25664/26112] \t Train Loss(MAE): 1.2399 \t Train RMAE: 1.1135\n[Epoch: 103 \t Valid MAE: 1.2338\n[Epoch: 103 \t Train MAE: 1.0386\nValidation loss decreased (1.255209 --> 1.233828).  Saving model ...\nTrain Epoch: 104 [64/26112] \t Train Loss(MAE): 0.9393 \t Train RMAE: 0.9692\nTrain Epoch: 104 [12864/26112] \t Train Loss(MAE): 1.3593 \t Train RMAE: 1.1659\nTrain Epoch: 104 [25664/26112] \t Train Loss(MAE): 1.0626 \t Train RMAE: 1.0308\n[Epoch: 104 \t Valid MAE: 1.2252\n[Epoch: 104 \t Train MAE: 1.0346\nValidation loss decreased (1.233828 --> 1.225195).  Saving model ...\nTrain Epoch: 105 [64/26112] \t Train Loss(MAE): 1.1674 \t Train RMAE: 1.0805\nTrain Epoch: 105 [12864/26112] \t Train Loss(MAE): 0.7985 \t Train RMAE: 0.8936\nTrain Epoch: 105 [25664/26112] \t Train Loss(MAE): 1.5519 \t Train RMAE: 1.2457\n[Epoch: 105 \t Valid MAE: 1.2382\n[Epoch: 105 \t Train MAE: 1.0423\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 106 [64/26112] \t Train Loss(MAE): 0.9313 \t Train RMAE: 0.9650\nTrain Epoch: 106 [12864/26112] \t Train Loss(MAE): 1.1773 \t Train RMAE: 1.0851\nTrain Epoch: 106 [25664/26112] \t Train Loss(MAE): 0.9007 \t Train RMAE: 0.9491\n[Epoch: 106 \t Valid MAE: 1.2626\n[Epoch: 106 \t Train MAE: 1.0265\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 107 [64/26112] \t Train Loss(MAE): 1.1508 \t Train RMAE: 1.0727\nTrain Epoch: 107 [12864/26112] \t Train Loss(MAE): 1.0203 \t Train RMAE: 1.0101\nTrain Epoch: 107 [25664/26112] \t Train Loss(MAE): 0.6963 \t Train RMAE: 0.8344\n[Epoch: 107 \t Valid MAE: 1.2171\n[Epoch: 107 \t Train MAE: 1.0216\nValidation loss decreased (1.225195 --> 1.217066).  Saving model ...\nTrain Epoch: 108 [64/26112] \t Train Loss(MAE): 0.8069 \t Train RMAE: 0.8983\nTrain Epoch: 108 [12864/26112] \t Train Loss(MAE): 1.1892 \t Train RMAE: 1.0905\nTrain Epoch: 108 [25664/26112] \t Train Loss(MAE): 0.9104 \t Train RMAE: 0.9541\n[Epoch: 108 \t Valid MAE: 1.2167\n[Epoch: 108 \t Train MAE: 1.0264\nValidation loss decreased (1.217066 --> 1.216698).  Saving model ...\nTrain Epoch: 109 [64/26112] \t Train Loss(MAE): 0.8593 \t Train RMAE: 0.9270\nTrain Epoch: 109 [12864/26112] \t Train Loss(MAE): 1.2356 \t Train RMAE: 1.1116\nTrain Epoch: 109 [25664/26112] \t Train Loss(MAE): 1.2076 \t Train RMAE: 1.0989\n[Epoch: 109 \t Valid MAE: 1.2342\n[Epoch: 109 \t Train MAE: 1.0139\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 110 [64/26112] \t Train Loss(MAE): 1.0880 \t Train RMAE: 1.0431\nTrain Epoch: 110 [12864/26112] \t Train Loss(MAE): 1.1625 \t Train RMAE: 1.0782\nTrain Epoch: 110 [25664/26112] \t Train Loss(MAE): 0.7708 \t Train RMAE: 0.8780\n[Epoch: 110 \t Valid MAE: 1.2475\n[Epoch: 110 \t Train MAE: 1.0109\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 111 [64/26112] \t Train Loss(MAE): 0.7357 \t Train RMAE: 0.8577\nTrain Epoch: 111 [12864/26112] \t Train Loss(MAE): 0.7848 \t Train RMAE: 0.8859\nTrain Epoch: 111 [25664/26112] \t Train Loss(MAE): 1.7146 \t Train RMAE: 1.3094\n[Epoch: 111 \t Valid MAE: 1.2209\n[Epoch: 111 \t Train MAE: 1.0137\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 112 [64/26112] \t Train Loss(MAE): 0.7959 \t Train RMAE: 0.8921\nTrain Epoch: 112 [12864/26112] \t Train Loss(MAE): 1.0544 \t Train RMAE: 1.0268\nTrain Epoch: 112 [25664/26112] \t Train Loss(MAE): 0.9215 \t Train RMAE: 0.9599\n[Epoch: 112 \t Valid MAE: 1.2192\n[Epoch: 112 \t Train MAE: 1.0089\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 113 [64/26112] \t Train Loss(MAE): 0.7343 \t Train RMAE: 0.8569\nTrain Epoch: 113 [12864/26112] \t Train Loss(MAE): 0.9315 \t Train RMAE: 0.9651\nTrain Epoch: 113 [25664/26112] \t Train Loss(MAE): 1.0154 \t Train RMAE: 1.0077\n[Epoch: 113 \t Valid MAE: 1.2575\n[Epoch: 113 \t Train MAE: 1.0057\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 114 [64/26112] \t Train Loss(MAE): 0.9133 \t Train RMAE: 0.9557\nTrain Epoch: 114 [12864/26112] \t Train Loss(MAE): 0.9863 \t Train RMAE: 0.9931\nTrain Epoch: 114 [25664/26112] \t Train Loss(MAE): 0.8139 \t Train RMAE: 0.9021\n[Epoch: 114 \t Valid MAE: 1.2295\n[Epoch: 114 \t Train MAE: 1.0051\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 115 [64/26112] \t Train Loss(MAE): 1.3044 \t Train RMAE: 1.1421\nTrain Epoch: 115 [12864/26112] \t Train Loss(MAE): 0.9770 \t Train RMAE: 0.9884\nTrain Epoch: 115 [25664/26112] \t Train Loss(MAE): 0.8769 \t Train RMAE: 0.9365\n[Epoch: 115 \t Valid MAE: 1.2276\n[Epoch: 115 \t Train MAE: 1.0010\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 116 [64/26112] \t Train Loss(MAE): 1.8249 \t Train RMAE: 1.3509\nTrain Epoch: 116 [12864/26112] \t Train Loss(MAE): 0.8409 \t Train RMAE: 0.9170\nTrain Epoch: 116 [25664/26112] \t Train Loss(MAE): 1.3693 \t Train RMAE: 1.1702\n[Epoch: 116 \t Valid MAE: 1.2068\n[Epoch: 116 \t Train MAE: 0.9933\nValidation loss decreased (1.216698 --> 1.206805).  Saving model ...\nTrain Epoch: 117 [64/26112] \t Train Loss(MAE): 1.0263 \t Train RMAE: 1.0131\nTrain Epoch: 117 [12864/26112] \t Train Loss(MAE): 0.7695 \t Train RMAE: 0.8772\nTrain Epoch: 117 [25664/26112] \t Train Loss(MAE): 1.0847 \t Train RMAE: 1.0415\n[Epoch: 117 \t Valid MAE: 1.2022\n[Epoch: 117 \t Train MAE: 0.9898\nValidation loss decreased (1.206805 --> 1.202157).  Saving model ...\nTrain Epoch: 118 [64/26112] \t Train Loss(MAE): 0.7833 \t Train RMAE: 0.8850\nTrain Epoch: 118 [12864/26112] \t Train Loss(MAE): 0.8515 \t Train RMAE: 0.9228\nTrain Epoch: 118 [25664/26112] \t Train Loss(MAE): 0.7791 \t Train RMAE: 0.8827\n[Epoch: 118 \t Valid MAE: 1.2102\n[Epoch: 118 \t Train MAE: 0.9858\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 119 [64/26112] \t Train Loss(MAE): 0.9523 \t Train RMAE: 0.9758\nTrain Epoch: 119 [12864/26112] \t Train Loss(MAE): 1.3203 \t Train RMAE: 1.1490\nTrain Epoch: 119 [25664/26112] \t Train Loss(MAE): 0.8242 \t Train RMAE: 0.9079\n[Epoch: 119 \t Valid MAE: 1.2508\n[Epoch: 119 \t Train MAE: 0.9899\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 120 [64/26112] \t Train Loss(MAE): 1.2523 \t Train RMAE: 1.1191\nTrain Epoch: 120 [12864/26112] \t Train Loss(MAE): 0.9213 \t Train RMAE: 0.9599\nTrain Epoch: 120 [25664/26112] \t Train Loss(MAE): 1.1708 \t Train RMAE: 1.0820\n[Epoch: 120 \t Valid MAE: 1.2116\n[Epoch: 120 \t Train MAE: 0.9826\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 121 [64/26112] \t Train Loss(MAE): 1.1186 \t Train RMAE: 1.0576\nTrain Epoch: 121 [12864/26112] \t Train Loss(MAE): 0.7745 \t Train RMAE: 0.8800\nTrain Epoch: 121 [25664/26112] \t Train Loss(MAE): 1.0767 \t Train RMAE: 1.0376\n[Epoch: 121 \t Valid MAE: 1.2195\n[Epoch: 121 \t Train MAE: 0.9773\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 122 [64/26112] \t Train Loss(MAE): 0.9045 \t Train RMAE: 0.9511\nTrain Epoch: 122 [12864/26112] \t Train Loss(MAE): 0.8920 \t Train RMAE: 0.9444\nTrain Epoch: 122 [25664/26112] \t Train Loss(MAE): 0.7836 \t Train RMAE: 0.8852\n[Epoch: 122 \t Valid MAE: 1.2069\n[Epoch: 122 \t Train MAE: 0.9788\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 123 [64/26112] \t Train Loss(MAE): 1.4743 \t Train RMAE: 1.2142\nTrain Epoch: 123 [12864/26112] \t Train Loss(MAE): 1.0153 \t Train RMAE: 1.0076\nTrain Epoch: 123 [25664/26112] \t Train Loss(MAE): 0.7104 \t Train RMAE: 0.8429\n[Epoch: 123 \t Valid MAE: 1.2020\n[Epoch: 123 \t Train MAE: 0.9681\nValidation loss decreased (1.202157 --> 1.201975).  Saving model ...\nTrain Epoch: 124 [64/26112] \t Train Loss(MAE): 0.6886 \t Train RMAE: 0.8298\nTrain Epoch: 124 [12864/26112] \t Train Loss(MAE): 1.0623 \t Train RMAE: 1.0307\nTrain Epoch: 124 [25664/26112] \t Train Loss(MAE): 0.9681 \t Train RMAE: 0.9839\n[Epoch: 124 \t Valid MAE: 1.2382\n[Epoch: 124 \t Train MAE: 0.9583\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 125 [64/26112] \t Train Loss(MAE): 1.0837 \t Train RMAE: 1.0410\nTrain Epoch: 125 [12864/26112] \t Train Loss(MAE): 1.0259 \t Train RMAE: 1.0128\nTrain Epoch: 125 [25664/26112] \t Train Loss(MAE): 0.8220 \t Train RMAE: 0.9066\n[Epoch: 125 \t Valid MAE: 1.2107\n[Epoch: 125 \t Train MAE: 0.9689\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 126 [64/26112] \t Train Loss(MAE): 0.9535 \t Train RMAE: 0.9765\nTrain Epoch: 126 [12864/26112] \t Train Loss(MAE): 0.8819 \t Train RMAE: 0.9391\nTrain Epoch: 126 [25664/26112] \t Train Loss(MAE): 0.8630 \t Train RMAE: 0.9290\n[Epoch: 126 \t Valid MAE: 1.1967\n[Epoch: 126 \t Train MAE: 0.9631\nValidation loss decreased (1.201975 --> 1.196736).  Saving model ...\nTrain Epoch: 127 [64/26112] \t Train Loss(MAE): 1.1975 \t Train RMAE: 1.0943\nTrain Epoch: 127 [12864/26112] \t Train Loss(MAE): 0.7284 \t Train RMAE: 0.8534\nTrain Epoch: 127 [25664/26112] \t Train Loss(MAE): 1.6068 \t Train RMAE: 1.2676\n[Epoch: 127 \t Valid MAE: 1.2259\n[Epoch: 127 \t Train MAE: 0.9534\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 128 [64/26112] \t Train Loss(MAE): 1.0287 \t Train RMAE: 1.0143\nTrain Epoch: 128 [12864/26112] \t Train Loss(MAE): 0.9978 \t Train RMAE: 0.9989\nTrain Epoch: 128 [25664/26112] \t Train Loss(MAE): 0.6738 \t Train RMAE: 0.8209\n[Epoch: 128 \t Valid MAE: 1.2105\n[Epoch: 128 \t Train MAE: 0.9693\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 129 [64/26112] \t Train Loss(MAE): 1.0374 \t Train RMAE: 1.0185\nTrain Epoch: 129 [12864/26112] \t Train Loss(MAE): 0.7150 \t Train RMAE: 0.8456\nTrain Epoch: 129 [25664/26112] \t Train Loss(MAE): 0.9637 \t Train RMAE: 0.9817\n[Epoch: 129 \t Valid MAE: 1.2058\n[Epoch: 129 \t Train MAE: 0.9520\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 130 [64/26112] \t Train Loss(MAE): 1.5290 \t Train RMAE: 1.2365\nTrain Epoch: 130 [12864/26112] \t Train Loss(MAE): 1.1094 \t Train RMAE: 1.0533\nTrain Epoch: 130 [25664/26112] \t Train Loss(MAE): 1.3482 \t Train RMAE: 1.1611\n[Epoch: 130 \t Valid MAE: 1.2031\n[Epoch: 130 \t Train MAE: 0.9524\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 131 [64/26112] \t Train Loss(MAE): 0.6303 \t Train RMAE: 0.7939\nTrain Epoch: 131 [12864/26112] \t Train Loss(MAE): 0.6464 \t Train RMAE: 0.8040\nTrain Epoch: 131 [25664/26112] \t Train Loss(MAE): 0.9238 \t Train RMAE: 0.9611\n[Epoch: 131 \t Valid MAE: 1.2192\n[Epoch: 131 \t Train MAE: 0.9521\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 132 [64/26112] \t Train Loss(MAE): 1.2510 \t Train RMAE: 1.1185\nTrain Epoch: 132 [12864/26112] \t Train Loss(MAE): 1.1938 \t Train RMAE: 1.0926\nTrain Epoch: 132 [25664/26112] \t Train Loss(MAE): 1.3384 \t Train RMAE: 1.1569\n[Epoch: 132 \t Valid MAE: 1.2087\n[Epoch: 132 \t Train MAE: 0.9475\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 133 [64/26112] \t Train Loss(MAE): 0.9988 \t Train RMAE: 0.9994\nTrain Epoch: 133 [12864/26112] \t Train Loss(MAE): 0.5686 \t Train RMAE: 0.7541\nTrain Epoch: 133 [25664/26112] \t Train Loss(MAE): 1.2157 \t Train RMAE: 1.1026\n[Epoch: 133 \t Valid MAE: 1.1863\n[Epoch: 133 \t Train MAE: 0.9404\nValidation loss decreased (1.196736 --> 1.186280).  Saving model ...\nTrain Epoch: 134 [64/26112] \t Train Loss(MAE): 0.7533 \t Train RMAE: 0.8679\nTrain Epoch: 134 [12864/26112] \t Train Loss(MAE): 0.7869 \t Train RMAE: 0.8871\nTrain Epoch: 134 [25664/26112] \t Train Loss(MAE): 0.8583 \t Train RMAE: 0.9265\n[Epoch: 134 \t Valid MAE: 1.2030\n[Epoch: 134 \t Train MAE: 0.9415\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 135 [64/26112] \t Train Loss(MAE): 0.7344 \t Train RMAE: 0.8570\nTrain Epoch: 135 [12864/26112] \t Train Loss(MAE): 0.8042 \t Train RMAE: 0.8968\nTrain Epoch: 135 [25664/26112] \t Train Loss(MAE): 0.8020 \t Train RMAE: 0.8955\n[Epoch: 135 \t Valid MAE: 1.2148\n[Epoch: 135 \t Train MAE: 0.9352\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 136 [64/26112] \t Train Loss(MAE): 1.2843 \t Train RMAE: 1.1333\nTrain Epoch: 136 [12864/26112] \t Train Loss(MAE): 0.6672 \t Train RMAE: 0.8169\nTrain Epoch: 136 [25664/26112] \t Train Loss(MAE): 0.7877 \t Train RMAE: 0.8875\n[Epoch: 136 \t Valid MAE: 1.1960\n[Epoch: 136 \t Train MAE: 0.9371\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 137 [64/26112] \t Train Loss(MAE): 0.6768 \t Train RMAE: 0.8227\nTrain Epoch: 137 [12864/26112] \t Train Loss(MAE): 1.0735 \t Train RMAE: 1.0361\nTrain Epoch: 137 [25664/26112] \t Train Loss(MAE): 0.9352 \t Train RMAE: 0.9670\n[Epoch: 137 \t Valid MAE: 1.1922\n[Epoch: 137 \t Train MAE: 0.9341\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 138 [64/26112] \t Train Loss(MAE): 0.6787 \t Train RMAE: 0.8238\nTrain Epoch: 138 [12864/26112] \t Train Loss(MAE): 0.8331 \t Train RMAE: 0.9127\nTrain Epoch: 138 [25664/26112] \t Train Loss(MAE): 0.6520 \t Train RMAE: 0.8075\n[Epoch: 138 \t Valid MAE: 1.1992\n[Epoch: 138 \t Train MAE: 0.9329\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 139 [64/26112] \t Train Loss(MAE): 1.1168 \t Train RMAE: 1.0568\nTrain Epoch: 139 [12864/26112] \t Train Loss(MAE): 0.7334 \t Train RMAE: 0.8564\nTrain Epoch: 139 [25664/26112] \t Train Loss(MAE): 0.7342 \t Train RMAE: 0.8568\n[Epoch: 139 \t Valid MAE: 1.2151\n[Epoch: 139 \t Train MAE: 0.9333\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 140 [64/26112] \t Train Loss(MAE): 0.8080 \t Train RMAE: 0.8989\nTrain Epoch: 140 [12864/26112] \t Train Loss(MAE): 1.0470 \t Train RMAE: 1.0232\nTrain Epoch: 140 [25664/26112] \t Train Loss(MAE): 2.1341 \t Train RMAE: 1.4609\n[Epoch: 140 \t Valid MAE: 1.2113\n[Epoch: 140 \t Train MAE: 0.9291\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 141 [64/26112] \t Train Loss(MAE): 0.9281 \t Train RMAE: 0.9634\nTrain Epoch: 141 [12864/26112] \t Train Loss(MAE): 0.8403 \t Train RMAE: 0.9167\nTrain Epoch: 141 [25664/26112] \t Train Loss(MAE): 0.6222 \t Train RMAE: 0.7888\n[Epoch: 141 \t Valid MAE: 1.2098\n[Epoch: 141 \t Train MAE: 0.9246\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 142 [64/26112] \t Train Loss(MAE): 0.8499 \t Train RMAE: 0.9219\nTrain Epoch: 142 [12864/26112] \t Train Loss(MAE): 0.7569 \t Train RMAE: 0.8700\nTrain Epoch: 142 [25664/26112] \t Train Loss(MAE): 0.9353 \t Train RMAE: 0.9671\n[Epoch: 142 \t Valid MAE: 1.2174\n[Epoch: 142 \t Train MAE: 0.9268\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 143 [64/26112] \t Train Loss(MAE): 0.6359 \t Train RMAE: 0.7974\nTrain Epoch: 143 [12864/26112] \t Train Loss(MAE): 0.7615 \t Train RMAE: 0.8726\nTrain Epoch: 143 [25664/26112] \t Train Loss(MAE): 0.7766 \t Train RMAE: 0.8812\n[Epoch: 143 \t Valid MAE: 1.2003\n[Epoch: 143 \t Train MAE: 0.9196\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 144 [64/26112] \t Train Loss(MAE): 0.8401 \t Train RMAE: 0.9166\nTrain Epoch: 144 [12864/26112] \t Train Loss(MAE): 1.0969 \t Train RMAE: 1.0473\nTrain Epoch: 144 [25664/26112] \t Train Loss(MAE): 1.1663 \t Train RMAE: 1.0799\n[Epoch: 144 \t Valid MAE: 1.2167\n[Epoch: 144 \t Train MAE: 0.9214\nEpoch 00145: reducing learning rate of group 0 to 6.2500e-04.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 145 [64/26112] \t Train Loss(MAE): 0.9029 \t Train RMAE: 0.9502\nTrain Epoch: 145 [12864/26112] \t Train Loss(MAE): 0.6352 \t Train RMAE: 0.7970\nTrain Epoch: 145 [25664/26112] \t Train Loss(MAE): 0.7836 \t Train RMAE: 0.8852\n[Epoch: 145 \t Valid MAE: 1.1680\n[Epoch: 145 \t Train MAE: 0.8791\nValidation loss decreased (1.186280 --> 1.167995).  Saving model ...\nTrain Epoch: 146 [64/26112] \t Train Loss(MAE): 0.5757 \t Train RMAE: 0.7588\nTrain Epoch: 146 [12864/26112] \t Train Loss(MAE): 0.8289 \t Train RMAE: 0.9104\nTrain Epoch: 146 [25664/26112] \t Train Loss(MAE): 0.7789 \t Train RMAE: 0.8825\n[Epoch: 146 \t Valid MAE: 1.1813\n[Epoch: 146 \t Train MAE: 0.8648\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 147 [64/26112] \t Train Loss(MAE): 0.6695 \t Train RMAE: 0.8182\nTrain Epoch: 147 [12864/26112] \t Train Loss(MAE): 0.6692 \t Train RMAE: 0.8180\nTrain Epoch: 147 [25664/26112] \t Train Loss(MAE): 0.7272 \t Train RMAE: 0.8528\n[Epoch: 147 \t Valid MAE: 1.1848\n[Epoch: 147 \t Train MAE: 0.8664\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 148 [64/26112] \t Train Loss(MAE): 0.6616 \t Train RMAE: 0.8134\nTrain Epoch: 148 [12864/26112] \t Train Loss(MAE): 0.7243 \t Train RMAE: 0.8511\nTrain Epoch: 148 [25664/26112] \t Train Loss(MAE): 0.7755 \t Train RMAE: 0.8806\n[Epoch: 148 \t Valid MAE: 1.1599\n[Epoch: 148 \t Train MAE: 0.8619\nValidation loss decreased (1.167995 --> 1.159944).  Saving model ...\nTrain Epoch: 149 [64/26112] \t Train Loss(MAE): 0.4782 \t Train RMAE: 0.6916\nTrain Epoch: 149 [12864/26112] \t Train Loss(MAE): 1.3860 \t Train RMAE: 1.1773\nTrain Epoch: 149 [25664/26112] \t Train Loss(MAE): 0.8211 \t Train RMAE: 0.9061\n[Epoch: 149 \t Valid MAE: 1.1616\n[Epoch: 149 \t Train MAE: 0.8606\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 150 [64/26112] \t Train Loss(MAE): 0.6607 \t Train RMAE: 0.8128\nTrain Epoch: 150 [12864/26112] \t Train Loss(MAE): 0.7271 \t Train RMAE: 0.8527\nTrain Epoch: 150 [25664/26112] \t Train Loss(MAE): 0.8588 \t Train RMAE: 0.9267\n[Epoch: 150 \t Valid MAE: 1.1716\n[Epoch: 150 \t Train MAE: 0.8564\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 151 [64/26112] \t Train Loss(MAE): 0.6232 \t Train RMAE: 0.7894\nTrain Epoch: 151 [12864/26112] \t Train Loss(MAE): 0.6284 \t Train RMAE: 0.7927\nTrain Epoch: 151 [25664/26112] \t Train Loss(MAE): 1.2407 \t Train RMAE: 1.1139\n[Epoch: 151 \t Valid MAE: 1.1650\n[Epoch: 151 \t Train MAE: 0.8554\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 152 [64/26112] \t Train Loss(MAE): 0.9875 \t Train RMAE: 0.9937\nTrain Epoch: 152 [12864/26112] \t Train Loss(MAE): 0.6991 \t Train RMAE: 0.8361\nTrain Epoch: 152 [25664/26112] \t Train Loss(MAE): 0.7695 \t Train RMAE: 0.8772\n[Epoch: 152 \t Valid MAE: 1.1771\n[Epoch: 152 \t Train MAE: 0.8550\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 153 [64/26112] \t Train Loss(MAE): 0.6588 \t Train RMAE: 0.8117\nTrain Epoch: 153 [12864/26112] \t Train Loss(MAE): 0.6164 \t Train RMAE: 0.7851\nTrain Epoch: 153 [25664/26112] \t Train Loss(MAE): 1.3753 \t Train RMAE: 1.1728\n[Epoch: 153 \t Valid MAE: 1.1768\n[Epoch: 153 \t Train MAE: 0.8556\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 154 [64/26112] \t Train Loss(MAE): 0.6335 \t Train RMAE: 0.7959\nTrain Epoch: 154 [12864/26112] \t Train Loss(MAE): 1.0856 \t Train RMAE: 1.0419\nTrain Epoch: 154 [25664/26112] \t Train Loss(MAE): 0.9530 \t Train RMAE: 0.9762\n[Epoch: 154 \t Valid MAE: 1.1493\n[Epoch: 154 \t Train MAE: 0.8505\nValidation loss decreased (1.159944 --> 1.149278).  Saving model ...\nTrain Epoch: 155 [64/26112] \t Train Loss(MAE): 0.7402 \t Train RMAE: 0.8603\nTrain Epoch: 155 [12864/26112] \t Train Loss(MAE): 1.3724 \t Train RMAE: 1.1715\nTrain Epoch: 155 [25664/26112] \t Train Loss(MAE): 0.5805 \t Train RMAE: 0.7619\n[Epoch: 155 \t Valid MAE: 1.1607\n[Epoch: 155 \t Train MAE: 0.8528\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 156 [64/26112] \t Train Loss(MAE): 0.9032 \t Train RMAE: 0.9504\nTrain Epoch: 156 [12864/26112] \t Train Loss(MAE): 1.3548 \t Train RMAE: 1.1639\nTrain Epoch: 156 [25664/26112] \t Train Loss(MAE): 0.9144 \t Train RMAE: 0.9563\n[Epoch: 156 \t Valid MAE: 1.1742\n[Epoch: 156 \t Train MAE: 0.8443\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 157 [64/26112] \t Train Loss(MAE): 1.1202 \t Train RMAE: 1.0584\nTrain Epoch: 157 [12864/26112] \t Train Loss(MAE): 0.5977 \t Train RMAE: 0.7731\nTrain Epoch: 157 [25664/26112] \t Train Loss(MAE): 1.4289 \t Train RMAE: 1.1954\n[Epoch: 157 \t Valid MAE: 1.1808\n[Epoch: 157 \t Train MAE: 0.8431\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 158 [64/26112] \t Train Loss(MAE): 0.6281 \t Train RMAE: 0.7925\nTrain Epoch: 158 [12864/26112] \t Train Loss(MAE): 1.1441 \t Train RMAE: 1.0696\nTrain Epoch: 158 [25664/26112] \t Train Loss(MAE): 0.8004 \t Train RMAE: 0.8947\n[Epoch: 158 \t Valid MAE: 1.1737\n[Epoch: 158 \t Train MAE: 0.8439\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 159 [64/26112] \t Train Loss(MAE): 0.6281 \t Train RMAE: 0.7925\nTrain Epoch: 159 [12864/26112] \t Train Loss(MAE): 0.5988 \t Train RMAE: 0.7739\nTrain Epoch: 159 [25664/26112] \t Train Loss(MAE): 0.8750 \t Train RMAE: 0.9354\n[Epoch: 159 \t Valid MAE: 1.1696\n[Epoch: 159 \t Train MAE: 0.8437\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 160 [64/26112] \t Train Loss(MAE): 0.8702 \t Train RMAE: 0.9328\nTrain Epoch: 160 [12864/26112] \t Train Loss(MAE): 0.5008 \t Train RMAE: 0.7076\nTrain Epoch: 160 [25664/26112] \t Train Loss(MAE): 1.4550 \t Train RMAE: 1.2062\n[Epoch: 160 \t Valid MAE: 1.1770\n[Epoch: 160 \t Train MAE: 0.8403\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 161 [64/26112] \t Train Loss(MAE): 0.5485 \t Train RMAE: 0.7406\nTrain Epoch: 161 [12864/26112] \t Train Loss(MAE): 0.8709 \t Train RMAE: 0.9332\nTrain Epoch: 161 [25664/26112] \t Train Loss(MAE): 0.8348 \t Train RMAE: 0.9137\n[Epoch: 161 \t Valid MAE: 1.1791\n[Epoch: 161 \t Train MAE: 0.8389\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 162 [64/26112] \t Train Loss(MAE): 0.7744 \t Train RMAE: 0.8800\nTrain Epoch: 162 [12864/26112] \t Train Loss(MAE): 0.7714 \t Train RMAE: 0.8783\nTrain Epoch: 162 [25664/26112] \t Train Loss(MAE): 0.8521 \t Train RMAE: 0.9231\n[Epoch: 162 \t Valid MAE: 1.1617\n[Epoch: 162 \t Train MAE: 0.8391\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 163 [64/26112] \t Train Loss(MAE): 1.1961 \t Train RMAE: 1.0937\nTrain Epoch: 163 [12864/26112] \t Train Loss(MAE): 0.9514 \t Train RMAE: 0.9754\nTrain Epoch: 163 [25664/26112] \t Train Loss(MAE): 0.7588 \t Train RMAE: 0.8711\n[Epoch: 163 \t Valid MAE: 1.1678\n[Epoch: 163 \t Train MAE: 0.8390\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 164 [64/26112] \t Train Loss(MAE): 0.6080 \t Train RMAE: 0.7797\nTrain Epoch: 164 [12864/26112] \t Train Loss(MAE): 0.7520 \t Train RMAE: 0.8672\nTrain Epoch: 164 [25664/26112] \t Train Loss(MAE): 0.6070 \t Train RMAE: 0.7791\n[Epoch: 164 \t Valid MAE: 1.1634\n[Epoch: 164 \t Train MAE: 0.8304\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 165 [64/26112] \t Train Loss(MAE): 0.8270 \t Train RMAE: 0.9094\nTrain Epoch: 165 [12864/26112] \t Train Loss(MAE): 0.9231 \t Train RMAE: 0.9608\nTrain Epoch: 165 [25664/26112] \t Train Loss(MAE): 0.6698 \t Train RMAE: 0.8184\n[Epoch: 165 \t Valid MAE: 1.1717\n[Epoch: 165 \t Train MAE: 0.8349\nEpoch 00166: reducing learning rate of group 0 to 3.1250e-04.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 166 [64/26112] \t Train Loss(MAE): 0.6841 \t Train RMAE: 0.8271\nTrain Epoch: 166 [12864/26112] \t Train Loss(MAE): 0.5326 \t Train RMAE: 0.7298\nTrain Epoch: 166 [25664/26112] \t Train Loss(MAE): 0.8566 \t Train RMAE: 0.9255\n[Epoch: 166 \t Valid MAE: 1.1637\n[Epoch: 166 \t Train MAE: 0.8118\nEarlyStopping counter: 12 out of 70\nTrain Epoch: 167 [64/26112] \t Train Loss(MAE): 0.6181 \t Train RMAE: 0.7862\nTrain Epoch: 167 [12864/26112] \t Train Loss(MAE): 0.7691 \t Train RMAE: 0.8770\nTrain Epoch: 167 [25664/26112] \t Train Loss(MAE): 0.6946 \t Train RMAE: 0.8334\n[Epoch: 167 \t Valid MAE: 1.1525\n[Epoch: 167 \t Train MAE: 0.8031\nEarlyStopping counter: 13 out of 70\nTrain Epoch: 168 [64/26112] \t Train Loss(MAE): 0.6734 \t Train RMAE: 0.8206\nTrain Epoch: 168 [12864/26112] \t Train Loss(MAE): 0.6720 \t Train RMAE: 0.8198\nTrain Epoch: 168 [25664/26112] \t Train Loss(MAE): 0.7257 \t Train RMAE: 0.8519\n[Epoch: 168 \t Valid MAE: 1.1584\n[Epoch: 168 \t Train MAE: 0.8023\nEarlyStopping counter: 14 out of 70\nTrain Epoch: 169 [64/26112] \t Train Loss(MAE): 1.0090 \t Train RMAE: 1.0045\nTrain Epoch: 169 [12864/26112] \t Train Loss(MAE): 1.2005 \t Train RMAE: 1.0957\nTrain Epoch: 169 [25664/26112] \t Train Loss(MAE): 0.6232 \t Train RMAE: 0.7894\n[Epoch: 169 \t Valid MAE: 1.1493\n[Epoch: 169 \t Train MAE: 0.8001\nEarlyStopping counter: 15 out of 70\nTrain Epoch: 170 [64/26112] \t Train Loss(MAE): 0.5275 \t Train RMAE: 0.7263\nTrain Epoch: 170 [12864/26112] \t Train Loss(MAE): 0.7720 \t Train RMAE: 0.8786\nTrain Epoch: 170 [25664/26112] \t Train Loss(MAE): 0.4836 \t Train RMAE: 0.6954\n[Epoch: 170 \t Valid MAE: 1.1474\n[Epoch: 170 \t Train MAE: 0.7981\nValidation loss decreased (1.149278 --> 1.147392).  Saving model ...\nTrain Epoch: 171 [64/26112] \t Train Loss(MAE): 0.5469 \t Train RMAE: 0.7395\nTrain Epoch: 171 [12864/26112] \t Train Loss(MAE): 0.6917 \t Train RMAE: 0.8317\nTrain Epoch: 171 [25664/26112] \t Train Loss(MAE): 1.1672 \t Train RMAE: 1.0804\n[Epoch: 171 \t Valid MAE: 1.1547\n[Epoch: 171 \t Train MAE: 0.7978\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 172 [64/26112] \t Train Loss(MAE): 0.6057 \t Train RMAE: 0.7783\nTrain Epoch: 172 [12864/26112] \t Train Loss(MAE): 0.7599 \t Train RMAE: 0.8717\nTrain Epoch: 172 [25664/26112] \t Train Loss(MAE): 0.7606 \t Train RMAE: 0.8721\n[Epoch: 172 \t Valid MAE: 1.1486\n[Epoch: 172 \t Train MAE: 0.7970\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 173 [64/26112] \t Train Loss(MAE): 0.9126 \t Train RMAE: 0.9553\nTrain Epoch: 173 [12864/26112] \t Train Loss(MAE): 0.7524 \t Train RMAE: 0.8674\nTrain Epoch: 173 [25664/26112] \t Train Loss(MAE): 0.9352 \t Train RMAE: 0.9671\n[Epoch: 173 \t Valid MAE: 1.1528\n[Epoch: 173 \t Train MAE: 0.7961\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 174 [64/26112] \t Train Loss(MAE): 0.7991 \t Train RMAE: 0.8939\nTrain Epoch: 174 [12864/26112] \t Train Loss(MAE): 0.7354 \t Train RMAE: 0.8576\nTrain Epoch: 174 [25664/26112] \t Train Loss(MAE): 0.6128 \t Train RMAE: 0.7828\n[Epoch: 174 \t Valid MAE: 1.1481\n[Epoch: 174 \t Train MAE: 0.7956\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 175 [64/26112] \t Train Loss(MAE): 0.5740 \t Train RMAE: 0.7576\nTrain Epoch: 175 [12864/26112] \t Train Loss(MAE): 0.6990 \t Train RMAE: 0.8361\nTrain Epoch: 175 [25664/26112] \t Train Loss(MAE): 0.9701 \t Train RMAE: 0.9849\n[Epoch: 175 \t Valid MAE: 1.1625\n[Epoch: 175 \t Train MAE: 0.7936\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 176 [64/26112] \t Train Loss(MAE): 0.8575 \t Train RMAE: 0.9260\nTrain Epoch: 176 [12864/26112] \t Train Loss(MAE): 1.6118 \t Train RMAE: 1.2696\nTrain Epoch: 176 [25664/26112] \t Train Loss(MAE): 0.6209 \t Train RMAE: 0.7880\n[Epoch: 176 \t Valid MAE: 1.1575\n[Epoch: 176 \t Train MAE: 0.7934\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 177 [64/26112] \t Train Loss(MAE): 0.9785 \t Train RMAE: 0.9892\nTrain Epoch: 177 [12864/26112] \t Train Loss(MAE): 0.9385 \t Train RMAE: 0.9687\nTrain Epoch: 177 [25664/26112] \t Train Loss(MAE): 0.5348 \t Train RMAE: 0.7313\n[Epoch: 177 \t Valid MAE: 1.1516\n[Epoch: 177 \t Train MAE: 0.7915\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 178 [64/26112] \t Train Loss(MAE): 0.7993 \t Train RMAE: 0.8941\nTrain Epoch: 178 [12864/26112] \t Train Loss(MAE): 1.0127 \t Train RMAE: 1.0063\nTrain Epoch: 178 [25664/26112] \t Train Loss(MAE): 0.6881 \t Train RMAE: 0.8295\n[Epoch: 178 \t Valid MAE: 1.1548\n[Epoch: 178 \t Train MAE: 0.7922\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 179 [64/26112] \t Train Loss(MAE): 0.8421 \t Train RMAE: 0.9177\nTrain Epoch: 179 [12864/26112] \t Train Loss(MAE): 0.5595 \t Train RMAE: 0.7480\nTrain Epoch: 179 [25664/26112] \t Train Loss(MAE): 0.8350 \t Train RMAE: 0.9138\n[Epoch: 179 \t Valid MAE: 1.1482\n[Epoch: 179 \t Train MAE: 0.7905\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 180 [64/26112] \t Train Loss(MAE): 0.9282 \t Train RMAE: 0.9634\nTrain Epoch: 180 [12864/26112] \t Train Loss(MAE): 0.7386 \t Train RMAE: 0.8594\nTrain Epoch: 180 [25664/26112] \t Train Loss(MAE): 0.5998 \t Train RMAE: 0.7744\n[Epoch: 180 \t Valid MAE: 1.1526\n[Epoch: 180 \t Train MAE: 0.7906\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 181 [64/26112] \t Train Loss(MAE): 0.7777 \t Train RMAE: 0.8819\nTrain Epoch: 181 [12864/26112] \t Train Loss(MAE): 0.7536 \t Train RMAE: 0.8681\nTrain Epoch: 181 [25664/26112] \t Train Loss(MAE): 0.8365 \t Train RMAE: 0.9146\n[Epoch: 181 \t Valid MAE: 1.1637\n[Epoch: 181 \t Train MAE: 0.7884\nEpoch 00182: reducing learning rate of group 0 to 1.5625e-04.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 182 [64/26112] \t Train Loss(MAE): 0.7240 \t Train RMAE: 0.8509\nTrain Epoch: 182 [12864/26112] \t Train Loss(MAE): 0.8202 \t Train RMAE: 0.9057\nTrain Epoch: 182 [25664/26112] \t Train Loss(MAE): 0.5582 \t Train RMAE: 0.7471\n[Epoch: 182 \t Valid MAE: 1.1470\n[Epoch: 182 \t Train MAE: 0.7759\nValidation loss decreased (1.147392 --> 1.146985).  Saving model ...\nTrain Epoch: 183 [64/26112] \t Train Loss(MAE): 0.5730 \t Train RMAE: 0.7570\nTrain Epoch: 183 [12864/26112] \t Train Loss(MAE): 0.6283 \t Train RMAE: 0.7927\nTrain Epoch: 183 [25664/26112] \t Train Loss(MAE): 0.6285 \t Train RMAE: 0.7928\n[Epoch: 183 \t Valid MAE: 1.1496\n[Epoch: 183 \t Train MAE: 0.7731\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 184 [64/26112] \t Train Loss(MAE): 0.8597 \t Train RMAE: 0.9272\nTrain Epoch: 184 [12864/26112] \t Train Loss(MAE): 0.5951 \t Train RMAE: 0.7714\nTrain Epoch: 184 [25664/26112] \t Train Loss(MAE): 0.5651 \t Train RMAE: 0.7517\n[Epoch: 184 \t Valid MAE: 1.1479\n[Epoch: 184 \t Train MAE: 0.7726\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 185 [64/26112] \t Train Loss(MAE): 0.6561 \t Train RMAE: 0.8100\nTrain Epoch: 185 [12864/26112] \t Train Loss(MAE): 0.6620 \t Train RMAE: 0.8136\nTrain Epoch: 185 [25664/26112] \t Train Loss(MAE): 0.6560 \t Train RMAE: 0.8099\n[Epoch: 185 \t Valid MAE: 1.1429\n[Epoch: 185 \t Train MAE: 0.7713\nValidation loss decreased (1.146985 --> 1.142913).  Saving model ...\nTrain Epoch: 186 [64/26112] \t Train Loss(MAE): 0.5738 \t Train RMAE: 0.7575\nTrain Epoch: 186 [12864/26112] \t Train Loss(MAE): 0.7232 \t Train RMAE: 0.8504\nTrain Epoch: 186 [25664/26112] \t Train Loss(MAE): 0.6590 \t Train RMAE: 0.8118\n[Epoch: 186 \t Valid MAE: 1.1496\n[Epoch: 186 \t Train MAE: 0.7703\nEarlyStopping counter: 1 out of 70\nTrain Epoch: 187 [64/26112] \t Train Loss(MAE): 0.5596 \t Train RMAE: 0.7481\nTrain Epoch: 187 [12864/26112] \t Train Loss(MAE): 0.6956 \t Train RMAE: 0.8340\nTrain Epoch: 187 [25664/26112] \t Train Loss(MAE): 0.4616 \t Train RMAE: 0.6794\n[Epoch: 187 \t Valid MAE: 1.1472\n[Epoch: 187 \t Train MAE: 0.7710\nEarlyStopping counter: 2 out of 70\nTrain Epoch: 188 [64/26112] \t Train Loss(MAE): 0.6967 \t Train RMAE: 0.8347\nTrain Epoch: 188 [12864/26112] \t Train Loss(MAE): 0.9560 \t Train RMAE: 0.9778\nTrain Epoch: 188 [25664/26112] \t Train Loss(MAE): 0.8263 \t Train RMAE: 0.9090\n[Epoch: 188 \t Valid MAE: 1.1486\n[Epoch: 188 \t Train MAE: 0.7689\nEarlyStopping counter: 3 out of 70\nTrain Epoch: 189 [64/26112] \t Train Loss(MAE): 0.8669 \t Train RMAE: 0.9311\nTrain Epoch: 189 [12864/26112] \t Train Loss(MAE): 1.1189 \t Train RMAE: 1.0578\nTrain Epoch: 189 [25664/26112] \t Train Loss(MAE): 1.2688 \t Train RMAE: 1.1264\n[Epoch: 189 \t Valid MAE: 1.1487\n[Epoch: 189 \t Train MAE: 0.7688\nEarlyStopping counter: 4 out of 70\nTrain Epoch: 190 [64/26112] \t Train Loss(MAE): 0.6396 \t Train RMAE: 0.7997\nTrain Epoch: 190 [12864/26112] \t Train Loss(MAE): 0.6067 \t Train RMAE: 0.7789\nTrain Epoch: 190 [25664/26112] \t Train Loss(MAE): 0.8250 \t Train RMAE: 0.9083\n[Epoch: 190 \t Valid MAE: 1.1474\n[Epoch: 190 \t Train MAE: 0.7684\nEarlyStopping counter: 5 out of 70\nTrain Epoch: 191 [64/26112] \t Train Loss(MAE): 0.6414 \t Train RMAE: 0.8009\nTrain Epoch: 191 [12864/26112] \t Train Loss(MAE): 0.6679 \t Train RMAE: 0.8173\nTrain Epoch: 191 [25664/26112] \t Train Loss(MAE): 0.5763 \t Train RMAE: 0.7592\n[Epoch: 191 \t Valid MAE: 1.1480\n[Epoch: 191 \t Train MAE: 0.7676\nEarlyStopping counter: 6 out of 70\nTrain Epoch: 192 [64/26112] \t Train Loss(MAE): 0.7153 \t Train RMAE: 0.8457\nTrain Epoch: 192 [12864/26112] \t Train Loss(MAE): 0.7127 \t Train RMAE: 0.8442\nTrain Epoch: 192 [25664/26112] \t Train Loss(MAE): 1.8398 \t Train RMAE: 1.3564\n[Epoch: 192 \t Valid MAE: 1.1481\n[Epoch: 192 \t Train MAE: 0.7670\nEarlyStopping counter: 7 out of 70\nTrain Epoch: 193 [64/26112] \t Train Loss(MAE): 0.8966 \t Train RMAE: 0.9469\nTrain Epoch: 193 [12864/26112] \t Train Loss(MAE): 1.4957 \t Train RMAE: 1.2230\nTrain Epoch: 193 [25664/26112] \t Train Loss(MAE): 0.6773 \t Train RMAE: 0.8230\n[Epoch: 193 \t Valid MAE: 1.1456\n[Epoch: 193 \t Train MAE: 0.7662\nEarlyStopping counter: 8 out of 70\nTrain Epoch: 194 [64/26112] \t Train Loss(MAE): 0.6724 \t Train RMAE: 0.8200\nTrain Epoch: 194 [12864/26112] \t Train Loss(MAE): 0.6526 \t Train RMAE: 0.8078\nTrain Epoch: 194 [25664/26112] \t Train Loss(MAE): 1.2439 \t Train RMAE: 1.1153\n[Epoch: 194 \t Valid MAE: 1.1462\n[Epoch: 194 \t Train MAE: 0.7666\nEarlyStopping counter: 9 out of 70\nTrain Epoch: 195 [64/26112] \t Train Loss(MAE): 0.9135 \t Train RMAE: 0.9558\nTrain Epoch: 195 [12864/26112] \t Train Loss(MAE): 1.1781 \t Train RMAE: 1.0854\nTrain Epoch: 195 [25664/26112] \t Train Loss(MAE): 0.6776 \t Train RMAE: 0.8232\n[Epoch: 195 \t Valid MAE: 1.1477\n[Epoch: 195 \t Train MAE: 0.7650\nEarlyStopping counter: 10 out of 70\nTrain Epoch: 196 [64/26112] \t Train Loss(MAE): 0.7260 \t Train RMAE: 0.8521\nTrain Epoch: 196 [12864/26112] \t Train Loss(MAE): 0.9861 \t Train RMAE: 0.9930\nTrain Epoch: 196 [25664/26112] \t Train Loss(MAE): 1.6325 \t Train RMAE: 1.2777\n[Epoch: 196 \t Valid MAE: 1.1447\n[Epoch: 196 \t Train MAE: 0.7651\nEpoch 00197: reducing learning rate of group 0 to 7.8125e-05.\nEarlyStopping counter: 11 out of 70\nTrain Epoch: 197 [64/26112] \t Train Loss(MAE): 0.9023 \t Train RMAE: 0.9499\nTrain Epoch: 197 [12864/26112] \t Train Loss(MAE): 0.8289 \t Train RMAE: 0.9104\nTrain Epoch: 197 [25664/26112] \t Train Loss(MAE): 0.8459 \t Train RMAE: 0.9197\n[Epoch: 197 \t Valid MAE: 1.1463\n[Epoch: 197 \t Train MAE: 0.7584\nEarlyStopping counter: 12 out of 70\nTrain Epoch: 198 [64/26112] \t Train Loss(MAE): 0.9908 \t Train RMAE: 0.9954\nTrain Epoch: 198 [12864/26112] \t Train Loss(MAE): 0.6220 \t Train RMAE: 0.7887\nTrain Epoch: 198 [25664/26112] \t Train Loss(MAE): 0.8216 \t Train RMAE: 0.9064\n[Epoch: 198 \t Valid MAE: 1.1467\n[Epoch: 198 \t Train MAE: 0.7569\nEarlyStopping counter: 13 out of 70\nTrain Epoch: 199 [64/26112] \t Train Loss(MAE): 0.4051 \t Train RMAE: 0.6365\nTrain Epoch: 199 [12864/26112] \t Train Loss(MAE): 0.5499 \t Train RMAE: 0.7415\nTrain Epoch: 199 [25664/26112] \t Train Loss(MAE): 0.5085 \t Train RMAE: 0.7131\n[Epoch: 199 \t Valid MAE: 1.1500\n[Epoch: 199 \t Train MAE: 0.7565\nEarlyStopping counter: 14 out of 70\nTrain Epoch: 200 [64/26112] \t Train Loss(MAE): 0.8502 \t Train RMAE: 0.9221\nTrain Epoch: 200 [12864/26112] \t Train Loss(MAE): 0.7059 \t Train RMAE: 0.8402\nTrain Epoch: 200 [25664/26112] \t Train Loss(MAE): 1.0729 \t Train RMAE: 1.0358\n[Epoch: 200 \t Valid MAE: 1.1526\n[Epoch: 200 \t Train MAE: 0.7559\nEarlyStopping counter: 15 out of 70\nTrain Epoch: 201 [64/26112] \t Train Loss(MAE): 0.6230 \t Train RMAE: 0.7893\nTrain Epoch: 201 [12864/26112] \t Train Loss(MAE): 0.8317 \t Train RMAE: 0.9120\nTrain Epoch: 201 [25664/26112] \t Train Loss(MAE): 0.4274 \t Train RMAE: 0.6537\n[Epoch: 201 \t Valid MAE: 1.1463\n[Epoch: 201 \t Train MAE: 0.7554\nEarlyStopping counter: 16 out of 70\nTrain Epoch: 202 [64/26112] \t Train Loss(MAE): 0.5353 \t Train RMAE: 0.7316\nTrain Epoch: 202 [12864/26112] \t Train Loss(MAE): 0.5273 \t Train RMAE: 0.7262\nTrain Epoch: 202 [25664/26112] \t Train Loss(MAE): 0.6084 \t Train RMAE: 0.7800\n[Epoch: 202 \t Valid MAE: 1.1485\n[Epoch: 202 \t Train MAE: 0.7554\nEarlyStopping counter: 17 out of 70\nTrain Epoch: 203 [64/26112] \t Train Loss(MAE): 1.1705 \t Train RMAE: 1.0819\nTrain Epoch: 203 [12864/26112] \t Train Loss(MAE): 0.6960 \t Train RMAE: 0.8343\nTrain Epoch: 203 [25664/26112] \t Train Loss(MAE): 0.6386 \t Train RMAE: 0.7991\n[Epoch: 203 \t Valid MAE: 1.1486\n[Epoch: 203 \t Train MAE: 0.7556\nEarlyStopping counter: 18 out of 70\nTrain Epoch: 204 [64/26112] \t Train Loss(MAE): 0.7381 \t Train RMAE: 0.8591\nTrain Epoch: 204 [12864/26112] \t Train Loss(MAE): 0.5351 \t Train RMAE: 0.7315\nTrain Epoch: 204 [25664/26112] \t Train Loss(MAE): 0.6008 \t Train RMAE: 0.7751\n[Epoch: 204 \t Valid MAE: 1.1453\n[Epoch: 204 \t Train MAE: 0.7541\nEarlyStopping counter: 19 out of 70\nTrain Epoch: 205 [64/26112] \t Train Loss(MAE): 0.4581 \t Train RMAE: 0.6768\nTrain Epoch: 205 [12864/26112] \t Train Loss(MAE): 0.6752 \t Train RMAE: 0.8217\nTrain Epoch: 205 [25664/26112] \t Train Loss(MAE): 0.7495 \t Train RMAE: 0.8658\n[Epoch: 205 \t Valid MAE: 1.1438\n[Epoch: 205 \t Train MAE: 0.7550\nEarlyStopping counter: 20 out of 70\nTrain Epoch: 206 [64/26112] \t Train Loss(MAE): 0.6143 \t Train RMAE: 0.7837\nTrain Epoch: 206 [12864/26112] \t Train Loss(MAE): 0.4998 \t Train RMAE: 0.7070\nTrain Epoch: 206 [25664/26112] \t Train Loss(MAE): 1.5728 \t Train RMAE: 1.2541\n[Epoch: 206 \t Valid MAE: 1.1461\n[Epoch: 206 \t Train MAE: 0.7537\nEarlyStopping counter: 21 out of 70\nTrain Epoch: 207 [64/26112] \t Train Loss(MAE): 0.6412 \t Train RMAE: 0.8008\nTrain Epoch: 207 [12864/26112] \t Train Loss(MAE): 0.5634 \t Train RMAE: 0.7506\nTrain Epoch: 207 [25664/26112] \t Train Loss(MAE): 1.0748 \t Train RMAE: 1.0367\n[Epoch: 207 \t Valid MAE: 1.1471\n[Epoch: 207 \t Train MAE: 0.7533\nEpoch 00208: reducing learning rate of group 0 to 5.0000e-05.\nEarlyStopping counter: 22 out of 70\nTrain Epoch: 208 [64/26112] \t Train Loss(MAE): 0.9056 \t Train RMAE: 0.9516\nTrain Epoch: 208 [12864/26112] \t Train Loss(MAE): 0.6872 \t Train RMAE: 0.8290\nTrain Epoch: 208 [25664/26112] \t Train Loss(MAE): 0.5205 \t Train RMAE: 0.7214\n[Epoch: 208 \t Valid MAE: 1.1466\n[Epoch: 208 \t Train MAE: 0.7511\nEarlyStopping counter: 23 out of 70\nTrain Epoch: 209 [64/26112] \t Train Loss(MAE): 1.3297 \t Train RMAE: 1.1531\nTrain Epoch: 209 [12864/26112] \t Train Loss(MAE): 0.6432 \t Train RMAE: 0.8020\nTrain Epoch: 209 [25664/26112] \t Train Loss(MAE): 0.6155 \t Train RMAE: 0.7845\n[Epoch: 209 \t Valid MAE: 1.1459\n[Epoch: 209 \t Train MAE: 0.7505\nEarlyStopping counter: 24 out of 70\nTrain Epoch: 210 [64/26112] \t Train Loss(MAE): 0.4919 \t Train RMAE: 0.7014\nTrain Epoch: 210 [12864/26112] \t Train Loss(MAE): 0.7540 \t Train RMAE: 0.8683\nTrain Epoch: 210 [25664/26112] \t Train Loss(MAE): 0.5838 \t Train RMAE: 0.7641\n[Epoch: 210 \t Valid MAE: 1.1467\n[Epoch: 210 \t Train MAE: 0.7501\nEarlyStopping counter: 25 out of 70\nTrain Epoch: 211 [64/26112] \t Train Loss(MAE): 1.1530 \t Train RMAE: 1.0738\nTrain Epoch: 211 [12864/26112] \t Train Loss(MAE): 0.7382 \t Train RMAE: 0.8592\nTrain Epoch: 211 [25664/26112] \t Train Loss(MAE): 0.6832 \t Train RMAE: 0.8266\n[Epoch: 211 \t Valid MAE: 1.1476\n[Epoch: 211 \t Train MAE: 0.7500\nEarlyStopping counter: 26 out of 70\nTrain Epoch: 212 [64/26112] \t Train Loss(MAE): 0.6265 \t Train RMAE: 0.7915\nTrain Epoch: 212 [12864/26112] \t Train Loss(MAE): 0.5524 \t Train RMAE: 0.7433\nTrain Epoch: 212 [25664/26112] \t Train Loss(MAE): 1.1225 \t Train RMAE: 1.0595\n[Epoch: 212 \t Valid MAE: 1.1454\n[Epoch: 212 \t Train MAE: 0.7496\nEarlyStopping counter: 27 out of 70\nTrain Epoch: 213 [64/26112] \t Train Loss(MAE): 0.4403 \t Train RMAE: 0.6635\nTrain Epoch: 213 [12864/26112] \t Train Loss(MAE): 0.7079 \t Train RMAE: 0.8414\nTrain Epoch: 213 [25664/26112] \t Train Loss(MAE): 0.6943 \t Train RMAE: 0.8333\n[Epoch: 213 \t Valid MAE: 1.1451\n[Epoch: 213 \t Train MAE: 0.7495\nEarlyStopping counter: 28 out of 70\nTrain Epoch: 214 [64/26112] \t Train Loss(MAE): 0.8168 \t Train RMAE: 0.9038\nTrain Epoch: 214 [12864/26112] \t Train Loss(MAE): 0.5278 \t Train RMAE: 0.7265\nTrain Epoch: 214 [25664/26112] \t Train Loss(MAE): 0.7776 \t Train RMAE: 0.8818\n[Epoch: 214 \t Valid MAE: 1.1456\n[Epoch: 214 \t Train MAE: 0.7493\nEarlyStopping counter: 29 out of 70\nTrain Epoch: 215 [64/26112] \t Train Loss(MAE): 0.5169 \t Train RMAE: 0.7189\nTrain Epoch: 215 [12864/26112] \t Train Loss(MAE): 0.4833 \t Train RMAE: 0.6952\nTrain Epoch: 215 [25664/26112] \t Train Loss(MAE): 0.5715 \t Train RMAE: 0.7560\n[Epoch: 215 \t Valid MAE: 1.1457\n[Epoch: 215 \t Train MAE: 0.7488\nEarlyStopping counter: 30 out of 70\nTrain Epoch: 216 [64/26112] \t Train Loss(MAE): 0.7218 \t Train RMAE: 0.8496\nTrain Epoch: 216 [12864/26112] \t Train Loss(MAE): 0.6437 \t Train RMAE: 0.8023\nTrain Epoch: 216 [25664/26112] \t Train Loss(MAE): 0.7232 \t Train RMAE: 0.8504\n[Epoch: 216 \t Valid MAE: 1.1459\n[Epoch: 216 \t Train MAE: 0.7486\nEarlyStopping counter: 31 out of 70\nTrain Epoch: 217 [64/26112] \t Train Loss(MAE): 0.7989 \t Train RMAE: 0.8938\nTrain Epoch: 217 [12864/26112] \t Train Loss(MAE): 1.1833 \t Train RMAE: 1.0878\nTrain Epoch: 217 [25664/26112] \t Train Loss(MAE): 1.2006 \t Train RMAE: 1.0957\n[Epoch: 217 \t Valid MAE: 1.1446\n[Epoch: 217 \t Train MAE: 0.7489\nEarlyStopping counter: 32 out of 70\nTrain Epoch: 218 [64/26112] \t Train Loss(MAE): 0.9503 \t Train RMAE: 0.9748\nTrain Epoch: 218 [12864/26112] \t Train Loss(MAE): 0.7389 \t Train RMAE: 0.8596\nTrain Epoch: 218 [25664/26112] \t Train Loss(MAE): 0.8372 \t Train RMAE: 0.9150\n[Epoch: 218 \t Valid MAE: 1.1448\n[Epoch: 218 \t Train MAE: 0.7485\nEarlyStopping counter: 33 out of 70\nTrain Epoch: 219 [64/26112] \t Train Loss(MAE): 1.0381 \t Train RMAE: 1.0188\nTrain Epoch: 219 [12864/26112] \t Train Loss(MAE): 0.5676 \t Train RMAE: 0.7534\nTrain Epoch: 219 [25664/26112] \t Train Loss(MAE): 0.9228 \t Train RMAE: 0.9606\n[Epoch: 219 \t Valid MAE: 1.1471\n[Epoch: 219 \t Train MAE: 0.7483\nEarlyStopping counter: 34 out of 70\nTrain Epoch: 220 [64/26112] \t Train Loss(MAE): 0.6058 \t Train RMAE: 0.7783\nTrain Epoch: 220 [12864/26112] \t Train Loss(MAE): 0.6599 \t Train RMAE: 0.8123\nTrain Epoch: 220 [25664/26112] \t Train Loss(MAE): 0.7570 \t Train RMAE: 0.8700\n[Epoch: 220 \t Valid MAE: 1.1455\n[Epoch: 220 \t Train MAE: 0.7484\nEarlyStopping counter: 35 out of 70\nTrain Epoch: 221 [64/26112] \t Train Loss(MAE): 0.7952 \t Train RMAE: 0.8917\nTrain Epoch: 221 [12864/26112] \t Train Loss(MAE): 0.7748 \t Train RMAE: 0.8802\nTrain Epoch: 221 [25664/26112] \t Train Loss(MAE): 0.8916 \t Train RMAE: 0.9442\n[Epoch: 221 \t Valid MAE: 1.1451\n[Epoch: 221 \t Train MAE: 0.7480\nEarlyStopping counter: 36 out of 70\nTrain Epoch: 222 [64/26112] \t Train Loss(MAE): 0.8326 \t Train RMAE: 0.9125\nTrain Epoch: 222 [12864/26112] \t Train Loss(MAE): 0.7868 \t Train RMAE: 0.8870\nTrain Epoch: 222 [25664/26112] \t Train Loss(MAE): 0.7075 \t Train RMAE: 0.8412\n[Epoch: 222 \t Valid MAE: 1.1458\n[Epoch: 222 \t Train MAE: 0.7476\nEarlyStopping counter: 37 out of 70\nTrain Epoch: 223 [64/26112] \t Train Loss(MAE): 0.6880 \t Train RMAE: 0.8294\nTrain Epoch: 223 [12864/26112] \t Train Loss(MAE): 0.7358 \t Train RMAE: 0.8578\nTrain Epoch: 223 [25664/26112] \t Train Loss(MAE): 0.7544 \t Train RMAE: 0.8686\n[Epoch: 223 \t Valid MAE: 1.1459\n[Epoch: 223 \t Train MAE: 0.7474\nEarlyStopping counter: 38 out of 70\nTrain Epoch: 224 [64/26112] \t Train Loss(MAE): 1.2835 \t Train RMAE: 1.1329\nTrain Epoch: 224 [12864/26112] \t Train Loss(MAE): 0.6582 \t Train RMAE: 0.8113\nTrain Epoch: 224 [25664/26112] \t Train Loss(MAE): 0.7626 \t Train RMAE: 0.8732\n[Epoch: 224 \t Valid MAE: 1.1502\n[Epoch: 224 \t Train MAE: 0.7472\nEarlyStopping counter: 39 out of 70\nTrain Epoch: 225 [64/26112] \t Train Loss(MAE): 0.7048 \t Train RMAE: 0.8395\nTrain Epoch: 225 [12864/26112] \t Train Loss(MAE): 0.7643 \t Train RMAE: 0.8742\nTrain Epoch: 225 [25664/26112] \t Train Loss(MAE): 0.6314 \t Train RMAE: 0.7946\n[Epoch: 225 \t Valid MAE: 1.1459\n[Epoch: 225 \t Train MAE: 0.7471\nEarlyStopping counter: 40 out of 70\nTrain Epoch: 226 [64/26112] \t Train Loss(MAE): 0.5622 \t Train RMAE: 0.7498\nTrain Epoch: 226 [12864/26112] \t Train Loss(MAE): 0.4821 \t Train RMAE: 0.6944\nTrain Epoch: 226 [25664/26112] \t Train Loss(MAE): 0.4081 \t Train RMAE: 0.6388\n[Epoch: 226 \t Valid MAE: 1.1456\n[Epoch: 226 \t Train MAE: 0.7471\nEarlyStopping counter: 41 out of 70\nTrain Epoch: 227 [64/26112] \t Train Loss(MAE): 0.9320 \t Train RMAE: 0.9654\nTrain Epoch: 227 [12864/26112] \t Train Loss(MAE): 0.6269 \t Train RMAE: 0.7918\nTrain Epoch: 227 [25664/26112] \t Train Loss(MAE): 0.4854 \t Train RMAE: 0.6967\n[Epoch: 227 \t Valid MAE: 1.1457\n[Epoch: 227 \t Train MAE: 0.7471\nEarlyStopping counter: 42 out of 70\nTrain Epoch: 228 [64/26112] \t Train Loss(MAE): 0.7035 \t Train RMAE: 0.8387\nTrain Epoch: 228 [12864/26112] \t Train Loss(MAE): 1.0630 \t Train RMAE: 1.0310\nTrain Epoch: 228 [25664/26112] \t Train Loss(MAE): 0.8353 \t Train RMAE: 0.9140\n[Epoch: 228 \t Valid MAE: 1.1447\n[Epoch: 228 \t Train MAE: 0.7465\nEarlyStopping counter: 43 out of 70\nTrain Epoch: 229 [64/26112] \t Train Loss(MAE): 0.6922 \t Train RMAE: 0.8320\nTrain Epoch: 229 [12864/26112] \t Train Loss(MAE): 0.6186 \t Train RMAE: 0.7865\nTrain Epoch: 229 [25664/26112] \t Train Loss(MAE): 0.7004 \t Train RMAE: 0.8369\n[Epoch: 229 \t Valid MAE: 1.1491\n[Epoch: 229 \t Train MAE: 0.7465\nEarlyStopping counter: 44 out of 70\nTrain Epoch: 230 [64/26112] \t Train Loss(MAE): 0.7061 \t Train RMAE: 0.8403\nTrain Epoch: 230 [12864/26112] \t Train Loss(MAE): 0.5919 \t Train RMAE: 0.7694\nTrain Epoch: 230 [25664/26112] \t Train Loss(MAE): 1.0622 \t Train RMAE: 1.0307\n[Epoch: 230 \t Valid MAE: 1.1454\n[Epoch: 230 \t Train MAE: 0.7462\nEarlyStopping counter: 45 out of 70\nTrain Epoch: 231 [64/26112] \t Train Loss(MAE): 0.6940 \t Train RMAE: 0.8331\nTrain Epoch: 231 [12864/26112] \t Train Loss(MAE): 0.5768 \t Train RMAE: 0.7595\nTrain Epoch: 231 [25664/26112] \t Train Loss(MAE): 0.7907 \t Train RMAE: 0.8892\n[Epoch: 231 \t Valid MAE: 1.1435\n[Epoch: 231 \t Train MAE: 0.7459\nEarlyStopping counter: 46 out of 70\nTrain Epoch: 232 [64/26112] \t Train Loss(MAE): 0.7071 \t Train RMAE: 0.8409\nTrain Epoch: 232 [12864/26112] \t Train Loss(MAE): 0.7503 \t Train RMAE: 0.8662\nTrain Epoch: 232 [25664/26112] \t Train Loss(MAE): 0.6711 \t Train RMAE: 0.8192\n[Epoch: 232 \t Valid MAE: 1.1466\n[Epoch: 232 \t Train MAE: 0.7456\nEarlyStopping counter: 47 out of 70\nTrain Epoch: 233 [64/26112] \t Train Loss(MAE): 0.5116 \t Train RMAE: 0.7153\nTrain Epoch: 233 [12864/26112] \t Train Loss(MAE): 0.6434 \t Train RMAE: 0.8021\nTrain Epoch: 233 [25664/26112] \t Train Loss(MAE): 0.7171 \t Train RMAE: 0.8468\n[Epoch: 233 \t Valid MAE: 1.1471\n[Epoch: 233 \t Train MAE: 0.7457\nEarlyStopping counter: 48 out of 70\nTrain Epoch: 234 [64/26112] \t Train Loss(MAE): 0.7284 \t Train RMAE: 0.8535\nTrain Epoch: 234 [12864/26112] \t Train Loss(MAE): 0.5576 \t Train RMAE: 0.7468\nTrain Epoch: 234 [25664/26112] \t Train Loss(MAE): 0.6670 \t Train RMAE: 0.8167\n[Epoch: 234 \t Valid MAE: 1.1445\n[Epoch: 234 \t Train MAE: 0.7456\nEarlyStopping counter: 49 out of 70\nTrain Epoch: 235 [64/26112] \t Train Loss(MAE): 0.5853 \t Train RMAE: 0.7651\nTrain Epoch: 235 [12864/26112] \t Train Loss(MAE): 0.6291 \t Train RMAE: 0.7931\nTrain Epoch: 235 [25664/26112] \t Train Loss(MAE): 0.6338 \t Train RMAE: 0.7961\n[Epoch: 235 \t Valid MAE: 1.1445\n[Epoch: 235 \t Train MAE: 0.7449\nEarlyStopping counter: 50 out of 70\nTrain Epoch: 236 [64/26112] \t Train Loss(MAE): 0.7566 \t Train RMAE: 0.8698\nTrain Epoch: 236 [12864/26112] \t Train Loss(MAE): 0.4677 \t Train RMAE: 0.6839\nTrain Epoch: 236 [25664/26112] \t Train Loss(MAE): 0.9029 \t Train RMAE: 0.9502\n[Epoch: 236 \t Valid MAE: 1.1474\n[Epoch: 236 \t Train MAE: 0.7450\nEarlyStopping counter: 51 out of 70\nTrain Epoch: 237 [64/26112] \t Train Loss(MAE): 1.4696 \t Train RMAE: 1.2123\nTrain Epoch: 237 [12864/26112] \t Train Loss(MAE): 0.6853 \t Train RMAE: 0.8278\nTrain Epoch: 237 [25664/26112] \t Train Loss(MAE): 0.6111 \t Train RMAE: 0.7817\n[Epoch: 237 \t Valid MAE: 1.1456\n[Epoch: 237 \t Train MAE: 0.7449\nEarlyStopping counter: 52 out of 70\nTrain Epoch: 238 [64/26112] \t Train Loss(MAE): 0.4839 \t Train RMAE: 0.6957\nTrain Epoch: 238 [12864/26112] \t Train Loss(MAE): 0.4841 \t Train RMAE: 0.6958\nTrain Epoch: 238 [25664/26112] \t Train Loss(MAE): 0.4892 \t Train RMAE: 0.6994\n[Epoch: 238 \t Valid MAE: 1.1463\n[Epoch: 238 \t Train MAE: 0.7446\nEarlyStopping counter: 53 out of 70\nTrain Epoch: 239 [64/26112] \t Train Loss(MAE): 0.7191 \t Train RMAE: 0.8480\nTrain Epoch: 239 [12864/26112] \t Train Loss(MAE): 1.1671 \t Train RMAE: 1.0803\nTrain Epoch: 239 [25664/26112] \t Train Loss(MAE): 0.8160 \t Train RMAE: 0.9033\n[Epoch: 239 \t Valid MAE: 1.1453\n[Epoch: 239 \t Train MAE: 0.7443\nEarlyStopping counter: 54 out of 70\nTrain Epoch: 240 [64/26112] \t Train Loss(MAE): 0.8571 \t Train RMAE: 0.9258\nTrain Epoch: 240 [12864/26112] \t Train Loss(MAE): 0.7106 \t Train RMAE: 0.8429\nTrain Epoch: 240 [25664/26112] \t Train Loss(MAE): 0.8744 \t Train RMAE: 0.9351\n[Epoch: 240 \t Valid MAE: 1.1453\n[Epoch: 240 \t Train MAE: 0.7446\nEarlyStopping counter: 55 out of 70\nTrain Epoch: 241 [64/26112] \t Train Loss(MAE): 0.8405 \t Train RMAE: 0.9168\nTrain Epoch: 241 [12864/26112] \t Train Loss(MAE): 0.5976 \t Train RMAE: 0.7731\nTrain Epoch: 241 [25664/26112] \t Train Loss(MAE): 0.6072 \t Train RMAE: 0.7792\n[Epoch: 241 \t Valid MAE: 1.1466\n[Epoch: 241 \t Train MAE: 0.7443\nEarlyStopping counter: 56 out of 70\nTrain Epoch: 242 [64/26112] \t Train Loss(MAE): 0.5759 \t Train RMAE: 0.7589\nTrain Epoch: 242 [12864/26112] \t Train Loss(MAE): 0.7579 \t Train RMAE: 0.8706\nTrain Epoch: 242 [25664/26112] \t Train Loss(MAE): 0.7426 \t Train RMAE: 0.8617\n[Epoch: 242 \t Valid MAE: 1.1475\n[Epoch: 242 \t Train MAE: 0.7442\nEarlyStopping counter: 57 out of 70\nTrain Epoch: 243 [64/26112] \t Train Loss(MAE): 0.7116 \t Train RMAE: 0.8436\nTrain Epoch: 243 [12864/26112] \t Train Loss(MAE): 0.6704 \t Train RMAE: 0.8188\nTrain Epoch: 243 [25664/26112] \t Train Loss(MAE): 0.5032 \t Train RMAE: 0.7094\n[Epoch: 243 \t Valid MAE: 1.1456\n[Epoch: 243 \t Train MAE: 0.7439\nEarlyStopping counter: 58 out of 70\nTrain Epoch: 244 [64/26112] \t Train Loss(MAE): 0.6805 \t Train RMAE: 0.8249\nTrain Epoch: 244 [12864/26112] \t Train Loss(MAE): 0.7167 \t Train RMAE: 0.8466\nTrain Epoch: 244 [25664/26112] \t Train Loss(MAE): 1.2262 \t Train RMAE: 1.1074\n[Epoch: 244 \t Valid MAE: 1.1450\n[Epoch: 244 \t Train MAE: 0.7437\nEarlyStopping counter: 59 out of 70\nTrain Epoch: 245 [64/26112] \t Train Loss(MAE): 0.6745 \t Train RMAE: 0.8212\nTrain Epoch: 245 [12864/26112] \t Train Loss(MAE): 0.6637 \t Train RMAE: 0.8147\nTrain Epoch: 245 [25664/26112] \t Train Loss(MAE): 0.8664 \t Train RMAE: 0.9308\n[Epoch: 245 \t Valid MAE: 1.1448\n[Epoch: 245 \t Train MAE: 0.7433\nEarlyStopping counter: 60 out of 70\nTrain Epoch: 246 [64/26112] \t Train Loss(MAE): 0.7224 \t Train RMAE: 0.8500\nTrain Epoch: 246 [12864/26112] \t Train Loss(MAE): 0.5836 \t Train RMAE: 0.7639\nTrain Epoch: 246 [25664/26112] \t Train Loss(MAE): 0.8589 \t Train RMAE: 0.9268\n[Epoch: 246 \t Valid MAE: 1.1479\n[Epoch: 246 \t Train MAE: 0.7432\nEarlyStopping counter: 61 out of 70\nTrain Epoch: 247 [64/26112] \t Train Loss(MAE): 0.6848 \t Train RMAE: 0.8275\nTrain Epoch: 247 [12864/26112] \t Train Loss(MAE): 0.8874 \t Train RMAE: 0.9420\nTrain Epoch: 247 [25664/26112] \t Train Loss(MAE): 1.0517 \t Train RMAE: 1.0255\n[Epoch: 247 \t Valid MAE: 1.1462\n[Epoch: 247 \t Train MAE: 0.7427\nEarlyStopping counter: 62 out of 70\nTrain Epoch: 248 [64/26112] \t Train Loss(MAE): 0.6485 \t Train RMAE: 0.8053\nTrain Epoch: 248 [12864/26112] \t Train Loss(MAE): 0.7361 \t Train RMAE: 0.8579\nTrain Epoch: 248 [25664/26112] \t Train Loss(MAE): 0.5021 \t Train RMAE: 0.7086\n[Epoch: 248 \t Valid MAE: 1.1439\n[Epoch: 248 \t Train MAE: 0.7430\nEarlyStopping counter: 63 out of 70\nTrain Epoch: 249 [64/26112] \t Train Loss(MAE): 0.7960 \t Train RMAE: 0.8922\nTrain Epoch: 249 [12864/26112] \t Train Loss(MAE): 0.9051 \t Train RMAE: 0.9513\nTrain Epoch: 249 [25664/26112] \t Train Loss(MAE): 0.4860 \t Train RMAE: 0.6971\n[Epoch: 249 \t Valid MAE: 1.1435\n[Epoch: 249 \t Train MAE: 0.7427\nEarlyStopping counter: 64 out of 70\nTrain Epoch: 250 [64/26112] \t Train Loss(MAE): 0.5267 \t Train RMAE: 0.7257\nTrain Epoch: 250 [12864/26112] \t Train Loss(MAE): 0.6438 \t Train RMAE: 0.8024\nTrain Epoch: 250 [25664/26112] \t Train Loss(MAE): 0.7269 \t Train RMAE: 0.8526\n[Epoch: 250 \t Valid MAE: 1.1461\n[Epoch: 250 \t Train MAE: 0.7420\nEarlyStopping counter: 65 out of 70\nTrain Epoch: 251 [64/26112] \t Train Loss(MAE): 0.6472 \t Train RMAE: 0.8045\nTrain Epoch: 251 [12864/26112] \t Train Loss(MAE): 1.0416 \t Train RMAE: 1.0206\nTrain Epoch: 251 [25664/26112] \t Train Loss(MAE): 0.8147 \t Train RMAE: 0.9026\n[Epoch: 251 \t Valid MAE: 1.1449\n[Epoch: 251 \t Train MAE: 0.7421\nEarlyStopping counter: 66 out of 70\nTrain Epoch: 252 [64/26112] \t Train Loss(MAE): 0.5719 \t Train RMAE: 0.7563\nTrain Epoch: 252 [12864/26112] \t Train Loss(MAE): 0.6017 \t Train RMAE: 0.7757\nTrain Epoch: 252 [25664/26112] \t Train Loss(MAE): 1.0338 \t Train RMAE: 1.0167\n[Epoch: 252 \t Valid MAE: 1.1445\n[Epoch: 252 \t Train MAE: 0.7422\nEarlyStopping counter: 67 out of 70\nTrain Epoch: 253 [64/26112] \t Train Loss(MAE): 0.4806 \t Train RMAE: 0.6933\nTrain Epoch: 253 [12864/26112] \t Train Loss(MAE): 0.7331 \t Train RMAE: 0.8562\nTrain Epoch: 253 [25664/26112] \t Train Loss(MAE): 1.2462 \t Train RMAE: 1.1163\n[Epoch: 253 \t Valid MAE: 1.1432\n[Epoch: 253 \t Train MAE: 0.7423\nEarlyStopping counter: 68 out of 70\nTrain Epoch: 254 [64/26112] \t Train Loss(MAE): 0.5372 \t Train RMAE: 0.7330\nTrain Epoch: 254 [12864/26112] \t Train Loss(MAE): 0.5386 \t Train RMAE: 0.7339\nTrain Epoch: 254 [25664/26112] \t Train Loss(MAE): 0.6175 \t Train RMAE: 0.7858\n[Epoch: 254 \t Valid MAE: 1.1466\n[Epoch: 254 \t Train MAE: 0.7418\nEarlyStopping counter: 69 out of 70\nTrain Epoch: 255 [64/26112] \t Train Loss(MAE): 1.4419 \t Train RMAE: 1.2008\nTrain Epoch: 255 [12864/26112] \t Train Loss(MAE): 1.0225 \t Train RMAE: 1.0112\nTrain Epoch: 255 [25664/26112] \t Train Loss(MAE): 0.9793 \t Train RMAE: 0.9896\n[Epoch: 255 \t Valid MAE: 1.1450\n[Epoch: 255 \t Train MAE: 0.7412\nEarlyStopping counter: 70 out of 70\nEarly stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Valid best:',best_mae)\nprint('Train best:',best_train_mae)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:34.875114Z","iopub.execute_input":"2023-12-12T10:30:34.875843Z","iopub.status.idle":"2023-12-12T10:30:34.881033Z","shell.execute_reply.started":"2023-12-12T10:30:34.875802Z","shell.execute_reply":"2023-12-12T10:30:34.879808Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Valid best: 1.1429130072687186\nTrain best: 0.7712854650967261\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Epoch visualization","metadata":{}},{"cell_type":"code","source":"# visualizing\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,10))\nax1 = fig.add_subplot(1,1,1)\nax1.plot(train_mae_list, ls='-', color='blue', label='train')\nax1.set_ylim(0,3)\n\nax2 = ax1.twinx()\nax2.plot(valid_mae_list, ls='--', color='red', label='valid')\nax2.set_ylim(0,3)\n\nax1.set_title('MAE error')\nax1.legend(loc='upper right')\nax2.legend(loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:32:08.521675Z","iopub.execute_input":"2023-12-12T10:32:08.522716Z","iopub.status.idle":"2023-12-12T10:32:08.999513Z","shell.execute_reply.started":"2023-12-12T10:32:08.522677Z","shell.execute_reply":"2023-12-12T10:32:08.998510Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x1000 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Best performance","metadata":{}},{"cell_type":"code","source":"# get weights\nmodel = Distance_Estimator(input_dim, hidden_dim,layer_dim)\nmodel.load_state_dict(torch.load('./weights/lstm_model.pth'))\nmodel.eval()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:32:18.083010Z","iopub.execute_input":"2023-12-12T10:32:18.083392Z","iopub.status.idle":"2023-12-12T10:32:18.187287Z","shell.execute_reply.started":"2023-12-12T10:32:18.083361Z","shell.execute_reply":"2023-12-12T10:32:18.186305Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"Distance_Estimator(\n  (rnn): LSTM(10, 612, num_layers=3, batch_first=True)\n  (fc): Sequential(\n    (0): Linear(in_features=612, out_features=306, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=306, out_features=154, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=154, out_features=76, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=76, out_features=1, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Predict Train","metadata":{}},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=len(df_train), shuffle=False)\nfor idx, batch in enumerate(train_dataloader):\n    if idx == 1:\n        break\n    train_pred = batch[0]\npredict_zloc = model(train_pred.reshape(-1,1,input_dim).to(device))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:32:23.025259Z","iopub.execute_input":"2023-12-12T10:32:23.026174Z","iopub.status.idle":"2023-12-12T10:32:23.047311Z","shell.execute_reply.started":"2023-12-12T10:32:23.026143Z","shell.execute_reply":"2023-12-12T10:32:23.046568Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"df_train['predict'] = predict_zloc.cpu().detach().numpy()\ndf_train[['zloc','predict']].head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:32:23.498235Z","iopub.execute_input":"2023-12-12T10:32:23.498952Z","iopub.status.idle":"2023-12-12T10:32:23.513086Z","shell.execute_reply.started":"2023-12-12T10:32:23.498918Z","shell.execute_reply":"2023-12-12T10:32:23.512113Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/3425435218.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train['predict'] = predict_zloc.cpu().detach().numpy()\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"      zloc    predict\n4    48.09  71.609116\n30   32.43  79.611343\n41    9.37  36.253368\n58   37.82  33.471336\n90   26.58  16.454918\n92    7.07  29.595739\n104  43.21  67.606155\n118  66.09  11.971720\n132  62.19  64.246048\n142  43.45  10.558659","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zloc</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>48.09</td>\n      <td>71.609116</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>32.43</td>\n      <td>79.611343</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>9.37</td>\n      <td>36.253368</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>37.82</td>\n      <td>33.471336</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>26.58</td>\n      <td>16.454918</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>7.07</td>\n      <td>29.595739</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>43.21</td>\n      <td>67.606155</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>66.09</td>\n      <td>11.971720</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>62.19</td>\n      <td>64.246048</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>43.45</td>\n      <td>10.558659</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# calculate\nimport numpy as np\nabs0 = np.abs(df_train.zloc-df_train.predict)\nabs0","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:35.871994Z","iopub.execute_input":"2023-12-12T10:30:35.872326Z","iopub.status.idle":"2023-12-12T10:30:35.882226Z","shell.execute_reply.started":"2023-12-12T10:30:35.872293Z","shell.execute_reply":"2023-12-12T10:30:35.881181Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0        0.019116\n1        0.608672\n2        1.833368\n3        0.578667\n4        0.305086\n           ...   \n26098    0.281758\n26099    1.574234\n26100    0.121302\n26101    0.796267\n26102    0.350783\nLength: 26103, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# mae\nsum(abs0/len(df_train))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:35.883478Z","iopub.execute_input":"2023-12-12T10:30:35.884060Z","iopub.status.idle":"2023-12-12T10:30:35.893396Z","shell.execute_reply.started":"2023-12-12T10:30:35.884033Z","shell.execute_reply":"2023-12-12T10:30:35.892568Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.7655633443789195"},"metadata":{}}]},{"cell_type":"code","source":"# rmse\nnp.mean(np.square(df_train['zloc']-df_train['predict']))**(1/2)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:35.894384Z","iopub.execute_input":"2023-12-12T10:30:35.894633Z","iopub.status.idle":"2023-12-12T10:30:35.908027Z","shell.execute_reply.started":"2023-12-12T10:30:35.894611Z","shell.execute_reply":"2023-12-12T10:30:35.907128Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"2.1058508613888196"},"metadata":{}}]},{"cell_type":"code","source":"# accuracy\nfunc = np.sum(np.abs((df_train.predict - df_train.zloc))/(df_train.predict))\nmen = func/len(df_train)\n1-men","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:35.909093Z","iopub.execute_input":"2023-12-12T10:30:35.909345Z","iopub.status.idle":"2023-12-12T10:30:35.921254Z","shell.execute_reply.started":"2023-12-12T10:30:35.909322Z","shell.execute_reply":"2023-12-12T10:30:35.920275Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.9711977637560565"},"metadata":{}}]},{"cell_type":"markdown","source":"# Predict Valid","metadata":{}},{"cell_type":"code","source":"# valid set\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=len(df_train), shuffle=False)\nfor idx, batch in enumerate(valid_dataloader):\n    if idx == 1:\n        break\n    valid_pred = batch[0]\npredict_zloc = model(valid_pred.reshape(-1,1,input_dim).to(device))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:35.925586Z","iopub.execute_input":"2023-12-12T10:30:35.925871Z","iopub.status.idle":"2023-12-12T10:30:36.016399Z","shell.execute_reply.started":"2023-12-12T10:30:35.925847Z","shell.execute_reply":"2023-12-12T10:30:36.015576Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# estimate valid\ndf_valid['predict'] = predict_zloc.cpu().detach().numpy()\ndf_valid[['zloc','predict']].head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.017392Z","iopub.execute_input":"2023-12-12T10:30:36.017714Z","iopub.status.idle":"2023-12-12T10:30:36.032346Z","shell.execute_reply.started":"2023-12-12T10:30:36.017681Z","shell.execute_reply":"2023-12-12T10:30:36.031566Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"    zloc    predict\n0  11.15  11.230824\n1  41.50  41.804409\n2  24.17  24.604700\n3  35.94  35.712772\n4  10.83  11.050531\n5  36.92  34.495800\n6  25.10  24.720335\n7   9.43   9.469545\n8  13.85  14.077168\n9  38.33  39.047859","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zloc</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11.15</td>\n      <td>11.230824</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41.50</td>\n      <td>41.804409</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24.17</td>\n      <td>24.604700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.94</td>\n      <td>35.712772</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.83</td>\n      <td>11.050531</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>36.92</td>\n      <td>34.495800</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>25.10</td>\n      <td>24.720335</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>9.43</td>\n      <td>9.469545</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>13.85</td>\n      <td>14.077168</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>38.33</td>\n      <td>39.047859</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"abs0 = np.abs(df_valid.zloc-df_valid.predict)\nabs0","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.033246Z","iopub.execute_input":"2023-12-12T10:30:36.033494Z","iopub.status.idle":"2023-12-12T10:30:36.041297Z","shell.execute_reply.started":"2023-12-12T10:30:36.033471Z","shell.execute_reply":"2023-12-12T10:30:36.040357Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0       0.080824\n1       0.304409\n2       0.434700\n3       0.227228\n4       0.220531\n          ...   \n3257    0.900149\n3258    0.951438\n3259    0.631634\n3260    1.834403\n3261    1.201801\nLength: 3262, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# mae\nsum(abs0/len(df_valid))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.042680Z","iopub.execute_input":"2023-12-12T10:30:36.043029Z","iopub.status.idle":"2023-12-12T10:30:36.055220Z","shell.execute_reply.started":"2023-12-12T10:30:36.042995Z","shell.execute_reply":"2023-12-12T10:30:36.054421Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"1.1428284937000792"},"metadata":{}}]},{"cell_type":"code","source":"#rmse\nnp.mean(np.square(df_valid['zloc']-df_valid['predict']))**(1/2)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.056160Z","iopub.execute_input":"2023-12-12T10:30:36.056399Z","iopub.status.idle":"2023-12-12T10:30:36.069434Z","shell.execute_reply.started":"2023-12-12T10:30:36.056377Z","shell.execute_reply":"2023-12-12T10:30:36.068565Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"2.4276795608315695"},"metadata":{}}]},{"cell_type":"code","source":"# accuracy\nfunc = np.sum(np.abs((df_valid.predict - df_valid.zloc))/(df_valid.predict))\nmen = func/len(df_valid)\n1-men","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.070623Z","iopub.execute_input":"2023-12-12T10:30:36.070944Z","iopub.status.idle":"2023-12-12T10:30:36.081927Z","shell.execute_reply.started":"2023-12-12T10:30:36.070909Z","shell.execute_reply":"2023-12-12T10:30:36.080986Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.9555214845524247"},"metadata":{}}]},{"cell_type":"markdown","source":"# Predict Test","metadata":{}},{"cell_type":"code","source":"test_mse, test_rmse = evaluate(model, test_dataloader)\nprint('Test MAE: {:4f} \\t Test RMAE: {:4f}'.format(test_mse, test_rmse))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.082988Z","iopub.execute_input":"2023-12-12T10:30:36.083244Z","iopub.status.idle":"2023-12-12T10:30:36.179286Z","shell.execute_reply.started":"2023-12-12T10:30:36.083221Z","shell.execute_reply":"2023-12-12T10:30:36.178474Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Test MAE: 1.201929 \t Test RMAE: 1.096325\n","output_type":"stream"}]},{"cell_type":"code","source":"# look dataset\nfor idx, batch in enumerate(test_dataloader):\n    if idx == 1:\n        break\n    test_pred = batch[0]\npredict_zloc = model(test_pred.reshape(-1,1,input_dim).to(device))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.180243Z","iopub.execute_input":"2023-12-12T10:30:36.180536Z","iopub.status.idle":"2023-12-12T10:30:36.259610Z","shell.execute_reply.started":"2023-12-12T10:30:36.180510Z","shell.execute_reply":"2023-12-12T10:30:36.258739Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"df_test['predict'] = predict_zloc.cpu().detach().numpy()\ndf_test[['zloc','predict']].head(10)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.261222Z","iopub.execute_input":"2023-12-12T10:30:36.261559Z","iopub.status.idle":"2023-12-12T10:30:36.275579Z","shell.execute_reply.started":"2023-12-12T10:30:36.261528Z","shell.execute_reply":"2023-12-12T10:30:36.274635Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"    zloc    predict\n0  73.41  71.486458\n1  22.08  22.033094\n2  76.20  75.203117\n3  45.30  42.444019\n4  48.09  48.273182\n5  47.46  48.867748\n6   8.41   8.588284\n7  26.58  27.266050\n8   9.24   9.434153\n9  14.90  15.200146","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>zloc</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>73.41</td>\n      <td>71.486458</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22.08</td>\n      <td>22.033094</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>76.20</td>\n      <td>75.203117</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45.30</td>\n      <td>42.444019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>48.09</td>\n      <td>48.273182</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>47.46</td>\n      <td>48.867748</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8.41</td>\n      <td>8.588284</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>26.58</td>\n      <td>27.266050</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9.24</td>\n      <td>9.434153</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14.90</td>\n      <td>15.200146</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nabs0 = np.abs(df_test.zloc-df_test.predict)\nabs0","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.276725Z","iopub.execute_input":"2023-12-12T10:30:36.277006Z","iopub.status.idle":"2023-12-12T10:30:36.284975Z","shell.execute_reply.started":"2023-12-12T10:30:36.276983Z","shell.execute_reply":"2023-12-12T10:30:36.284127Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"0       1.923542\n1       0.046906\n2       0.996883\n3       2.855981\n4       0.183182\n          ...   \n3259    1.094955\n3260    0.014641\n3261    0.328580\n3262    0.415169\n3263    1.366924\nLength: 3264, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# mae\nsum(abs0/len(df_test))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.285995Z","iopub.execute_input":"2023-12-12T10:30:36.286270Z","iopub.status.idle":"2023-12-12T10:30:36.297660Z","shell.execute_reply.started":"2023-12-12T10:30:36.286246Z","shell.execute_reply":"2023-12-12T10:30:36.296808Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"1.2019288631806195"},"metadata":{}}]},{"cell_type":"code","source":"# rmse\nnp.mean(np.square(df_test['zloc']-df_test['predict']))**(1/2)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-12T10:30:36.298791Z","iopub.execute_input":"2023-12-12T10:30:36.299068Z","iopub.status.idle":"2023-12-12T10:30:36.311828Z","shell.execute_reply.started":"2023-12-12T10:30:36.299043Z","shell.execute_reply":"2023-12-12T10:30:36.310813Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"2.8692870382465436"},"metadata":{}}]},{"cell_type":"code","source":"# accuracy\nfunc = np.sum(np.abs((df_test.predict - df_test.zloc))/(df_test.predict))\nmen = func/len(df_test)\n1-men","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.313008Z","iopub.execute_input":"2023-12-12T10:30:36.313311Z","iopub.status.idle":"2023-12-12T10:30:36.325347Z","shell.execute_reply.started":"2023-12-12T10:30:36.313285Z","shell.execute_reply":"2023-12-12T10:30:36.324521Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0.9518226141679321"},"metadata":{}}]},{"cell_type":"code","source":"df_test['class'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.326288Z","iopub.execute_input":"2023-12-12T10:30:36.326568Z","iopub.status.idle":"2023-12-12T10:30:36.347666Z","shell.execute_reply.started":"2023-12-12T10:30:36.326538Z","shell.execute_reply":"2023-12-12T10:30:36.346898Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"array(['Car', 'Truck', 'Van', 'Pedestrian', 'Cyclist', 'Person_sitting',\n       'Tram', 'Misc'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"matrix = pd.DataFrame(columns=['type','RMSE','MAE','Accuracy'])\nmatrix","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.348969Z","iopub.execute_input":"2023-12-12T10:30:36.349285Z","iopub.status.idle":"2023-12-12T10:30:36.363997Z","shell.execute_reply.started":"2023-12-12T10:30:36.349254Z","shell.execute_reply":"2023-12-12T10:30:36.362986Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [type, RMSE, MAE, Accuracy]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Truck\ntruck = df_test['class']=='Truck'\ndf_truck = df_test[truck]\n\n# mae\nabs0 = np.abs(df_truck.zloc-df_truck.predict)\nprint(sum(abs0/len(df_truck))) # 1.8629\n      \n# rmse \nprint(np.mean(np.square(df_truck['zloc']-df_truck['predict']))**(1/2)) # 3.2170\n\n# accuracy\nfunc = np.sum(np.abs((df_truck.predict - df_truck.zloc))/(df_truck.predict))\nmen = func/len(df_truck)\nprint(1-men) # 0.9376\n\nmatrix.loc[0,'type'] = 'Truck'\nmatrix.loc[0,'RMSE'] = round(np.mean(np.square(df_truck['zloc']-df_truck['predict']))**(1/2),4)\nmatrix.loc[0,'MAE'] = round(sum(abs0/len(df_truck)),4)\nmatrix.loc[0,'Accuracy'] = round(1-men,4)\nmatrix","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.365134Z","iopub.execute_input":"2023-12-12T10:30:36.365394Z","iopub.status.idle":"2023-12-12T10:30:36.389696Z","shell.execute_reply.started":"2023-12-12T10:30:36.365372Z","shell.execute_reply":"2023-12-12T10:30:36.388806Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"1.4844431055206617\n3.4974685312081535\n0.9605969800319697\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"    type    RMSE     MAE Accuracy\n0  Truck  3.4975  1.4844   0.9606","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Truck</td>\n      <td>3.4975</td>\n      <td>1.4844</td>\n      <td>0.9606</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# car\ncar = df_test['class']=='Car'\ndf_car = df_test[car]\n\n# mae\nabs0 = np.abs(df_car.zloc-df_car.predict)\nprint(sum(abs0/len(df_car))) # 1.2531\n      \n# rmse \nprint(np.mean(np.square(df_car['zloc']-df_car['predict']))**(1/2)) # 2.2713\n\n# accuracy\nfunc = np.sum(np.abs((df_car.predict - df_car.zloc))/(df_car.predict))\nmen = func/len(df_car)\nprint(1-men) # 0.9519\n\nmatrix.loc[1,'type'] = 'Car'\nmatrix.loc[1,'RMSE'] = round(np.mean(np.square(df_car['zloc']-df_car['predict']))**(1/2),4)\nmatrix.loc[1,'MAE'] = round(sum(abs0/len(df_car)),4)\nmatrix.loc[1,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.391042Z","iopub.execute_input":"2023-12-12T10:30:36.391353Z","iopub.status.idle":"2023-12-12T10:30:36.405298Z","shell.execute_reply.started":"2023-12-12T10:30:36.391330Z","shell.execute_reply":"2023-12-12T10:30:36.404390Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"1.1581840113902775\n3.0301865830902357\n0.9536600351384275\n","output_type":"stream"}]},{"cell_type":"code","source":"# person\nperson = df_test['class']=='Pedestrian'\ndf_person = df_test[person]\n\n# mae\nabs0 = np.abs(df_person.zloc-df_person.predict)\nprint(sum(abs0/len(df_person))) # 0.7012\n      \n# rmse \nprint(np.mean(np.square(df_person['zloc']-df_person['predict']))**(1/2)) # 1.2880\n\n# accuracy\nfunc = np.sum(np.abs((df_person.predict - df_person.zloc))/(df_person.predict))\nmen = func/len(df_person)\nprint(1-men) # 0.9529\n\nmatrix.loc[2,'type'] = 'Pedestrian'\nmatrix.loc[2,'RMSE'] = round(np.mean(np.square(df_person['zloc']-df_person['predict']))**(1/2),4)\nmatrix.loc[2,'MAE'] = round(sum(abs0/len(df_person)),4)\nmatrix.loc[2,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.406380Z","iopub.execute_input":"2023-12-12T10:30:36.406699Z","iopub.status.idle":"2023-12-12T10:30:36.420213Z","shell.execute_reply.started":"2023-12-12T10:30:36.406667Z","shell.execute_reply":"2023-12-12T10:30:36.419268Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"0.7829133267238224\n1.6355795536599296\n0.9568187583015592\n","output_type":"stream"}]},{"cell_type":"code","source":"# Van\ntrain = df_test['class']=='Van'\ndf_train = df_test[train] \n\n# mae\nabs0 = np.abs(df_train.zloc-df_train.predict)\nprint(sum(abs0/len(df_train)))  # 1.6821\n      \n# rmse \nprint(np.mean(np.square(df_train['zloc']-df_train['predict']))**(1/2)) # 2.3989\n\n# accuracy\nfunc = np.sum(np.abs((df_train.predict - df_train.zloc))/(df_train.predict))\nmen = func/len(df_train)\nprint(1-men) # 0.8611\n\nmatrix.loc[3,'type'] = 'Van'\nmatrix.loc[3,'RMSE'] = round(np.mean(np.square(df_train['zloc']-df_train['predict']))**(1/2),4)\nmatrix.loc[3,'MAE'] = round(sum(abs0/len(df_train)),4)\nmatrix.loc[3,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.426520Z","iopub.execute_input":"2023-12-12T10:30:36.426917Z","iopub.status.idle":"2023-12-12T10:30:36.442702Z","shell.execute_reply.started":"2023-12-12T10:30:36.426891Z","shell.execute_reply":"2023-12-12T10:30:36.441732Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"2.0738795154094682\n3.32259406488358\n0.9359776498837414\n","output_type":"stream"}]},{"cell_type":"code","source":"# misc\nmisc = df_test['class']=='Misc'\ndf_misc = df_test[misc] \n\n# mae\nabs0 = np.abs(df_misc.zloc-df_misc.predict)\nprint(sum(abs0/len(df_misc)))  # 1.2972\n      \n# rmse \nprint(np.mean(np.square(df_misc['zloc']-df_misc['predict']))**(1/2)) # 1.7389\n\n# accuracy\nfunc = np.sum(np.abs((df_misc.predict - df_misc.zloc))/(df_misc.predict))\nmen = func/len(df_misc)\nprint(1-men) # 0.9384\n\nmatrix.loc[4,'type'] = 'Misc'\nmatrix.loc[4,'RMSE'] = round(np.mean(np.square(df_misc['zloc']-df_misc['predict']))**(1/2),4)\nmatrix.loc[4,'MAE'] = round(sum(abs0/len(df_misc)),4)\nmatrix.loc[4,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.443997Z","iopub.execute_input":"2023-12-12T10:30:36.444706Z","iopub.status.idle":"2023-12-12T10:30:36.458705Z","shell.execute_reply.started":"2023-12-12T10:30:36.444666Z","shell.execute_reply":"2023-12-12T10:30:36.457833Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"1.3374670126563621\n2.082981459566981\n0.9402009516947851\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cyclist\nbicycle = df_test['class']=='Cyclist'\ndf_bicycle = df_test[bicycle] \n\n# mae\nabs0 = np.abs(df_bicycle.zloc-df_bicycle.predict)\nprint(sum(abs0/len(df_bicycle)))  # 1.0336\n      \n# rmse \nprint(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2)) # 1.1845\n\n# accuracy\nfunc = np.sum(np.abs((df_bicycle.predict - df_bicycle.zloc))/(df_bicycle.predict))\nmen = func/len(df_bicycle)\nprint(1-men) # 0.9392\n\nmatrix.loc[5,'type'] = 'Cyclist'\nmatrix.loc[5,'RMSE'] = round(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2),4)\nmatrix.loc[5,'MAE'] = round(sum(abs0/len(df_bicycle)),4)\nmatrix.loc[5,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.459774Z","iopub.execute_input":"2023-12-12T10:30:36.460088Z","iopub.status.idle":"2023-12-12T10:30:36.475712Z","shell.execute_reply.started":"2023-12-12T10:30:36.460061Z","shell.execute_reply":"2023-12-12T10:30:36.474790Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"1.0942258212740057\n1.6678602926674388\n0.938864501631802\n","output_type":"stream"}]},{"cell_type":"code","source":"# Person_sitting\nbicycle = df_test['class']=='Person_sitting'\ndf_bicycle = df_test[bicycle] \n\n# mae\nabs0 = np.abs(df_bicycle.zloc-df_bicycle.predict)\nprint(sum(abs0/len(df_bicycle)))  # 1.0336\n      \n# rmse \nprint(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2)) # 1.1845\n\n# accuracy\nfunc = np.sum(np.abs((df_bicycle.predict - df_bicycle.zloc))/(df_bicycle.predict))\nmen = func/len(df_bicycle)\nprint(1-men) # 0.9392\n\nmatrix.loc[6,'type'] = 'Person_sitting'\nmatrix.loc[6,'RMSE'] = round(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2),4)\nmatrix.loc[6,'MAE'] = round(sum(abs0/len(df_bicycle)),4)\nmatrix.loc[6,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.477057Z","iopub.execute_input":"2023-12-12T10:30:36.477369Z","iopub.status.idle":"2023-12-12T10:30:36.494763Z","shell.execute_reply.started":"2023-12-12T10:30:36.477343Z","shell.execute_reply":"2023-12-12T10:30:36.494057Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"1.4534618403116861\n3.028346086648422\n0.9066300069874211\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tram\nbicycle = df_test['class']=='Tram'\ndf_bicycle = df_test[bicycle] \n\n# mae\nabs0 = np.abs(df_bicycle.zloc-df_bicycle.predict)\nprint(sum(abs0/len(df_bicycle)))  # 1.0336\n      \n# rmse \nprint(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2)) # 1.1845\n\n# accuracy\nfunc = np.sum(np.abs((df_bicycle.predict - df_bicycle.zloc))/(df_bicycle.predict))\nmen = func/len(df_bicycle)\nprint(1-men) # 0.9392\n\nmatrix.loc[7,'type'] = 'Tram'\nmatrix.loc[7,'RMSE'] = round(np.mean(np.square(df_bicycle['zloc']-df_bicycle['predict']))**(1/2),4)\nmatrix.loc[7,'MAE'] = round(sum(abs0/len(df_bicycle)),4)\nmatrix.loc[7,'Accuracy'] = round(1-men,4)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.495892Z","iopub.execute_input":"2023-12-12T10:30:36.496180Z","iopub.status.idle":"2023-12-12T10:30:36.513158Z","shell.execute_reply.started":"2023-12-12T10:30:36.496155Z","shell.execute_reply":"2023-12-12T10:30:36.512411Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"1.4596585133870448\n1.9569119228836\n0.9633662000599112\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame\nmatrix","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.514259Z","iopub.execute_input":"2023-12-12T10:30:36.514535Z","iopub.status.idle":"2023-12-12T10:30:36.531368Z","shell.execute_reply.started":"2023-12-12T10:30:36.514511Z","shell.execute_reply":"2023-12-12T10:30:36.530349Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"             type    RMSE     MAE Accuracy\n0           Truck  3.4975  1.4844   0.9606\n1             Car  3.0302  1.1582   0.9537\n2      Pedestrian  1.6356  0.7829   0.9568\n3             Van  3.3226  2.0739    0.936\n4            Misc   2.083  1.3375   0.9402\n5         Cyclist  1.6679  1.0942   0.9389\n6  Person_sitting  3.0283  1.4535   0.9066\n7            Tram  1.9569  1.4597   0.9634","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Truck</td>\n      <td>3.4975</td>\n      <td>1.4844</td>\n      <td>0.9606</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Car</td>\n      <td>3.0302</td>\n      <td>1.1582</td>\n      <td>0.9537</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pedestrian</td>\n      <td>1.6356</td>\n      <td>0.7829</td>\n      <td>0.9568</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Van</td>\n      <td>3.3226</td>\n      <td>2.0739</td>\n      <td>0.936</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Misc</td>\n      <td>2.083</td>\n      <td>1.3375</td>\n      <td>0.9402</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Cyclist</td>\n      <td>1.6679</td>\n      <td>1.0942</td>\n      <td>0.9389</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Person_sitting</td>\n      <td>3.0283</td>\n      <td>1.4535</td>\n      <td>0.9066</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Tram</td>\n      <td>1.9569</td>\n      <td>1.4597</td>\n      <td>0.9634</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"matrix.set_index('type', inplace=True)\nmatrix.loc[['Misc','Truck','Car','Pedestrian','Van','Cyclist','Person_sitting','Tram'],['RMSE','MAE','Accuracy']]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:30:36.533460Z","iopub.execute_input":"2023-12-12T10:30:36.533721Z","iopub.status.idle":"2023-12-12T10:30:36.547794Z","shell.execute_reply.started":"2023-12-12T10:30:36.533698Z","shell.execute_reply":"2023-12-12T10:30:36.546819Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"                  RMSE     MAE Accuracy\ntype                                   \nMisc             2.083  1.3375   0.9402\nTruck           3.4975  1.4844   0.9606\nCar             3.0302  1.1582   0.9537\nPedestrian      1.6356  0.7829   0.9568\nVan             3.3226  2.0739    0.936\nCyclist         1.6679  1.0942   0.9389\nPerson_sitting  3.0283  1.4535   0.9066\nTram            1.9569  1.4597   0.9634","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>Accuracy</th>\n    </tr>\n    <tr>\n      <th>type</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Misc</th>\n      <td>2.083</td>\n      <td>1.3375</td>\n      <td>0.9402</td>\n    </tr>\n    <tr>\n      <th>Truck</th>\n      <td>3.4975</td>\n      <td>1.4844</td>\n      <td>0.9606</td>\n    </tr>\n    <tr>\n      <th>Car</th>\n      <td>3.0302</td>\n      <td>1.1582</td>\n      <td>0.9537</td>\n    </tr>\n    <tr>\n      <th>Pedestrian</th>\n      <td>1.6356</td>\n      <td>0.7829</td>\n      <td>0.9568</td>\n    </tr>\n    <tr>\n      <th>Van</th>\n      <td>3.3226</td>\n      <td>2.0739</td>\n      <td>0.936</td>\n    </tr>\n    <tr>\n      <th>Cyclist</th>\n      <td>1.6679</td>\n      <td>1.0942</td>\n      <td>0.9389</td>\n    </tr>\n    <tr>\n      <th>Person_sitting</th>\n      <td>3.0283</td>\n      <td>1.4535</td>\n      <td>0.9066</td>\n    </tr>\n    <tr>\n      <th>Tram</th>\n      <td>1.9569</td>\n      <td>1.4597</td>\n      <td>0.9634</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Calculating Metrics Based on Other Papers","metadata":{}},{"cell_type":"code","source":"performance = pd.DataFrame(index=['LSTM'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:13.068831Z","iopub.execute_input":"2023-12-12T10:33:13.069203Z","iopub.status.idle":"2023-12-12T10:33:13.074475Z","shell.execute_reply.started":"2023-12-12T10:33:13.069176Z","shell.execute_reply":"2023-12-12T10:33:13.073459Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Abs Relative difference (Abs Rel)\nAbs_rel = np.sum(np.abs(df_test.predict - df_test.zloc)/df_test.zloc)/len(df_test)\nprint('Abs_rel', Abs_rel) \nperformance['Abs_rel'] = round(Abs_rel,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:13.427200Z","iopub.execute_input":"2023-12-12T10:33:13.427541Z","iopub.status.idle":"2023-12-12T10:33:13.434806Z","shell.execute_reply.started":"2023-12-12T10:33:13.427510Z","shell.execute_reply":"2023-12-12T10:33:13.433791Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Abs_rel 0.04999145271669729\n","output_type":"stream"}]},{"cell_type":"code","source":"Squa_rel = np.sum((df_test.predict - df_test.zloc)**2/df_test.zloc)/len(df_test)\nprint('Squa_rel:',Squa_rel) \nperformance['Squa_rel'] = round(Squa_rel,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:13.973092Z","iopub.execute_input":"2023-12-12T10:33:13.973478Z","iopub.status.idle":"2023-12-12T10:33:13.980813Z","shell.execute_reply.started":"2023-12-12T10:33:13.973446Z","shell.execute_reply":"2023-12-12T10:33:13.979931Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Squa_rel: 0.2839704546633655\n","output_type":"stream"}]},{"cell_type":"code","source":"RMSE_log = np.sum(np.sqrt(((np.log(df_test.predict)-np.log(df_test.zloc))**2))/len(df_test))\nprint('RMSE_log', RMSE_log)\nperformance['RMSE_log'] = round(RMSE_log,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:17.016944Z","iopub.execute_input":"2023-12-12T10:33:17.017311Z","iopub.status.idle":"2023-12-12T10:33:17.026771Z","shell.execute_reply.started":"2023-12-12T10:33:17.017281Z","shell.execute_reply":"2023-12-12T10:33:17.025555Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"RMSE_log 0.04742076623105555\n","output_type":"stream"}]},{"cell_type":"code","source":"def threshold(delta):\n    percentage = 0\n    for i in range(len(df_test)):\n        max_value = max(df_test.loc[i,'zloc']/df_test.loc[i,'predict'], \\\n                        df_test.loc[i,'predict']/df_test.loc[i,'zloc'])\n        \n        if max_value < delta:\n            percentage += 1\n    return percentage/len(df_test)\n\npercentage_1 = round(threshold(1.25),3)\npercentage_2 = round(threshold(1.25**2),3)\npercentage_3 = round(threshold(1.25**3),3)\nprint('Delta 1.25', percentage_1)\nprint('Delta 1.25^2', percentage_2)\nprint('Delta 1.25^3', percentage_3)\n\n\nperformance['delta_1.25'] = round(percentage_1,3)\nperformance['delta_1.25^2'] = round(percentage_2,3)\nperformance['delta_1.25^3'] = round(percentage_3,3)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:17.301483Z","iopub.execute_input":"2023-12-12T10:33:17.301889Z","iopub.status.idle":"2023-12-12T10:33:17.920762Z","shell.execute_reply.started":"2023-12-12T10:33:17.301858Z","shell.execute_reply":"2023-12-12T10:33:17.919807Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Delta 1.25 0.977\nDelta 1.25^2 0.99\nDelta 1.25^3 0.995\n","output_type":"stream"}]},{"cell_type":"code","source":"performance","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:18.842836Z","iopub.execute_input":"2023-12-12T10:33:18.843566Z","iopub.status.idle":"2023-12-12T10:33:18.854970Z","shell.execute_reply.started":"2023-12-12T10:33:18.843534Z","shell.execute_reply":"2023-12-12T10:33:18.854015Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"      Abs_rel  Squa_rel  RMSE_log  delta_1.25  delta_1.25^2  delta_1.25^3\nLSTM     0.05     0.284     0.047       0.977          0.99         0.995","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Abs_rel</th>\n      <th>Squa_rel</th>\n      <th>RMSE_log</th>\n      <th>delta_1.25</th>\n      <th>delta_1.25^2</th>\n      <th>delta_1.25^3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LSTM</th>\n      <td>0.05</td>\n      <td>0.284</td>\n      <td>0.047</td>\n      <td>0.977</td>\n      <td>0.99</td>\n      <td>0.995</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Divide by distance range and calculate it","metadata":{}},{"cell_type":"code","source":"first = df_test[df_test['zloc']<=10]\nfunc1 = np.sum(np.abs((first.predict - first.zloc))/(first.predict))\nmen1 = func1/len(first)\n1-men1","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:25.178425Z","iopub.execute_input":"2023-12-12T10:33:25.179140Z","iopub.status.idle":"2023-12-12T10:33:25.188323Z","shell.execute_reply.started":"2023-12-12T10:33:25.179107Z","shell.execute_reply":"2023-12-12T10:33:25.187509Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"0.9094809693177136"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=10) & (df_test['zloc']<20)\nsecond = df_test[mask]\nfunc2 = np.sum(np.abs((second.predict - second.zloc))/(second.predict))\nmen2 = func2/len(second)\n1-men2","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:25.677725Z","iopub.execute_input":"2023-12-12T10:33:25.678086Z","iopub.status.idle":"2023-12-12T10:33:25.688744Z","shell.execute_reply.started":"2023-12-12T10:33:25.678058Z","shell.execute_reply":"2023-12-12T10:33:25.687775Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"0.9641281818621525"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=20) & (df_test['zloc']<30)\nthird = df_test[mask]\nfunc3 = np.sum(np.abs((third.predict - third.zloc))/(third.predict))\nmen3 = func3/len(third)\n1-men3","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:27.502651Z","iopub.execute_input":"2023-12-12T10:33:27.503066Z","iopub.status.idle":"2023-12-12T10:33:27.513938Z","shell.execute_reply.started":"2023-12-12T10:33:27.503025Z","shell.execute_reply":"2023-12-12T10:33:27.513024Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"0.9612093617121666"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=30) & (df_test['zloc']<40)\nfourth = df_test[mask]\nfunc4 = np.sum(np.abs((fourth.predict - fourth.zloc))/(fourth.predict))\nmen4 = func4/len(fourth)\n1-men4","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:27.747922Z","iopub.execute_input":"2023-12-12T10:33:27.749084Z","iopub.status.idle":"2023-12-12T10:33:27.760444Z","shell.execute_reply.started":"2023-12-12T10:33:27.749040Z","shell.execute_reply":"2023-12-12T10:33:27.759386Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"0.9592773819009036"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=40) & (df_test['zloc']<50)\nfifth = df_test[mask]\nfunc5 = np.sum(np.abs((fifth.predict - fifth.zloc))/(fifth.predict))\nmen5 = func5/len(fifth)\n1-men5","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:29.699864Z","iopub.execute_input":"2023-12-12T10:33:29.700223Z","iopub.status.idle":"2023-12-12T10:33:29.710961Z","shell.execute_reply.started":"2023-12-12T10:33:29.700195Z","shell.execute_reply":"2023-12-12T10:33:29.710012Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"0.9627165096237498"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=50) & (df_test['zloc']<60)\nsixth = df_test[mask]\nfunc6 = np.sum(np.abs((sixth.predict - sixth.zloc))/(sixth.predict))\nmen6 = func6/len(sixth)\n1-men6","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:31.505485Z","iopub.execute_input":"2023-12-12T10:33:31.505876Z","iopub.status.idle":"2023-12-12T10:33:31.516558Z","shell.execute_reply.started":"2023-12-12T10:33:31.505844Z","shell.execute_reply":"2023-12-12T10:33:31.515296Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"0.9578672943433836"},"metadata":{}}]},{"cell_type":"code","source":"mask = (df_test['zloc']>=60) & (df_test['zloc']<70)\nseventh = df_test[mask]\nfunc7 = np.sum(np.abs((seventh.predict - seventh.zloc))/(seventh.predict))\nmen7 = func7/len(seventh)\n1-men7","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:31.797750Z","iopub.execute_input":"2023-12-12T10:33:31.798163Z","iopub.status.idle":"2023-12-12T10:33:31.807674Z","shell.execute_reply.started":"2023-12-12T10:33:31.798119Z","shell.execute_reply":"2023-12-12T10:33:31.806667Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"0.9171631359677587"},"metadata":{}}]},{"cell_type":"code","source":"acc_list = []\nfor i in range(1,12):\n    mask = (df_test['zloc']<i*10) & (df_test['zloc'] >= (i-1)*10)\n    data = df_test[mask]\n    value = np.sum(np.abs((data.predict - data.zloc))/(data.predict))\n    output = value/len(data)\n    acc_list.append(1-output)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:34.569964Z","iopub.execute_input":"2023-12-12T10:33:34.570716Z","iopub.status.idle":"2023-12-12T10:33:34.592841Z","shell.execute_reply.started":"2023-12-12T10:33:34.570685Z","shell.execute_reply":"2023-12-12T10:33:34.591879Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_42/2504621056.py:6: RuntimeWarning: invalid value encountered in scalar divide\n  output = value/len(data)\n","output_type":"stream"}]},{"cell_type":"code","source":"acc_list","metadata":{"execution":{"iopub.status.busy":"2023-12-12T10:33:35.239475Z","iopub.execute_input":"2023-12-12T10:33:35.240115Z","iopub.status.idle":"2023-12-12T10:33:35.246159Z","shell.execute_reply.started":"2023-12-12T10:33:35.240083Z","shell.execute_reply":"2023-12-12T10:33:35.245214Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"[0.9091156083473985,\n 0.9641281818621525,\n 0.9612093617121666,\n 0.9592773819009036,\n 0.9627165096237498,\n 0.9578672943433836,\n 0.9171631359677587,\n 0.8958313564220484,\n 0.9788523152342665,\n nan,\n nan]"},"metadata":{}}]}]}